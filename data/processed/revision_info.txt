arguments: ./create-dataset/align_dataset_mtcnn.py data/raw/ data/processed --
--------------------
tensorflow version: 1.15.0
--------------------
git hash: b'b6624596d028cd10d090d02c943c82dc0ecdb58b'
--------------------
b'diff --git a/Learner.py b/Learner.py\nindex 5f70bc0..1e8dd8b 100644\n--- a/Learner.py\n+++ b/Learner.py\n@@ -235,7 +235,7 @@ class face_learner(object):\n         names : recorded names of faces in facebank\n         tta : test time augmentation (hfilp, that\'s all)\n         \'\'\'\n-        embs = []\n+        embs = [torch.zeros(1, 512)]\n         for img in faces:\n             if tta:\n                 mirror = trans.functional.hflip(img)\n@@ -243,10 +243,20 @@ class face_learner(object):\n                 emb_mirror = self.model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\n                 embs.append(l2_norm(emb + emb_mirror))\n             else:                        \n-                embs.append(self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\n-        source_embs = torch.cat(embs)\n+                emb = (self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\n+                embs.append(emb)\n+\n+        source_embs = torch.cat(embs[-1:])\n         \n-        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1,0).unsqueeze(0)\n+        # Move source_embs to CUDA\n+        source_embs = source_embs.to(\'cuda:0\')\n+\n+        # Move target_embs to CUDA\n+        target_embs = target_embs.to(\'cuda:0\')\n+\n+        # Perform the operation\n+        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1, 0).unsqueeze(0)\n+\n         dist = torch.sum(torch.pow(diff, 2), dim=1)\n         minimum, min_idx = torch.min(dist, dim=1)\n         if float(minimum[0]) > 0.9:\ndiff --git a/data/facebank/Joey/Joey.01.png b/data/facebank/Joey/Joey.01.png\ndeleted file mode 100644\nindex 3c685d0..0000000\nBinary files a/data/facebank/Joey/Joey.01.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.02.png b/data/facebank/Joey/Joey.02.png\ndeleted file mode 100644\nindex 9ca5128..0000000\nBinary files a/data/facebank/Joey/Joey.02.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.03.png b/data/facebank/Joey/Joey.03.png\ndeleted file mode 100644\nindex 6d2caca..0000000\nBinary files a/data/facebank/Joey/Joey.03.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.04.png b/data/facebank/Joey/Joey.04.png\ndeleted file mode 100644\nindex f698017..0000000\nBinary files a/data/facebank/Joey/Joey.04.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.05.png b/data/facebank/Joey/Joey.05.png\ndeleted file mode 100644\nindex 11c4831..0000000\nBinary files a/data/facebank/Joey/Joey.05.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.06.png b/data/facebank/Joey/Joey.06.png\ndeleted file mode 100644\nindex 96f7ed3..0000000\nBinary files a/data/facebank/Joey/Joey.06.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.07.png b/data/facebank/Joey/Joey.07.png\ndeleted file mode 100644\nindex 1e165fa..0000000\nBinary files a/data/facebank/Joey/Joey.07.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.08.png b/data/facebank/Joey/Joey.08.png\ndeleted file mode 100644\nindex 09144d8..0000000\nBinary files a/data/facebank/Joey/Joey.08.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.09.png b/data/facebank/Joey/Joey.09.png\ndeleted file mode 100644\nindex 7ed417e..0000000\nBinary files a/data/facebank/Joey/Joey.09.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.11.png b/data/facebank/Joey/Joey.11.png\ndeleted file mode 100644\nindex 580bc53..0000000\nBinary files a/data/facebank/Joey/Joey.11.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.12.png b/data/facebank/Joey/Joey.12.png\ndeleted file mode 100644\nindex f298964..0000000\nBinary files a/data/facebank/Joey/Joey.12.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.13.png b/data/facebank/Joey/Joey.13.png\ndeleted file mode 100644\nindex 0498bc2..0000000\nBinary files a/data/facebank/Joey/Joey.13.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.14.png b/data/facebank/Joey/Joey.14.png\ndeleted file mode 100644\nindex 54b8ddc..0000000\nBinary files a/data/facebank/Joey/Joey.14.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.15.png b/data/facebank/Joey/Joey.15.png\ndeleted file mode 100644\nindex c2a4757..0000000\nBinary files a/data/facebank/Joey/Joey.15.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.16.png b/data/facebank/Joey/Joey.16.png\ndeleted file mode 100644\nindex e3bc9b8..0000000\nBinary files a/data/facebank/Joey/Joey.16.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.17.png b/data/facebank/Joey/Joey.17.png\ndeleted file mode 100644\nindex 4e9312e..0000000\nBinary files a/data/facebank/Joey/Joey.17.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.18.png b/data/facebank/Joey/Joey.18.png\ndeleted file mode 100644\nindex 936d8a5..0000000\nBinary files a/data/facebank/Joey/Joey.18.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.19.png b/data/facebank/Joey/Joey.19.png\ndeleted file mode 100644\nindex bdfb07d..0000000\nBinary files a/data/facebank/Joey/Joey.19.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.20.png b/data/facebank/Joey/Joey.20.png\ndeleted file mode 100644\nindex b78b6bf..0000000\nBinary files a/data/facebank/Joey/Joey.20.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.02.png b/data/facebank/Monica/Monica.02.png\ndeleted file mode 100644\nindex 1a05511..0000000\nBinary files a/data/facebank/Monica/Monica.02.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.03.png b/data/facebank/Monica/Monica.03.png\ndeleted file mode 100644\nindex 1f58157..0000000\nBinary files a/data/facebank/Monica/Monica.03.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.04.png b/data/facebank/Monica/Monica.04.png\ndeleted file mode 100644\nindex 05a254a..0000000\nBinary files a/data/facebank/Monica/Monica.04.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.05.png b/data/facebank/Monica/Monica.05.png\ndeleted file mode 100644\nindex f97be0d..0000000\nBinary files a/data/facebank/Monica/Monica.05.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.08.png b/data/facebank/Monica/Monica.08.png\ndeleted file mode 100644\nindex 4ccbc47..0000000\nBinary files a/data/facebank/Monica/Monica.08.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.png b/data/facebank/Monica/Monica.png\ndeleted file mode 100644\nindex 758c3c6..0000000\nBinary files a/data/facebank/Monica/Monica.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica10.png b/data/facebank/Monica/Monica10.png\ndeleted file mode 100644\nindex 8994753..0000000\nBinary files a/data/facebank/Monica/Monica10.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica11.png b/data/facebank/Monica/Monica11.png\ndeleted file mode 100644\nindex 357b8bc..0000000\nBinary files a/data/facebank/Monica/Monica11.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica12.png b/data/facebank/Monica/Monica12.png\ndeleted file mode 100644\nindex dc9ac29..0000000\nBinary files a/data/facebank/Monica/Monica12.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica13.png b/data/facebank/Monica/Monica13.png\ndeleted file mode 100644\nindex d7c2cf6..0000000\nBinary files a/data/facebank/Monica/Monica13.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica14.png b/data/facebank/Monica/Monica14.png\ndeleted file mode 100644\nindex a528414..0000000\nBinary files a/data/facebank/Monica/Monica14.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica15.png b/data/facebank/Monica/Monica15.png\ndeleted file mode 100644\nindex 5bfe1a9..0000000\nBinary files a/data/facebank/Monica/Monica15.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica16.png b/data/facebank/Monica/Monica16.png\ndeleted file mode 100644\nindex f94e645..0000000\nBinary files a/data/facebank/Monica/Monica16.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica17.png b/data/facebank/Monica/Monica17.png\ndeleted file mode 100644\nindex bf24311..0000000\nBinary files a/data/facebank/Monica/Monica17.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica18.png b/data/facebank/Monica/Monica18.png\ndeleted file mode 100644\nindex b04f261..0000000\nBinary files a/data/facebank/Monica/Monica18.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica19.png b/data/facebank/Monica/Monica19.png\ndeleted file mode 100644\nindex 0eeb0d6..0000000\nBinary files a/data/facebank/Monica/Monica19.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica20.png b/data/facebank/Monica/Monica20.png\ndeleted file mode 100644\nindex e5d4e24..0000000\nBinary files a/data/facebank/Monica/Monica20.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica6.png b/data/facebank/Monica/Monica6.png\ndeleted file mode 100644\nindex 7112741..0000000\nBinary files a/data/facebank/Monica/Monica6.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica7.png b/data/facebank/Monica/Monica7.png\ndeleted file mode 100644\nindex 3567770..0000000\nBinary files a/data/facebank/Monica/Monica7.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica8.png b/data/facebank/Monica/Monica8.png\ndeleted file mode 100644\nindex de723c2..0000000\nBinary files a/data/facebank/Monica/Monica8.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica9.png b/data/facebank/Monica/Monica9.png\ndeleted file mode 100644\nindex c7ae768..0000000\nBinary files a/data/facebank/Monica/Monica9.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe06.png b/data/facebank/Phoebe/Pheobe06.png\ndeleted file mode 100644\nindex 3270a78..0000000\nBinary files a/data/facebank/Phoebe/Pheobe06.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe07.png b/data/facebank/Phoebe/Pheobe07.png\ndeleted file mode 100644\nindex 25cb4fd..0000000\nBinary files a/data/facebank/Phoebe/Pheobe07.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe08.png b/data/facebank/Phoebe/Pheobe08.png\ndeleted file mode 100644\nindex cd5d7e0..0000000\nBinary files a/data/facebank/Phoebe/Pheobe08.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe09.png b/data/facebank/Phoebe/Pheobe09.png\ndeleted file mode 100644\nindex 4df4c47..0000000\nBinary files a/data/facebank/Phoebe/Pheobe09.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe10.png b/data/facebank/Phoebe/Pheobe10.png\ndeleted file mode 100644\nindex 8637633..0000000\nBinary files a/data/facebank/Phoebe/Pheobe10.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe11.png b/data/facebank/Phoebe/Pheobe11.png\ndeleted file mode 100644\nindex 7ad6ed9..0000000\nBinary files a/data/facebank/Phoebe/Pheobe11.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe12.png b/data/facebank/Phoebe/Pheobe12.png\ndeleted file mode 100644\nindex 7ce09dc..0000000\nBinary files a/data/facebank/Phoebe/Pheobe12.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe13.png b/data/facebank/Phoebe/Pheobe13.png\ndeleted file mode 100644\nindex 17b6c11..0000000\nBinary files a/data/facebank/Phoebe/Pheobe13.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe15.png b/data/facebank/Phoebe/Pheobe15.png\ndeleted file mode 100644\nindex 6c47f10..0000000\nBinary files a/data/facebank/Phoebe/Pheobe15.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe16.png b/data/facebank/Phoebe/Pheobe16.png\ndeleted file mode 100644\nindex 56775ea..0000000\nBinary files a/data/facebank/Phoebe/Pheobe16.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe17.png b/data/facebank/Phoebe/Pheobe17.png\ndeleted file mode 100644\nindex 0897eb9..0000000\nBinary files a/data/facebank/Phoebe/Pheobe17.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe18.png b/data/facebank/Phoebe/Pheobe18.png\ndeleted file mode 100644\nindex 2579394..0000000\nBinary files a/data/facebank/Phoebe/Pheobe18.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe19.png b/data/facebank/Phoebe/Pheobe19.png\ndeleted file mode 100644\nindex 111693b..0000000\nBinary files a/data/facebank/Phoebe/Pheobe19.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe20.png b/data/facebank/Phoebe/Pheobe20.png\ndeleted file mode 100644\nindex 82ecd08..0000000\nBinary files a/data/facebank/Phoebe/Pheobe20.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.02.png b/data/facebank/Phoebe/Phoebe.02.png\ndeleted file mode 100644\nindex 5a67bbc..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.02.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.03.png b/data/facebank/Phoebe/Phoebe.03.png\ndeleted file mode 100644\nindex 43a1ae2..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.03.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.04.png b/data/facebank/Phoebe/Phoebe.04.png\ndeleted file mode 100644\nindex e221b0d..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.04.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.05.png b/data/facebank/Phoebe/Phoebe.05.png\ndeleted file mode 100644\nindex 96a6629..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.05.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.png b/data/facebank/Phoebe/Phoebe.png\ndeleted file mode 100644\nindex befbda6..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.02.png b/data/facebank/Rachel/Rachel.02.png\ndeleted file mode 100644\nindex e62a99a..0000000\nBinary files a/data/facebank/Rachel/Rachel.02.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.03.png b/data/facebank/Rachel/Rachel.03.png\ndeleted file mode 100644\nindex f4ac8fa..0000000\nBinary files a/data/facebank/Rachel/Rachel.03.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.04.png b/data/facebank/Rachel/Rachel.04.png\ndeleted file mode 100644\nindex 1ff0d0c..0000000\nBinary files a/data/facebank/Rachel/Rachel.04.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.05.png b/data/facebank/Rachel/Rachel.05.png\ndeleted file mode 100644\nindex 2d69283..0000000\nBinary files a/data/facebank/Rachel/Rachel.05.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel01.png b/data/facebank/Rachel/Rachel01.png\ndeleted file mode 100644\nindex 91e0fd9..0000000\nBinary files a/data/facebank/Rachel/Rachel01.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel06.png b/data/facebank/Rachel/Rachel06.png\ndeleted file mode 100644\nindex be80ce0..0000000\nBinary files a/data/facebank/Rachel/Rachel06.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel07.png b/data/facebank/Rachel/Rachel07.png\ndeleted file mode 100644\nindex 7f1c0d8..0000000\nBinary files a/data/facebank/Rachel/Rachel07.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel08.png b/data/facebank/Rachel/Rachel08.png\ndeleted file mode 100644\nindex 2295162..0000000\nBinary files a/data/facebank/Rachel/Rachel08.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel09.png b/data/facebank/Rachel/Rachel09.png\ndeleted file mode 100644\nindex 49dfe08..0000000\nBinary files a/data/facebank/Rachel/Rachel09.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel10.png b/data/facebank/Rachel/Rachel10.png\ndeleted file mode 100644\nindex bae25a1..0000000\nBinary files a/data/facebank/Rachel/Rachel10.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel11.png b/data/facebank/Rachel/Rachel11.png\ndeleted file mode 100644\nindex 0e39cef..0000000\nBinary files a/data/facebank/Rachel/Rachel11.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel12.png b/data/facebank/Rachel/Rachel12.png\ndeleted file mode 100644\nindex d0945d5..0000000\nBinary files a/data/facebank/Rachel/Rachel12.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel13.png b/data/facebank/Rachel/Rachel13.png\ndeleted file mode 100644\nindex 9a942f2..0000000\nBinary files a/data/facebank/Rachel/Rachel13.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel14.png b/data/facebank/Rachel/Rachel14.png\ndeleted file mode 100644\nindex 3786268..0000000\nBinary files a/data/facebank/Rachel/Rachel14.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel15.png b/data/facebank/Rachel/Rachel15.png\ndeleted file mode 100644\nindex c8b5b67..0000000\nBinary files a/data/facebank/Rachel/Rachel15.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel16.png b/data/facebank/Rachel/Rachel16.png\ndeleted file mode 100644\nindex 8fa13b5..0000000\nBinary files a/data/facebank/Rachel/Rachel16.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel17.png b/data/facebank/Rachel/Rachel17.png\ndeleted file mode 100644\nindex dcd563d..0000000\nBinary files a/data/facebank/Rachel/Rachel17.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel18.png b/data/facebank/Rachel/Rachel18.png\ndeleted file mode 100644\nindex 5d58bc1..0000000\nBinary files a/data/facebank/Rachel/Rachel18.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel19.png b/data/facebank/Rachel/Rachel19.png\ndeleted file mode 100644\nindex 5f9778a..0000000\nBinary files a/data/facebank/Rachel/Rachel19.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel20.png b/data/facebank/Rachel/Rachel20.png\ndeleted file mode 100644\nindex 7d62102..0000000\nBinary files a/data/facebank/Rachel/Rachel20.png and /dev/null differ\ndiff --git a/data/facebank/Ross/Ross03.png b/data/facebank/Ross/Ross03.png\ndeleted file mode 100644\nindex d8c2c7d..0000000\nBinary files a/data/facebank/Ross/Ross03.png and /dev/null differ\ndiff --git a/data/facebank/Ross/Ross04.png b/data/facebank/Ross/Ross04.png\ndeleted file mode 100644\nindex b3ccc1b..0000000\nBinary files a/data/facebank/Ross/Ross04.png and /dev/null differ\ndiff --git a/data/facebank/Ross/Ross05.png b/data/facebank/Ross/Ross05.png\ndeleted file mode 100644\nindex 95fda3f..0000000\nBinary files a/data/facebank/Ross/Ross05.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross01.png b/data/facebank/Ross/ross01.png\ndeleted file mode 100644\nindex 3459b69..0000000\nBinary files a/data/facebank/Ross/ross01.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross02.png b/data/facebank/Ross/ross02.png\ndeleted file mode 100644\nindex 332c896..0000000\nBinary files a/data/facebank/Ross/ross02.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross06.png b/data/facebank/Ross/ross06.png\ndeleted file mode 100644\nindex 8cc4528..0000000\nBinary files a/data/facebank/Ross/ross06.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross07.png b/data/facebank/Ross/ross07.png\ndeleted file mode 100644\nindex f26cf1d..0000000\nBinary files a/data/facebank/Ross/ross07.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross08.png b/data/facebank/Ross/ross08.png\ndeleted file mode 100644\nindex 76b0a88..0000000\nBinary files a/data/facebank/Ross/ross08.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross09.png b/data/facebank/Ross/ross09.png\ndeleted file mode 100644\nindex 50d07c8..0000000\nBinary files a/data/facebank/Ross/ross09.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross10.png b/data/facebank/Ross/ross10.png\ndeleted file mode 100644\nindex 64651c2..0000000\nBinary files a/data/facebank/Ross/ross10.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross11.png b/data/facebank/Ross/ross11.png\ndeleted file mode 100644\nindex bfc016b..0000000\nBinary files a/data/facebank/Ross/ross11.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross12.png b/data/facebank/Ross/ross12.png\ndeleted file mode 100644\nindex d65cc34..0000000\nBinary files a/data/facebank/Ross/ross12.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross13.png b/data/facebank/Ross/ross13.png\ndeleted file mode 100644\nindex 2671298..0000000\nBinary files a/data/facebank/Ross/ross13.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross14.png b/data/facebank/Ross/ross14.png\ndeleted file mode 100644\nindex c239eea..0000000\nBinary files a/data/facebank/Ross/ross14.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross15.png b/data/facebank/Ross/ross15.png\ndeleted file mode 100644\nindex 7326319..0000000\nBinary files a/data/facebank/Ross/ross15.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross16.png b/data/facebank/Ross/ross16.png\ndeleted file mode 100644\nindex 7786cdd..0000000\nBinary files a/data/facebank/Ross/ross16.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross17.png b/data/facebank/Ross/ross17.png\ndeleted file mode 100644\nindex 0c70f90..0000000\nBinary files a/data/facebank/Ross/ross17.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross18.png b/data/facebank/Ross/ross18.png\ndeleted file mode 100644\nindex 06d916d..0000000\nBinary files a/data/facebank/Ross/ross18.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross19.png b/data/facebank/Ross/ross19.png\ndeleted file mode 100644\nindex 8f79393..0000000\nBinary files a/data/facebank/Ross/ross19.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross20.png b/data/facebank/Ross/ross20.png\ndeleted file mode 100644\nindex 6621203..0000000\nBinary files a/data/facebank/Ross/ross20.png and /dev/null differ\ndiff --git a/data/facebank/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png b/data/facebank/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png\ndeleted file mode 100644\nindex 12d01cb..0000000\nBinary files a/data/facebank/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png and /dev/null differ\ndiff --git a/data/facebank/facebank.pth b/data/facebank/facebank.pth\nindex 5fdbbbc..847e0e9 100644\n--- a/data/facebank/facebank.pth\n+++ b/data/facebank/facebank.pth\n@@ -1,3 +1,3 @@\n version https://git-lfs.github.com/spec/v1\n-oid sha256:01da0a10ca9fae085c25f535aed1241340a3bad79a49c1f2ec28b07aaa3307d0\n-size 15083\n+oid sha256:90508e947263ababfecb4ce38fcecdf02d25732da1b799abb2d503d38a55f6ba\n+size 22867\ndiff --git a/data/facebank/names.npy b/data/facebank/names.npy\nindex 108dbb2..83a122b 100644\nBinary files a/data/facebank/names.npy and b/data/facebank/names.npy differ\ndiff --git a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png\nindex 14720ab..17514ab 100644\nBinary files a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png and b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png differ\ndiff --git a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png\nindex 52d5e06..833e9da 100644\nBinary files a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png and b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png differ\ndiff --git a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.14 PM.png b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.14 PM.png\nindex e62c53b..57906de 100644\nBinary files a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.14 PM.png and b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.14 PM.png differ\ndiff --git a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png\nindex 3c00e1d..83b5e6e 100644\nBinary files a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png and b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png differ\ndiff --git a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM.png b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM.png\nindex 7ad0e0a..895e621 100644\nBinary files a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM.png and b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM.png differ\ndiff --git a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.11 PM.png b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.11 PM.png\nindex 2a06492..09f6cf1 100644\nBinary files a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.11 PM.png and b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.11 PM.png differ\ndiff --git a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png b/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png\nindex d10cd02..e959450 100644\nBinary files a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png and b/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png differ\ndiff --git a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png b/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png\nindex 597fb32..52014c0 100644\nBinary files a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png and b/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png differ\ndiff --git a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.06 PM.png b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.06 PM.png\nindex c17575a..61f971d 100644\nBinary files a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.06 PM.png and b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.06 PM.png differ\ndiff --git a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png\nindex d6503c1..baf1b94 100644\nBinary files a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png and b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png differ\ndiff --git a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM.png b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM.png\nindex 81e4f6c..e65dbea 100644\nBinary files a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM.png and b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM.png differ\ndiff --git a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png\nindex bb1b1d2..7cdf278 100644\nBinary files a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png and b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png differ\ndiff --git a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png\nindex 4bf224f..620a66b 100644\nBinary files a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png and b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png differ\ndiff --git a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM.png b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM.png\nindex 3361266..a086847 100644\nBinary files a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM.png and b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM.png differ\ndiff --git a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png\nindex d48b5cb..0ec9077 100644\nBinary files a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png and b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png differ\ndiff --git a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png\nindex 58b7ccc..ef12b96 100644\nBinary files a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png and b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png differ\ndiff --git a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM.png b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM.png\nindex 6066aaa..aa78819 100644\nBinary files a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM.png and b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM.png differ\ndiff --git a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png\nindex ef2e857..e90431a 100644\nBinary files a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png and b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png differ\ndiff --git a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM.png b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM.png\nindex 45c8571..c249f6f 100644\nBinary files a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM.png and b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM.png differ\ndiff --git a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.13 PM.png b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.13 PM.png\nindex f7f3f63..e31f4f0 100644\nBinary files a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.13 PM.png and b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.13 PM.png differ\ndiff --git a/data/processed/Suhail/WhatsApp Image 2023-02-23 at 5.12.15 PM.png b/data/processed/Suhail/WhatsApp Image 2023-02-23 at 5.12.15 PM.png\nindex d9af0a2..3a14c94 100644\nBinary files a/data/processed/Suhail/WhatsApp Image 2023-02-23 at 5.12.15 PM.png and b/data/processed/Suhail/WhatsApp Image 2023-02-23 at 5.12.15 PM.png differ\ndiff --git a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png b/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png\nindex 12d01cb..e3d82a4 100644\nBinary files a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png and b/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png differ\ndiff --git a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM.png b/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM.png\nindex a418fd1..a5210fa 100644\nBinary files a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM.png and b/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM.png differ\ndiff --git a/data/processed/bounding_boxes_50704.txt b/data/processed/bounding_boxes_50704.txt\ndeleted file mode 100644\nindex 65b6e95..0000000\n--- a/data/processed/bounding_boxes_50704.txt\n+++ /dev/null\n@@ -1,23 +0,0 @@\n-data/processed\\Akshay\\WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png 208 309 533 747\n-data/processed\\Akshay\\WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png 326 328 541 621\n-data/processed\\Akshay\\WhatsApp Image 2023-02-23 at 5.12.14 PM.png 195 438 434 733\n-data/processed\\Ananya\\WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png 401 430 682 835\n-data/processed\\Ananya\\WhatsApp Image 2023-02-23 at 5.12.10 PM.png 441 590 726 963\n-data/processed\\Ananya\\WhatsApp Image 2023-02-23 at 5.12.11 PM.png 477 511 795 920\n-data/processed\\Bhavya\\WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png 431 479 664 764\n-data/processed\\Bhavya\\WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png 434 409 756 801\n-data/processed\\Golchi\\WhatsApp Image 2023-02-14 at 5.05.06 PM.png 359 250 706 730\n-data/processed\\Golchi\\WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png 305 196 634 616\n-data/processed\\Golchi\\WhatsApp Image 2023-02-14 at 5.05.07 PM.png 296 261 647 735\n-data/processed\\Jain\\WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png 308 269 604 653\n-data/processed\\Jain\\WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png 314 305 627 692\n-data/processed\\Jain\\WhatsApp Image 2023-02-14 at 5.05.08 PM.png 252 248 632 738\n-data/processed\\Saksham\\WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png 8 355 643 1106\n-data/processed\\Saksham\\WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png 239 280 860 1040\n-data/processed\\Saksham\\WhatsApp Image 2023-02-23 at 5.12.09 PM.png 561 392 1105 1085\n-data/processed\\Sandeep\\WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png 479 480 730 813\n-data/processed\\Sandeep\\WhatsApp Image 2023-02-23 at 5.12.12 PM.png 420 404 715 755\n-data/processed\\Sandeep\\WhatsApp Image 2023-02-23 at 5.12.13 PM.png 422 409 727 766\n-data/processed\\Suhail\\WhatsApp Image 2023-02-23 at 5.12.15 PM.png 248 108 484 411\n-data/processed\\Utkarsh\\WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png 581 514 658 596\n-data/processed\\Utkarsh\\WhatsApp Image 2023-02-23 at 5.08.13 PM.png 391 116 673 477\ndiff --git a/data/processed/revision_info.txt b/data/processed/revision_info.txt\nindex ed99a36..a863961 100644\n--- a/data/processed/revision_info.txt\n+++ b/data/processed/revision_info.txt\n@@ -1,7 +1,7 @@\n-arguments: .\\create-dataset\\align_dataset_mtcnn.py data/raw/ data/processed --image_size 112\n+arguments: ./create-dataset/align_dataset_mtcnn.py data/raw/ data/processed --\n --------------------\n tensorflow version: 1.15.0\n --------------------\n-git hash: b\'1769caaa4169f2bdae17e269e1348cb594357fb3\'\n+git hash: b\'b6624596d028cd10d090d02c943c82dc0ecdb58b\'\n --------------------\n-b\'diff --git a/.gitattributes b/.gitattributes\\ndeleted file mode 100644\\nindex ec4a626..0000000\\n--- a/.gitattributes\\n+++ /dev/null\\n@@ -1 +0,0 @@\\n-*.pth filter=lfs diff=lfs merge=lfs -text\\ndiff --git a/.gitignore b/.gitignore\\ndeleted file mode 100644\\nindex 894a44c..0000000\\n--- a/.gitignore\\n+++ /dev/null\\n@@ -1,104 +0,0 @@\\n-# Byte-compiled / optimized / DLL files\\n-__pycache__/\\n-*.py[cod]\\n-*$py.class\\n-\\n-# C extensions\\n-*.so\\n-\\n-# Distribution / packaging\\n-.Python\\n-build/\\n-develop-eggs/\\n-dist/\\n-downloads/\\n-eggs/\\n-.eggs/\\n-lib/\\n-lib64/\\n-parts/\\n-sdist/\\n-var/\\n-wheels/\\n-*.egg-info/\\n-.installed.cfg\\n-*.egg\\n-MANIFEST\\n-\\n-# PyInstaller\\n-#  Usually these files are written by a python script from a template\\n-#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\n-*.manifest\\n-*.spec\\n-\\n-# Installer logs\\n-pip-log.txt\\n-pip-delete-this-directory.txt\\n-\\n-# Unit test / coverage reports\\n-htmlcov/\\n-.tox/\\n-.coverage\\n-.coverage.*\\n-.cache\\n-nosetests.xml\\n-coverage.xml\\n-*.cover\\n-.hypothesis/\\n-.pytest_cache/\\n-\\n-# Translations\\n-*.mo\\n-*.pot\\n-\\n-# Django stuff:\\n-*.log\\n-local_settings.py\\n-db.sqlite3\\n-\\n-# Flask stuff:\\n-instance/\\n-.webassets-cache\\n-\\n-# Scrapy stuff:\\n-.scrapy\\n-\\n-# Sphinx documentation\\n-docs/_build/\\n-\\n-# PyBuilder\\n-target/\\n-\\n-# Jupyter Notebook\\n-.ipynb_checkpoints\\n-\\n-# pyenv\\n-.python-version\\n-\\n-# celery beat schedule file\\n-celerybeat-schedule\\n-\\n-# SageMath parsed files\\n-*.sage.py\\n-\\n-# Environments\\n-.env\\n-.venv\\n-env/\\n-venv/\\n-ENV/\\n-env.bak/\\n-venv.bak/\\n-\\n-# Spyder project settings\\n-.spyderproject\\n-.spyproject\\n-\\n-# Rope project settings\\n-.ropeproject\\n-\\n-# mkdocs documentation\\n-/site\\n-\\n-# mypy\\n-.mypy_cache/\\ndiff --git a/Learner.py b/Learner.py\\ndeleted file mode 100644\\nindex 5f70bc0..0000000\\n--- a/Learner.py\\n+++ /dev/null\\n@@ -1,254 +0,0 @@\\n-from data.data_pipe import de_preprocess, get_train_loader, get_val_data\\n-from model import Backbone, Arcface, MobileFaceNet, Am_softmax, l2_norm\\n-from verifacation import evaluate\\n-import torch\\n-from torch import optim\\n-import numpy as np\\n-from tqdm import tqdm\\n-from tensorboardX import SummaryWriter\\n-from matplotlib import pyplot as plt\\n-plt.switch_backend(\\\'agg\\\')\\n-from utils import get_time, gen_plot, hflip_batch, separate_bn_paras\\n-from PIL import Image\\n-from torchvision import transforms as trans\\n-import math\\n-import config\\n-\\n-class face_learner(object):\\n-    def __init__(self, conf, inference=False):\\n-        print(conf)\\n-        if conf.use_mobilfacenet:\\n-            self.model = MobileFaceNet(conf.embedding_size).to(conf.device)\\n-            print(\\\'MobileFaceNet model generated\\\')\\n-        else:\\n-            self.model = Backbone(conf.net_depth, conf.drop_ratio, conf.net_mode).to(conf.device)\\n-            print(\\\'{}_{} model opened\\\'.format(conf.net_mode, conf.net_depth))\\n-        \\n-        if not inference:\\n-            self.milestones = conf.milestones\\n-            self.loader, self.class_num = get_train_loader(conf)        \\n-\\n-            self.writer = SummaryWriter(conf.log_path)\\n-            self.step = 0\\n-            self.head = Arcface(embedding_size=conf.embedding_size, classnum=self.class_num).to(conf.device)\\n-\\n-            print(\\\'two model heads generated\\\')\\n-\\n-            paras_only_bn, paras_wo_bn = separate_bn_paras(self.model)\\n-            \\n-            if conf.use_mobilfacenet:\\n-                self.optimizer = optim.SGD([\\n-                                    {\\\'params\\\': paras_wo_bn[:-1], \\\'weight_decay\\\': 4e-5},\\n-                                    {\\\'params\\\': [paras_wo_bn[-1]] + [self.head.kernel], \\\'weight_decay\\\': 4e-4},\\n-                                    {\\\'params\\\': paras_only_bn}\\n-                                ], lr = conf.lr, momentum = conf.momentum)\\n-            else:\\n-                self.optimizer = optim.SGD([\\n-                                    {\\\'params\\\': paras_wo_bn + [self.head.kernel], \\\'weight_decay\\\': 5e-4},\\n-                                    {\\\'params\\\': paras_only_bn}\\n-                                ], lr = conf.lr, momentum = conf.momentum)\\n-            print(self.optimizer)\\n-#             self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=40, verbose=True)\\n-\\n-            print(\\\'optimizers generated\\\')    \\n-            self.board_loss_every = len(self.loader)//100\\n-            self.evaluate_every = len(self.loader)//10\\n-            self.save_every = len(self.loader)//5\\n-            self.agedb_30, self.cfp_fp, self.lfw, self.agedb_30_issame, self.cfp_fp_issame, self.lfw_issame = get_val_data(self.loader.dataset.root.parent)\\n-        else:\\n-            self.threshold = conf.threshold\\n-    \\n-    def save_state(self, conf, accuracy, to_save_folder=False, extra=None, model_only=False):\\n-        if to_save_folder:\\n-            save_path = conf.save_path\\n-        else:\\n-            save_path = conf.model_path\\n-        torch.save(\\n-            self.model.state_dict(), save_path /\\n-            (\\\'model_{}_accuracy:{}_step:{}_{}.pth\\\'.format(get_time(), accuracy, self.step, extra)))\\n-        if not model_only:\\n-            torch.save(\\n-                self.head.state_dict(), save_path /\\n-                (\\\'head_{}_accuracy:{}_step:{}_{}.pth\\\'.format(get_time(), accuracy, self.step, extra)))\\n-            torch.save(\\n-                self.optimizer.state_dict(), save_path /\\n-                (\\\'optimizer_{}_accuracy:{}_step:{}_{}.pth\\\'.format(get_time(), accuracy, self.step, extra)))\\n-    \\n-    def load_state(self, conf, fixed_str, from_save_folder=False, model_only=False):\\n-        if from_save_folder:\\n-            save_path = conf.save_path\\n-        else:\\n-            save_path = conf.model_path            \\n-        self.model.load_state_dict(torch.load(save_path/\\\'model_{}\\\'.format(fixed_str), map_location=torch.device(\\\'cpu\\\')))\\n-        if not model_only:\\n-            self.head.load_state_dict(torch.load(save_path/\\\'head_{}\\\'.format(fixed_str), map_location=torch.device(\\\'cpu\\\')))\\n-            self.optimizer.load_state_dict(torch.load(save_path/\\\'optimizer_{}\\\'.format(fixed_str), map_location=torch.device(\\\'cpu\\\')))\\n-        \\n-    def board_val(self, db_name, accuracy, best_threshold, roc_curve_tensor):\\n-        self.writer.add_scalar(\\\'{}_accuracy\\\'.format(db_name), accuracy, self.step)\\n-        self.writer.add_scalar(\\\'{}_best_threshold\\\'.format(db_name), best_threshold, self.step)\\n-        self.writer.add_image(\\\'{}_roc_curve\\\'.format(db_name), roc_curve_tensor, self.step)\\n-#         self.writer.add_scalar(\\\'{}_val:true accept ratio\\\'.format(db_name), val, self.step)\\n-#         self.writer.add_scalar(\\\'{}_val_std\\\'.format(db_name), val_std, self.step)\\n-#         self.writer.add_scalar(\\\'{}_far:False Acceptance Ratio\\\'.format(db_name), far, self.step)\\n-        \\n-    def evaluate(self, conf, carray, issame, nrof_folds = 5, tta = False):\\n-        self.model.eval()\\n-        idx = 0\\n-        embeddings = np.zeros([len(carray), conf.embedding_size])\\n-        with torch.no_grad():\\n-            while idx + conf.batch_size <= len(carray):\\n-                batch = torch.tensor(carray[idx:idx + conf.batch_size])\\n-                if tta:\\n-                    fliped = hflip_batch(batch)\\n-                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\\n-                    embeddings[idx:idx + conf.batch_size] = l2_norm(emb_batch)\\n-                else:\\n-                    embeddings[idx:idx + conf.batch_size] = self.model(batch.to(conf.device)).cpu()\\n-                idx += conf.batch_size\\n-            if idx < len(carray):\\n-                batch = torch.tensor(carray[idx:])            \\n-                if tta:\\n-                    fliped = hflip_batch(batch)\\n-                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\\n-                    embeddings[idx:] = l2_norm(emb_batch)\\n-                else:\\n-                    embeddings[idx:] = self.model(batch.to(conf.device)).cpu()\\n-        tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\\n-        buf = gen_plot(fpr, tpr)\\n-        roc_curve = Image.open(buf)\\n-        roc_curve_tensor = trans.ToTensor()(roc_curve)\\n-        return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\\n-    \\n-    def find_lr(self,\\n-                conf,\\n-                init_value=1e-8,\\n-                final_value=10.,\\n-                beta=0.98,\\n-                bloding_scale=3.,\\n-                num=None):\\n-        if not num:\\n-            num = len(self.loader)\\n-        mult = (final_value / init_value)**(1 / num)\\n-        lr = init_value\\n-        for params in self.optimizer.param_groups:\\n-            params[\\\'lr\\\'] = lr\\n-        self.model.train()\\n-        avg_loss = 0.\\n-        best_loss = 0.\\n-        batch_num = 0\\n-        losses = []\\n-        log_lrs = []\\n-        for i, (imgs, labels) in tqdm(enumerate(self.loader), total=num):\\n-\\n-            imgs = imgs.to(conf.device)\\n-            labels = labels.to(conf.device)\\n-            batch_num += 1          \\n-\\n-            self.optimizer.zero_grad()\\n-\\n-            embeddings = self.model(imgs)\\n-            thetas = self.head(embeddings, labels)\\n-            loss = conf.ce_loss(thetas, labels)          \\n-    \\n-            #Compute the smoothed loss\\n-            avg_loss = beta * avg_loss + (1 - beta) * loss.item()\\n-            self.writer.add_scalar(\\\'avg_loss\\\', avg_loss, batch_num)\\n-            smoothed_loss = avg_loss / (1 - beta**batch_num)\\n-            self.writer.add_scalar(\\\'smoothed_loss\\\', smoothed_loss,batch_num)\\n-            #Stop if the loss is exploding\\n-            if batch_num > 1 and smoothed_loss > bloding_scale * best_loss:\\n-                print(\\\'exited with best_loss at {}\\\'.format(best_loss))\\n-                plt.plot(log_lrs[10:-5], losses[10:-5])\\n-                return log_lrs, losses\\n-            #Record the best loss\\n-            if smoothed_loss < best_loss or batch_num == 1:\\n-                best_loss = smoothed_loss\\n-            #Store the values\\n-            losses.append(smoothed_loss)\\n-            log_lrs.append(math.log10(lr))\\n-            self.writer.add_scalar(\\\'log_lr\\\', math.log10(lr), batch_num)\\n-            #Do the SGD step\\n-            #Update the lr for the next step\\n-\\n-            loss.backward()\\n-            self.optimizer.step()\\n-\\n-            lr *= mult\\n-            for params in self.optimizer.param_groups:\\n-                params[\\\'lr\\\'] = lr\\n-            if batch_num > num:\\n-                plt.plot(log_lrs[10:-5], losses[10:-5])\\n-                return log_lrs, losses    \\n-\\n-    def train(self, conf, epochs):\\n-        self.model.train()\\n-        running_loss = 0.            \\n-        for e in range(epochs):\\n-            print(\\\'epoch {} started\\\'.format(e))\\n-            if e == self.milestones[0]:\\n-                self.schedule_lr()\\n-            if e == self.milestones[1]:\\n-                self.schedule_lr()      \\n-            if e == self.milestones[2]:\\n-                self.schedule_lr()                                 \\n-            for imgs, labels in tqdm(iter(self.loader)):\\n-                imgs = imgs.to(conf.device)\\n-                labels = labels.to(conf.device)\\n-                self.optimizer.zero_grad()\\n-                embeddings = self.model(imgs)\\n-                thetas = self.head(embeddings, labels)\\n-                loss = conf.ce_loss(thetas, labels)\\n-                loss.backward()\\n-                running_loss += loss.item()\\n-                self.optimizer.step()\\n-                \\n-                if self.step % self.board_loss_every == 0 and self.step != 0:\\n-                    loss_board = running_loss / self.board_loss_every\\n-                    self.writer.add_scalar(\\\'train_loss\\\', loss_board, self.step)\\n-                    running_loss = 0.\\n-                \\n-                if self.step % self.evaluate_every == 0 and self.step != 0:\\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.agedb_30, self.agedb_30_issame)\\n-                    self.board_val(\\\'agedb_30\\\', accuracy, best_threshold, roc_curve_tensor)\\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.lfw, self.lfw_issame)\\n-                    self.board_val(\\\'lfw\\\', accuracy, best_threshold, roc_curve_tensor)\\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.cfp_fp, self.cfp_fp_issame)\\n-                    self.board_val(\\\'cfp_fp\\\', accuracy, best_threshold, roc_curve_tensor)\\n-                    self.model.train()\\n-                if self.step % self.save_every == 0 and self.step != 0:\\n-                    self.save_state(conf, accuracy)\\n-                    \\n-                self.step += 1\\n-                \\n-        self.save_state(conf, accuracy, to_save_folder=True, extra=\\\'final\\\')\\n-\\n-    def schedule_lr(self):\\n-        for params in self.optimizer.param_groups:                 \\n-            params[\\\'lr\\\'] /= 10\\n-        print(self.optimizer)\\n-    \\n-    def infer(self, conf, faces, target_embs, tta=False):\\n-        \\\'\\\'\\\'\\n-        faces : list of PIL Image\\n-        target_embs : [n, 512] computed embeddings of faces in facebank\\n-        names : recorded names of faces in facebank\\n-        tta : test time augmentation (hfilp, that\\\'s all)\\n-        \\\'\\\'\\\'\\n-        embs = []\\n-        for img in faces:\\n-            if tta:\\n-                mirror = trans.functional.hflip(img)\\n-                emb = self.model(conf.test_transform(img).to(conf.device).unsqueeze(0))\\n-                emb_mirror = self.model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\\n-                embs.append(l2_norm(emb + emb_mirror))\\n-            else:                        \\n-                embs.append(self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\\n-        source_embs = torch.cat(embs)\\n-        \\n-        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1,0).unsqueeze(0)\\n-        dist = torch.sum(torch.pow(diff, 2), dim=1)\\n-        minimum, min_idx = torch.min(dist, dim=1)\\n-        if float(minimum[0]) > 0.9:\\n-            min_idx = -1\\n-        return min_idx, minimum               \\n\\\\ No newline at end of file\\ndiff --git a/Pipfile b/Pipfile\\ndeleted file mode 100644\\nindex 9edb193..0000000\\n--- a/Pipfile\\n+++ /dev/null\\n@@ -1,32 +0,0 @@\\n-[[source]]\\n-url = "https://pypi.org/simple"\\n-verify_ssl = true\\n-name = "pypi"\\n-\\n-[packages]\\n-flask = "==1.1.1"\\n-scipy = "==1.1.0"\\n-matplotlib = "*"\\n-werkzeug = "==0.15.5"\\n-easydict = "==1.9"\\n-opencv-python = "*"\\n-gevent = "==21.8.0"\\n-mxnet-mkl = "==1.5.0"\\n-tqdm = "==4.35.0"\\n-bcolz = "==1.2.1"\\n-Pillow = "*"\\n-mtcnn-pytorch = "==1.0.2"\\n-mxnet = "==1.5.0"\\n-scikit-learn = "==0.21.3"\\n-tensorboardx = "==1.8"\\n-torch = "==1.10.2"\\n-numpy = "==1.16.1"\\n-greenlet = "==1.1.3"\\n-torchvision = "==0.4.1"\\n-imageio = "*"\\n-tensorflow = "==1.15"\\n-\\n-[dev-packages]\\n-\\n-[requires]\\n-python_version = "3.6"\\ndiff --git a/Pipfile.lock b/Pipfile.lock\\ndeleted file mode 100644\\nindex 9bb2547..0000000\\n--- a/Pipfile.lock\\n+++ /dev/null\\n@@ -1,1102 +0,0 @@\\n-{\\n-    "_meta": {\\n-        "hash": {\\n-            "sha256": "223161e10a8ee7514d3b29019bf148cf04324958428babf22504a82ff208e70a"\\n-        },\\n-        "pipfile-spec": 6,\\n-        "requires": {\\n-            "python_version": "3.6"\\n-        },\\n-        "sources": [\\n-            {\\n-                "name": "pypi",\\n-                "url": "https://pypi.org/simple",\\n-                "verify_ssl": true\\n-            }\\n-        ]\\n-    },\\n-    "default": {\\n-        "absl-py": {\\n-            "hashes": [\\n-                "sha256:0d3fe606adfa4f7db64792dd4c7aee4ee0c38ab75dfd353b7a83ed3e957fcb47",\\n-                "sha256:d2c244d01048ba476e7c080bd2c6df5e141d211de80223460d5b3b8a2a58433d"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==1.4.0"\\n-        },\\n-        "astor": {\\n-            "hashes": [\\n-                "sha256:070a54e890cefb5b3739d19f30f5a5ec840ffc9c50ffa7d23cc9fc1a38ebbfc5",\\n-                "sha256:6a6effda93f4e1ce9f618779b2dd1d9d84f1e32812c23a29b3fff6fd7f63fa5e"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3\\\'",\\n-            "version": "==0.8.1"\\n-        },\\n-        "bcolz": {\\n-            "hashes": [\\n-                "sha256:c017d09bb0cb5bbb07f2ae223a3f3638285be3b574cb328e91525b2880300bd1"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.2.1"\\n-        },\\n-        "cached-property": {\\n-            "hashes": [\\n-                "sha256:9fa5755838eecbb2d234c3aa390bd80fbd3ac6b6869109bfc1b499f7bd89a130",\\n-                "sha256:df4f613cf7ad9a588cc381aaf4a512d26265ecebd5eb9e1ba12f1319eb85a6a0"\\n-            ],\\n-            "markers": "python_version < \\\'3.8\\\'",\\n-            "version": "==1.5.2"\\n-        },\\n-        "certifi": {\\n-            "hashes": [\\n-                "sha256:35824b4c3a97115964b408844d64aa14db1cc518f6562e8d7261699d1350a9e3",\\n-                "sha256:4ad3232f5e926d6718ec31cfc1fcadfde020920e278684144551c91769c7bc18"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==2022.12.7"\\n-        },\\n-        "cffi": {\\n-            "hashes": [\\n-                "sha256:00a9ed42e88df81ffae7a8ab6d9356b371399b91dbdf0c3cb1e84c03a13aceb5",\\n-                "sha256:03425bdae262c76aad70202debd780501fabeaca237cdfddc008987c0e0f59ef",\\n-                "sha256:04ed324bda3cda42b9b695d51bb7d54b680b9719cfab04227cdd1e04e5de3104",\\n-                "sha256:0e2642fe3142e4cc4af0799748233ad6da94c62a8bec3a6648bf8ee68b1c7426",\\n-                "sha256:173379135477dc8cac4bc58f45db08ab45d228b3363adb7af79436135d028405",\\n-                "sha256:198caafb44239b60e252492445da556afafc7d1e3ab7a1fb3f0584ef6d742375",\\n-                "sha256:1e74c6b51a9ed6589199c787bf5f9875612ca4a8a0785fb2d4a84429badaf22a",\\n-                "sha256:2012c72d854c2d03e45d06ae57f40d78e5770d252f195b93f581acf3ba44496e",\\n-                "sha256:21157295583fe8943475029ed5abdcf71eb3911894724e360acff1d61c1d54bc",\\n-                "sha256:2470043b93ff09bf8fb1d46d1cb756ce6132c54826661a32d4e4d132e1977adf",\\n-                "sha256:285d29981935eb726a4399badae8f0ffdff4f5050eaa6d0cfc3f64b857b77185",\\n-                "sha256:30d78fbc8ebf9c92c9b7823ee18eb92f2e6ef79b45ac84db507f52fbe3ec4497",\\n-                "sha256:320dab6e7cb2eacdf0e658569d2575c4dad258c0fcc794f46215e1e39f90f2c3",\\n-                "sha256:33ab79603146aace82c2427da5ca6e58f2b3f2fb5da893ceac0c42218a40be35",\\n-                "sha256:3548db281cd7d2561c9ad9984681c95f7b0e38881201e157833a2342c30d5e8c",\\n-                "sha256:3799aecf2e17cf585d977b780ce79ff0dc9b78d799fc694221ce814c2c19db83",\\n-                "sha256:39d39875251ca8f612b6f33e6b1195af86d1b3e60086068be9cc053aa4376e21",\\n-                "sha256:3b926aa83d1edb5aa5b427b4053dc420ec295a08e40911296b9eb1b6170f6cca",\\n-                "sha256:3bcde07039e586f91b45c88f8583ea7cf7a0770df3a1649627bf598332cb6984",\\n-                "sha256:3d08afd128ddaa624a48cf2b859afef385b720bb4b43df214f85616922e6a5ac",\\n-                "sha256:3eb6971dcff08619f8d91607cfc726518b6fa2a9eba42856be181c6d0d9515fd",\\n-                "sha256:40f4774f5a9d4f5e344f31a32b5096977b5d48560c5592e2f3d2c4374bd543ee",\\n-                "sha256:4289fc34b2f5316fbb762d75362931e351941fa95fa18789191b33fc4cf9504a",\\n-                "sha256:470c103ae716238bbe698d67ad020e1db9d9dba34fa5a899b5e21577e6d52ed2",\\n-                "sha256:4f2c9f67e9821cad2e5f480bc8d83b8742896f1242dba247911072d4fa94c192",\\n-                "sha256:50a74364d85fd319352182ef59c5c790484a336f6db772c1a9231f1c3ed0cbd7",\\n-                "sha256:54a2db7b78338edd780e7ef7f9f6c442500fb0d41a5a4ea24fff1c929d5af585",\\n-                "sha256:5635bd9cb9731e6d4a1132a498dd34f764034a8ce60cef4f5319c0541159392f",\\n-                "sha256:59c0b02d0a6c384d453fece7566d1c7e6b7bae4fc5874ef2ef46d56776d61c9e",\\n-                "sha256:5d598b938678ebf3c67377cdd45e09d431369c3b1a5b331058c338e201f12b27",\\n-                "sha256:5df2768244d19ab7f60546d0c7c63ce1581f7af8b5de3eb3004b9b6fc8a9f84b",\\n-                "sha256:5ef34d190326c3b1f822a5b7a45f6c4535e2f47ed06fec77d3d799c450b2651e",\\n-                "sha256:6975a3fac6bc83c4a65c9f9fcab9e47019a11d3d2cf7f3c0d03431bf145a941e",\\n-                "sha256:6c9a799e985904922a4d207a94eae35c78ebae90e128f0c4e521ce339396be9d",\\n-                "sha256:70df4e3b545a17496c9b3f41f5115e69a4f2e77e94e1d2a8e1070bc0c38c8a3c",\\n-                "sha256:7473e861101c9e72452f9bf8acb984947aa1661a7704553a9f6e4baa5ba64415",\\n-                "sha256:8102eaf27e1e448db915d08afa8b41d6c7ca7a04b7d73af6514df10a3e74bd82",\\n-                "sha256:87c450779d0914f2861b8526e035c5e6da0a3199d8f1add1a665e1cbc6fc6d02",\\n-                "sha256:8b7ee99e510d7b66cdb6c593f21c043c248537a32e0bedf02e01e9553a172314",\\n-                "sha256:91fc98adde3d7881af9b59ed0294046f3806221863722ba7d8d120c575314325",\\n-                "sha256:94411f22c3985acaec6f83c6df553f2dbe17b698cc7f8ae751ff2237d96b9e3c",\\n-                "sha256:98d85c6a2bef81588d9227dde12db8a7f47f639f4a17c9ae08e773aa9c697bf3",\\n-                "sha256:9ad5db27f9cabae298d151c85cf2bad1d359a1b9c686a275df03385758e2f914",\\n-                "sha256:a0b71b1b8fbf2b96e41c4d990244165e2c9be83d54962a9a1d118fd8657d2045",\\n-                "sha256:a0f100c8912c114ff53e1202d0078b425bee3649ae34d7b070e9697f93c5d52d",\\n-                "sha256:a591fe9e525846e4d154205572a029f653ada1a78b93697f3b5a8f1f2bc055b9",\\n-                "sha256:a5c84c68147988265e60416b57fc83425a78058853509c1b0629c180094904a5",\\n-                "sha256:a66d3508133af6e8548451b25058d5812812ec3798c886bf38ed24a98216fab2",\\n-                "sha256:a8c4917bd7ad33e8eb21e9a5bbba979b49d9a97acb3a803092cbc1133e20343c",\\n-                "sha256:b3bbeb01c2b273cca1e1e0c5df57f12dce9a4dd331b4fa1635b8bec26350bde3",\\n-                "sha256:cba9d6b9a7d64d4bd46167096fc9d2f835e25d7e4c121fb2ddfc6528fb0413b2",\\n-                "sha256:cc4d65aeeaa04136a12677d3dd0b1c0c94dc43abac5860ab33cceb42b801c1e8",\\n-                "sha256:ce4bcc037df4fc5e3d184794f27bdaab018943698f4ca31630bc7f84a7b69c6d",\\n-                "sha256:cec7d9412a9102bdc577382c3929b337320c4c4c4849f2c5cdd14d7368c5562d",\\n-                "sha256:d400bfb9a37b1351253cb402671cea7e89bdecc294e8016a707f6d1d8ac934f9",\\n-                "sha256:d61f4695e6c866a23a21acab0509af1cdfd2c013cf256bbf5b6b5e2695827162",\\n-                "sha256:db0fbb9c62743ce59a9ff687eb5f4afbe77e5e8403d6697f7446e5f609976f76",\\n-                "sha256:dd86c085fae2efd48ac91dd7ccffcfc0571387fe1193d33b6394db7ef31fe2a4",\\n-                "sha256:e00b098126fd45523dd056d2efba6c5a63b71ffe9f2bbe1a4fe1716e1d0c331e",\\n-                "sha256:e229a521186c75c8ad9490854fd8bbdd9a0c9aa3a524326b55be83b54d4e0ad9",\\n-                "sha256:e263d77ee3dd201c3a142934a086a4450861778baaeeb45db4591ef65550b0a6",\\n-                "sha256:ed9cb427ba5504c1dc15ede7d516b84757c3e3d7868ccc85121d9310d27eed0b",\\n-                "sha256:fa6693661a4c91757f4412306191b6dc88c1703f780c8234035eac011922bc01",\\n-                "sha256:fcd131dd944808b5bdb38e6f5b53013c5aa4f334c5cad0c72742f6eba4b73db0"\\n-            ],\\n-            "markers": "platform_python_implementation == \\\'CPython\\\' and sys_platform == \\\'win32\\\'",\\n-            "version": "==1.15.1"\\n-        },\\n-        "chardet": {\\n-            "hashes": [\\n-                "sha256:84ab92ed1c4d4f16916e05906b6b75a6c0fb5db821cc65e70cbd64a3e2a5eaae",\\n-                "sha256:fc323ffcaeaed0e0a02bf4d117757b98aed530d9ed4531e3e15460124c106691"\\n-            ],\\n-            "version": "==3.0.4"\\n-        },\\n-        "click": {\\n-            "hashes": [\\n-                "sha256:6a7a62563bbfabfda3a38f3023a1db4a35978c0abd76f6c9605ecd6554d6d9b1",\\n-                "sha256:8458d7b1287c5fb128c90e23381cf99dcde74beaf6c7ff6384ce84d6fe090adb"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==8.0.4"\\n-        },\\n-        "colorama": {\\n-            "hashes": [\\n-                "sha256:854bf444933e37f5824ae7bfc1e98d5bce2ebe4160d46b5edf346a89358e99da",\\n-                "sha256:e6c6b4334fc50988a639d9b98aa429a0b57da6e17b9a44f0451f930b6967b7a4"\\n-            ],\\n-            "markers": "platform_system == \\\'Windows\\\'",\\n-            "version": "==0.4.5"\\n-        },\\n-        "cycler": {\\n-            "hashes": [\\n-                "sha256:3a27e95f763a428a739d2add979fa7494c912a32c17c4c38c4d5f082cad165a3",\\n-                "sha256:9c87405839a19696e837b3b818fed3f5f69f16f1eec1a1ad77e043dcea9c772f"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==0.11.0"\\n-        },\\n-        "dataclasses": {\\n-            "hashes": [\\n-                "sha256:0201d89fa866f68c8ebd9d08ee6ff50c0b255f8ec63a71c16fda7af82bb887bf",\\n-                "sha256:8479067f342acf957dc82ec415d355ab5edb7e7646b90dc6e2fd1d96ad084c97"\\n-            ],\\n-            "markers": "python_version < \\\'3.7\\\'",\\n-            "version": "==0.8"\\n-        },\\n-        "easydict": {\\n-            "hashes": [\\n-                "sha256:3f3f0dab07c299f0f4df032db1f388d985bb57fa4c5be30acd25c5f9a516883b"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.9"\\n-        },\\n-        "flask": {\\n-            "hashes": [\\n-                "sha256:13f9f196f330c7c2c5d7a5cf91af894110ca0215ac051b5844701f2bfd934d52",\\n-                "sha256:45eb5a6fd193d6cf7e0cf5d8a5b31f83d5faae0293695626f539a823e93b13f6"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.1.1"\\n-        },\\n-        "gast": {\\n-            "hashes": [\\n-                "sha256:fe939df4583692f0512161ec1c880e0a10e71e6a232da045ab8edd3756fbadf0"\\n-            ],\\n-            "version": "==0.2.2"\\n-        },\\n-        "gevent": {\\n-            "hashes": [\\n-                "sha256:02d1e8ca227d0ab0b7917fd7e411f9a534475e0a41fb6f434e9264b20155201a",\\n-                "sha256:0c7b4763514fec74c9fe6ad10c3de62d8fe7b926d520b1e35eb6887181b954ff",\\n-                "sha256:1c9c87b15f792af80edc950a83ab8ef4f3ba3889712211c2c42740ddb57b5492",\\n-                "sha256:23077d87d1589ac141c22923fd76853d2cc5b7e3c5e1f1f9cdf6ff23bc9790fc",\\n-                "sha256:37a469a99e6000b42dd0b9bbd9d716dbd66cdc6e5738f136f6a266c29b90ee99",\\n-                "sha256:3b600145dc0c5b39c6f89c2e91ec6c55eb0dd52dc8148228479ca42cded358e4",\\n-                "sha256:3f5ba654bdd3c774079b553fef535ede5b52c7abd224cb235a15da90ae36251b",\\n-                "sha256:43e93e1a4738c922a2416baf33f0afb0a20b22d3dba886720bc037cd02a98575",\\n-                "sha256:473f918bdf7d2096e391f66bd8ce1e969639aa235e710aaf750a37774bb585bd",\\n-                "sha256:4c94d27be9f0439b28eb8bd0f879e6142918c62092fda7fb96b6d06f01886b94",\\n-                "sha256:55ede95f41b74e7506fab293ad04cc7fc2b6f662b42281e9f2d668ad3817b574",\\n-                "sha256:6cad37a55e904879beef2a7e7c57c57d62fde2331fef1bec7f2b2a7ef14da6a2",\\n-                "sha256:72d4c2a8e65bbc702db76456841c7ddd6de2d9ab544a24aa74ad9c2b6411a269",\\n-                "sha256:75c29ed5148c916021d39d2fac90ccc0e19adf854626a34eaee012aa6b1fcb67",\\n-                "sha256:84e1af2dfb4ea9495cb914b00b6303ca0d54bf0a92e688a17e60f6b033873df2",\\n-                "sha256:8d8655ce581368b7e1ab42c8a3a166c0b43ea04e59970efbade9448864585e99",\\n-                "sha256:90131877d3ce1a05da1b718631860815b89ff44e93c42d168c9c9e8893b26318",\\n-                "sha256:9d46bea8644048ceac5737950c08fc89c37a66c34a56a6c9e3648726e60cb767",\\n-                "sha256:a8656d6e02bf47d7fa47728cf7a7cbf408f77ef1fad12afd9e0e3246c5de1707",\\n-                "sha256:aaf1451cd0d9c32f65a50e461084a0540be52b8ea05c18669c95b42e1f71592a",\\n-                "sha256:afc877ff4f277d0e51a1206d748fdab8c1e0256f7a05e1b1067abbed71c64da9",\\n-                "sha256:b10c3326edb76ec3049646dc5131608d6d3733b5adfc75d34852028ecc67c52c",\\n-                "sha256:ceec7c5f15fb2f9b767b194daa55246830db6c7c3c2f0b1c7e9e90cb4d01f3f9",\\n-                "sha256:e00dc0450f79253b7a3a7f2a28e6ca959c8d0d47c0f9fa2c57894c7974d5965f",\\n-                "sha256:e91632fdcf1c9a33e97e35f96edcbdf0b10e36cf53b58caa946dca4836bb688c",\\n-                "sha256:f39d5defda9443b5fb99a185050e94782fe7ac38f34f751b491142216ad23bc7"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==21.8.0"\\n-        },\\n-        "google-pasta": {\\n-            "hashes": [\\n-                "sha256:4612951da876b1a10fe3960d7226f0c7682cf901e16ac06e473b267a5afa8954",\\n-                "sha256:b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed",\\n-                "sha256:c9f2c8dfc8f96d0d5808299920721be30c9eec37f2389f28904f454565c8a16e"\\n-            ],\\n-            "version": "==0.2.0"\\n-        },\\n-        "graphviz": {\\n-            "hashes": [\\n-                "sha256:4958a19cbd8461757a08db308a4a15c3d586660417e1e364f0107d2fe481689f",\\n-                "sha256:7caa53f0b0be42c5f2eaa3f3d71dcc863b15bacceb5d531c2ad7519e1980ff82"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3\\\'",\\n-            "version": "==0.8.4"\\n-        },\\n-        "greenlet": {\\n-            "hashes": [\\n-                "sha256:0118817c9341ef2b0f75f5af79ac377e4da6ff637e5ee4ac91802c0e379dadb4",\\n-                "sha256:048d2bed76c2aa6de7af500ae0ea51dd2267aec0e0f2a436981159053d0bc7cc",\\n-                "sha256:07c58e169bbe1e87b8bbf15a5c1b779a7616df9fd3e61cadc9d691740015b4f8",\\n-                "sha256:095a980288fe05adf3d002fbb180c99bdcf0f930e220aa66fcd56e7914a38202",\\n-                "sha256:0b181e9aa6cb2f5ec0cacc8cee6e5a3093416c841ba32c185c30c160487f0380",\\n-                "sha256:1626185d938d7381631e48e6f7713e8d4b964be246073e1a1d15c2f061ac9f08",\\n-                "sha256:184416e481295832350a4bf731ba619a92f5689bf5d0fa4341e98b98b1265bd7",\\n-                "sha256:1dd51d2650e70c6c4af37f454737bf4a11e568945b27f74b471e8e2a9fd21268",\\n-                "sha256:1ec2779774d8e42ed0440cf8bc55540175187e8e934f2be25199bf4ed948cd9e",\\n-                "sha256:2cf45e339cabea16c07586306a31cfcc5a3b5e1626d365714d283732afed6809",\\n-                "sha256:2fb0aa7f6996879551fd67461d5d3ab0c3c0245da98be90c89fcb7a18d437403",\\n-                "sha256:44b4817c34c9272c65550b788913620f1fdc80362b209bc9d7dd2f40d8793080",\\n-                "sha256:466ce0928e33421ee84ae04c4ac6f253a3a3e6b8d600a79bd43fd4403e0a7a76",\\n-                "sha256:4f166b4aca8d7d489e82d74627a7069ab34211ef5ebb57c300ec4b9337b60fc0",\\n-                "sha256:510c3b15587afce9800198b4b142202b323bf4b4b5f9d6c79cb9a35e5e3c30d2",\\n-                "sha256:5b756e6730ea59b2745072e28ad27f4c837084688e6a6b3633c8b1e509e6ae0e",\\n-                "sha256:5fbe1ab72b998ca77ceabbae63a9b2e2dc2d963f4299b9b278252ddba142d3f1",\\n-                "sha256:6200a11f003ec26815f7e3d2ded01b43a3810be3528dd760d2f1fa777490c3cd",\\n-                "sha256:65ad1a7a463a2a6f863661329a944a5802c7129f7ad33583dcc11069c17e622c",\\n-                "sha256:694ffa7144fa5cc526c8f4512665003a39fa09ef00d19bbca5c8d3406db72fbe",\\n-                "sha256:6f5d4b2280ceea76c55c893827961ed0a6eadd5a584a7c4e6e6dd7bc10dfdd96",\\n-                "sha256:7532a46505470be30cbf1dbadb20379fb481244f1ca54207d7df3bf0bbab6a20",\\n-                "sha256:76a53bfa10b367ee734b95988bd82a9a5f0038a25030f9f23bbbc005010ca600",\\n-                "sha256:77e41db75f9958f2083e03e9dd39da12247b3430c92267df3af77c83d8ff9eed",\\n-                "sha256:7a43bbfa9b6cfdfaeefbd91038dde65ea2c421dc387ed171613df340650874f2",\\n-                "sha256:7b41d19c0cfe5c259fe6c539fd75051cd39a5d33d05482f885faf43f7f5e7d26",\\n-                "sha256:7c5227963409551ae4a6938beb70d56bf1918c554a287d3da6853526212fbe0a",\\n-                "sha256:870a48007872d12e95a996fca3c03a64290d3ea2e61076aa35d3b253cf34cd32",\\n-                "sha256:88b04e12c9b041a1e0bcb886fec709c488192638a9a7a3677513ac6ba81d8e79",\\n-                "sha256:8c287ae7ac921dfde88b1c125bd9590b7ec3c900c2d3db5197f1286e144e712b",\\n-                "sha256:903fa5716b8fbb21019268b44f73f3748c41d1a30d71b4a49c84b642c2fed5fa",\\n-                "sha256:9537e4baf0db67f382eb29255a03154fcd4984638303ff9baaa738b10371fa57",\\n-                "sha256:9951dcbd37850da32b2cb6e391f621c1ee456191c6ae5528af4a34afe357c30e",\\n-                "sha256:9b2f7d0408ddeb8ea1fd43d3db79a8cefaccadd2a812f021333b338ed6b10aba",\\n-                "sha256:9c88e134d51d5e82315a7c32b914a58751b7353eb5268dbd02eabf020b4c4700",\\n-                "sha256:9fae214f6c43cd47f7bef98c56919b9222481e833be2915f6857a1e9e8a15318",\\n-                "sha256:a3a669f11289a8995d24fbfc0e63f8289dd03c9aaa0cc8f1eab31d18ca61a382",\\n-                "sha256:aa741c1a8a8cc25eb3a3a01a62bdb5095a773d8c6a86470bde7f607a447e7905",\\n-                "sha256:b0877a9a2129a2c56a2eae2da016743db7d9d6a05d5e1c198f1b7808c602a30e",\\n-                "sha256:bcb6c6dd1d6be6d38d6db283747d07fda089ff8c559a835236560a4410340455",\\n-                "sha256:caff52cb5cd7626872d9696aee5b794abe172804beb7db52eed1fd5824b63910",\\n-                "sha256:cbc1eb55342cbac8f7ec159088d54e2cfdd5ddf61c87b8bbe682d113789331b2",\\n-                "sha256:cd16a89efe3a003029c87ff19e9fba635864e064da646bc749fc1908a4af18f3",\\n-                "sha256:ce5b64dfe8d0cca407d88b0ee619d80d4215a2612c1af8c98a92180e7109f4b5",\\n-                "sha256:d58a5a71c4c37354f9e0c24c9c8321f0185f6945ef027460b809f4bb474bfe41",\\n-                "sha256:db41f3845eb579b544c962864cce2c2a0257fe30f0f1e18e51b1e8cbb4e0ac6d",\\n-                "sha256:db5b25265010a1b3dca6a174a443a0ed4c4ab12d5e2883a11c97d6e6d59b12f9",\\n-                "sha256:dd0404d154084a371e6d2bafc787201612a1359c2dee688ae334f9118aa0bf47",\\n-                "sha256:de431765bd5fe62119e0bc6bc6e7b17ac53017ae1782acf88fcf6b7eae475a49",\\n-                "sha256:df02fdec0c533301497acb0bc0f27f479a3a63dcdc3a099ae33a902857f07477",\\n-                "sha256:e8533f5111704d75de3139bf0b8136d3a6c1642c55c067866fa0a51c2155ee33",\\n-                "sha256:f2f908239b7098799b8845e5936c2ccb91d8c2323be02e82f8dcb4a80dcf4a25",\\n-                "sha256:f8bfd36f368efe0ab2a6aa3db7f14598aac454b06849fb633b762ddbede1db90",\\n-                "sha256:ffe73f9e7aea404722058405ff24041e59d31ca23d1da0895af48050a07b6932"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.1.3"\\n-        },\\n-        "grpcio": {\\n-            "hashes": [\\n-                "sha256:0802b080b6b8603a065e505ce83190b6a06229b9a74d0a1681175271ac84fe12",\\n-                "sha256:0bfb637344442b273b698ff425d735a5d806ca8715f988875ad669277fb9b1e6",\\n-                "sha256:104b555e1cb2e0614f05c1def24eb8bb06f1277460058aa0f9c9e6a1018716da",\\n-                "sha256:110028e0b9c346230ae69b8a6d8b25d4d43bfd37bda61a8ec46486da1e781dcb",\\n-                "sha256:13c3b69f8efb214a54f48e8dd1e235a4d8d22fa985f32a9b2844373993c5a605",\\n-                "sha256:199526758f6f8d35a596c610f33ea76faae65ec175dc109e8481ea3404d8527c",\\n-                "sha256:1e2dc213fe71566efbf9a5d704c665ff4b1760a88d37f8533b19ca92776070c9",\\n-                "sha256:1fea4cb4368dd0467eb2d208e2d5e3c4f0be28fe33965d45ac9e1d562be67a8c",\\n-                "sha256:26ed6d07f91ce8aeb4697b7e71d930355282ec80acb7a488f4030a3a75c2f7a8",\\n-                "sha256:2bb1df2920a4968f0c09041b49e591df96f2e6f801f15eed3821c1f16a12f1a8",\\n-                "sha256:2d99fb56c7e836f165828719c3695d3d27ac70b103ab52226f7a7c237e4a3928",\\n-                "sha256:2f185b8c5130663c455f6542906ce99f046608e94950c8b354aa22462c202c2d",\\n-                "sha256:3463256399158e9abf115620994e968db8f003224c36bda0d14570eab8a44cfc",\\n-                "sha256:3d3225d477663c27b9051546a32551babd1ccb80192905e08340264deccc975b",\\n-                "sha256:3f52ef5ba7a8bc334daa87675838d6dafda7d8a116a72b567b8351e561ace498",\\n-                "sha256:47ace91631176efa575c7a34d5004286288f1af1e9de2ff380d1433f241aeed4",\\n-                "sha256:514392a30a275f4f719c2e05ea969c239e5f03eec4a25965852c7582073d8b94",\\n-                "sha256:550b08dfa938e30ffbc1652193cf2877906aa6242d6ba9f61318dc87fcecee63",\\n-                "sha256:5571cb828d694b34a7c75484722803e13a2f5e4760e47ae32fb077c83d0c9b2c",\\n-                "sha256:5792943481d4270b3e9a4700af0eea86e7183f4d3c250a46e0b357949cc09411",\\n-                "sha256:665141b3a97b7d22978c8d2ba0c0af7f67bd6d7a56889c5c0aa715d04009b518",\\n-                "sha256:6affa7e685edbb7421f942296eb618359362e89e641bcf46779c6ec7b944d275",\\n-                "sha256:6f693da8ffd2486c354c90ba5a8ca0f4c50bfb8853495501884cadc152551360",\\n-                "sha256:855c125e8cd1c3ab09a239689c940d26c30680edf2edf87c3c1543bd8633cc8f",\\n-                "sha256:894c5f02c25c83c2320310521a82978b4e252ee18b99a5c4c564d73daeb5c1de",\\n-                "sha256:8d130f666463e4d09a63ff033a6c5cd032867fd51a0db4660c18106aa352be3a",\\n-                "sha256:90e5da224c6b9b23658adf6f36de6f435ef7dbcc9c5c12330314d70d6f8de1f7",\\n-                "sha256:9dc08baa1b28749e90428aaa16e038e8c389d8ccb843ddc0dc8b95231640b432",\\n-                "sha256:a760ef87fde9a8f2761c7ad8ccf617fc590547ed743a9207fe7e367496164c60",\\n-                "sha256:abee7dd82443b2cd128004e053b263a6d7256d570df80956a974634b8c5bc121",\\n-                "sha256:b860e13c112bb9cb44007ef02853a19397d915b31c42dfa18570448bdc0a6245",\\n-                "sha256:b8768daa636e0fa48fec75517bea65ce8fdaca0066dc411fc0a2290d92032f91",\\n-                "sha256:b8ec07dcc1cbd77b8c09dfc0ce6274920cb7b09cc04013110971d95d8bcc0bbf",\\n-                "sha256:c19d6f337860f382ceaa35c5acab439d84c5ffaa8baba36df1f83ab6b9ac4bd3",\\n-                "sha256:c92b5ef64cd5a0c6aea82dd6862fdb8a1562510d537ea3c356a7fe60db7021af",\\n-                "sha256:cebeed160466a1e254eb75e7e1bbeeb1359c50b33a1b8f3b2241a8b8dc9bd216",\\n-                "sha256:d7b1a3a75c34ab39c9df73aa9fecc519dc1035e588a41af19f39b1298a283a57",\\n-                "sha256:d7ec6b04875a5065d04ad86cd2678ca6431dec868c01d731b8233f3de155bfdf",\\n-                "sha256:dc00681d546cae66e9d54451f650fe140f9e1aca2dc4f8c9686cfaa4dd5d680b",\\n-                "sha256:e48595440dc86e13245aec7c096238db12b659e5ae6078aecf99d66befb77678",\\n-                "sha256:e69a5907a2a4cf0011ff46205b6bff8f56b8391436acc3c66b70ce8519578d7e",\\n-                "sha256:ec2dd9f7ab0c809af6b2c65ed31c3cbef2ca9695f7f4d49866ec4707e7836890",\\n-                "sha256:f1d2cd5b1adecbcffee4ad6613f100e0b583ae2e253d2f8f685e7770ec72d622",\\n-                "sha256:f3c0d0995a0cd8c7198cb49b8ce98b4936c5a70109f9246c58e69c898e4f7329",\\n-                "sha256:f5697e4ab90a41a6a1202c1a3ec268a0d69f1cd127a4940d2b2521a0fbc1277b",\\n-                "sha256:f6afd1f4b5e0ec320fb2b027a646944fee8b58ba00fb43d081968f77d1a6e925"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==1.48.2"\\n-        },\\n-        "h5py": {\\n-            "hashes": [\\n-                "sha256:02c391fdb980762a1cc03a4bcaecd03dc463994a9a63a02264830114a96e111f",\\n-                "sha256:1cd367f89a5441236bdbb795e9fb9a9e3424929c00b4a54254ca760437f83d69",\\n-                "sha256:1cdfd1c5449ca1329d152f0b66830e93226ebce4f5e07dd8dc16bfc2b1a49d7b",\\n-                "sha256:1e2516f190652beedcb8c7acfa1c6fa92d99b42331cbef5e5c7ec2d65b0fc3c2",\\n-                "sha256:236ac8d943be30b617ab615c3d4a4bf4a438add2be87e54af3687ab721a18fac",\\n-                "sha256:2e37352ddfcf9d77a2a47f7c8f7e125c6d20cc06c2995edeb7be222d4e152636",\\n-                "sha256:80c623be10479e81b64fa713b7ed4c0bbe9f02e8e7d2a2e5382336087b615ce4",\\n-                "sha256:ba71f6229d2013fbb606476ecc29c6223fc16b244d35fcd8566ad9dbaf910857",\\n-                "sha256:cb74df83709d6d03d11e60b9480812f58da34f194beafa8c8314dbbeeedfe0a6",\\n-                "sha256:dccb89358bc84abcd711363c3e138f9f4eccfdf866f2139a8e72308328765b2c",\\n-                "sha256:e33f61d3eb862614c0f273a1f993a64dc2f093e1a3094932c50ada9d2db2170f",\\n-                "sha256:f89a3dae38843ffa49d17a31a3509a8129e9b46ece602a0138e1ed79e685c361",\\n-                "sha256:fea05349f63625a8fb808e57e42bb4c76930cf5d50ac58b678c52f913a48a89b"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==3.1.0"\\n-        },\\n-        "idna": {\\n-            "hashes": [\\n-                "sha256:2c6a5de3089009e3da7c5dde64a141dbc8551d5b7f6cf4ed7c2568d0cc520a8f",\\n-                "sha256:8c7309c718f94b3a625cb648ace320157ad16ff131ae0af362c9f21b80ef6ec4"\\n-            ],\\n-            "version": "==2.6"\\n-        },\\n-        "imageio": {\\n-            "hashes": [\\n-                "sha256:d0d7abb4e5c4044c06fc573233489c4a25582698f93ca94f7bd70b6f4ab172ec",\\n-                "sha256:d5e553c8e8fd01ac27094df32bf020b553f640a71b042c960c8341bf7d313a8e"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==2.15.0"\\n-        },\\n-        "importlib-metadata": {\\n-            "hashes": [\\n-                "sha256:65a9576a5b2d58ca44d133c42a241905cc45e34d2c06fd5ba2bafa221e5d7b5e",\\n-                "sha256:766abffff765960fcc18003801f7044eb6755ffae4521c8e8ce8e83b9c9b0668"\\n-            ],\\n-            "markers": "python_version < \\\'3.8\\\'",\\n-            "version": "==4.8.3"\\n-        },\\n-        "itsdangerous": {\\n-            "hashes": [\\n-                "sha256:5174094b9637652bdb841a3029700391451bd092ba3db90600dea710ba28e97c",\\n-                "sha256:9e724d68fc22902a1435351f84c3fb8623f303fffcc566a4cb952df8c572cff0"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==2.0.1"\\n-        },\\n-        "jinja2": {\\n-            "hashes": [\\n-                "sha256:077ce6014f7b40d03b47d1f1ca4b0fc8328a692bd284016f806ed0eaca390ad8",\\n-                "sha256:611bb273cd68f3b993fabdc4064fc858c5b47a973cb5aa7999ec1ba405c87cd7"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==3.0.3"\\n-        },\\n-        "joblib": {\\n-            "hashes": [\\n-                "sha256:301f0375f49586a7effee3f6348c419d5765fca1c750186b20690a0d90b82900",\\n-                "sha256:f9d6c3cdf2a7778e9058e10e9dba028e47771a1a355e5768f46704bf05342eba"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==1.1.1"\\n-        },\\n-        "keras-applications": {\\n-            "hashes": [\\n-                "sha256:5579f9a12bcde9748f4a12233925a59b93b73ae6947409ff34aa2ba258189fe5",\\n-                "sha256:df4323692b8c1174af821bf906f1e442e63fa7589bf0f1230a0b6bdc5a810c95"\\n-            ],\\n-            "version": "==1.0.8"\\n-        },\\n-        "keras-preprocessing": {\\n-            "hashes": [\\n-                "sha256:7b82029b130ff61cc99b55f3bd27427df4838576838c5b2f65940e4fcec99a7b",\\n-                "sha256:add82567c50c8bc648c14195bf544a5ce7c1f76761536956c3d2978970179ef3"\\n-            ],\\n-            "version": "==1.1.2"\\n-        },\\n-        "kiwisolver": {\\n-            "hashes": [\\n-                "sha256:0cd53f403202159b44528498de18f9285b04482bab2a6fc3f5dd8dbb9352e30d",\\n-                "sha256:1e1bc12fb773a7b2ffdeb8380609f4f8064777877b2225dec3da711b421fda31",\\n-                "sha256:225e2e18f271e0ed8157d7f4518ffbf99b9450fca398d561eb5c4a87d0986dd9",\\n-                "sha256:232c9e11fd7ac3a470d65cd67e4359eee155ec57e822e5220322d7b2ac84fbf0",\\n-                "sha256:24cc411232d14c8abafbd0dddb83e1a4f54d77770b53db72edcfe1d611b3bf11",\\n-                "sha256:31dfd2ac56edc0ff9ac295193eeaea1c0c923c0355bf948fbd99ed6018010b72",\\n-                "sha256:33449715e0101e4d34f64990352bce4095c8bf13bed1b390773fc0a7295967b3",\\n-                "sha256:401a2e9afa8588589775fe34fc22d918ae839aaaf0c0e96441c0fdbce6d8ebe6",\\n-                "sha256:44a62e24d9b01ba94ae7a4a6c3fb215dc4af1dde817e7498d901e229aaf50e4e",\\n-                "sha256:50af681a36b2a1dee1d3c169ade9fdc59207d3c31e522519181e12f1b3ba7000",\\n-                "sha256:563c649cfdef27d081c84e72a03b48ea9408c16657500c312575ae9d9f7bc1c3",\\n-                "sha256:5989db3b3b34b76c09253deeaf7fbc2707616f130e166996606c284395da3f18",\\n-                "sha256:5a7a7dbff17e66fac9142ae2ecafb719393aaee6a3768c9de2fd425c63b53e21",\\n-                "sha256:5c3e6455341008a054cccee8c5d24481bcfe1acdbc9add30aa95798e95c65621",\\n-                "sha256:5f6ccd3dd0b9739edcf407514016108e2280769c73a85b9e59aa390046dbf08b",\\n-                "sha256:6d9d8d9b31aa8c2d80a690693aebd8b5e2b7a45ab065bb78f1609995d2c79240",\\n-                "sha256:72c99e39d005b793fb7d3d4e660aed6b6281b502e8c1eaf8ee8346023c8e03bc",\\n-                "sha256:78751b33595f7f9511952e7e60ce858c6d64db2e062afb325985ddbd34b5c131",\\n-                "sha256:792e69140828babe9649de583e1a03a0f2ff39918a71782c76b3c683a67c6dfd",\\n-                "sha256:834ee27348c4aefc20b479335fd422a2c69db55f7d9ab61721ac8cd83eb78882",\\n-                "sha256:8be8d84b7d4f2ba4ffff3665bcd0211318aa632395a1a41553250484a871d454",\\n-                "sha256:950a199911a8d94683a6b10321f9345d5a3a8433ec58b217ace979e18f16e248",\\n-                "sha256:a357fd4f15ee49b4a98b44ec23a34a95f1e00292a139d6015c11f55774ef10de",\\n-                "sha256:a53d27d0c2a0ebd07e395e56a1fbdf75ffedc4a05943daf472af163413ce9598",\\n-                "sha256:acef3d59d47dd85ecf909c359d0fd2c81ed33bdff70216d3956b463e12c38a54",\\n-                "sha256:b38694dcdac990a743aa654037ff1188c7a9801ac3ccc548d3341014bc5ca278",\\n-                "sha256:b9edd0110a77fc321ab090aaa1cfcaba1d8499850a12848b81be2222eab648f6",\\n-                "sha256:c08e95114951dc2090c4a630c2385bef681cacf12636fb0241accdc6b303fd81",\\n-                "sha256:c5518d51a0735b1e6cee1fdce66359f8d2b59c3ca85dc2b0813a8aa86818a030",\\n-                "sha256:c8fd0f1ae9d92b42854b2979024d7597685ce4ada367172ed7c09edf2cef9cb8",\\n-                "sha256:ca3820eb7f7faf7f0aa88de0e54681bddcb46e485beb844fcecbcd1c8bd01689",\\n-                "sha256:cf8b574c7b9aa060c62116d4181f3a1a4e821b2ec5cbfe3775809474113748d4",\\n-                "sha256:d3155d828dec1d43283bd24d3d3e0d9c7c350cdfcc0bd06c0ad1209c1bbc36d0",\\n-                "sha256:d6563ccd46b645e966b400bb8a95d3457ca6cf3bba1e908f9e0927901dfebeb1",\\n-                "sha256:ef6eefcf3944e75508cdfa513c06cf80bafd7d179e14c1334ebdca9ebb8c2c66",\\n-                "sha256:f8d6f8db88049a699817fd9178782867bf22283e3813064302ac59f61d95be05",\\n-                "sha256:fd34fbbfbc40628200730bc1febe30631347103fc8d3d4fa012c21ab9c11eca9"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==1.3.1"\\n-        },\\n-        "markdown": {\\n-            "hashes": [\\n-                "sha256:cbb516f16218e643d8e0a95b309f77eb118cb138d39a4f27851e6a63581db874",\\n-                "sha256:f5da449a6e1c989a4cea2631aa8ee67caa5a2ef855d551c88f9e309f4634c621"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==3.3.7"\\n-        },\\n-        "markupsafe": {\\n-            "hashes": [\\n-                "sha256:01a9b8ea66f1658938f65b93a85ebe8bc016e6769611be228d797c9d998dd298",\\n-                "sha256:023cb26ec21ece8dc3907c0e8320058b2e0cb3c55cf9564da612bc325bed5e64",\\n-                "sha256:0446679737af14f45767963a1a9ef7620189912317d095f2d9ffa183a4d25d2b",\\n-                "sha256:04635854b943835a6ea959e948d19dcd311762c5c0c6e1f0e16ee57022669194",\\n-                "sha256:0717a7390a68be14b8c793ba258e075c6f4ca819f15edfc2a3a027c823718567",\\n-                "sha256:0955295dd5eec6cb6cc2fe1698f4c6d84af2e92de33fbcac4111913cd100a6ff",\\n-                "sha256:0d4b31cc67ab36e3392bbf3862cfbadac3db12bdd8b02a2731f509ed5b829724",\\n-                "sha256:10f82115e21dc0dfec9ab5c0223652f7197feb168c940f3ef61563fc2d6beb74",\\n-                "sha256:168cd0a3642de83558a5153c8bd34f175a9a6e7f6dc6384b9655d2697312a646",\\n-                "sha256:1d609f577dc6e1aa17d746f8bd3c31aa4d258f4070d61b2aa5c4166c1539de35",\\n-                "sha256:1f2ade76b9903f39aa442b4aadd2177decb66525062db244b35d71d0ee8599b6",\\n-                "sha256:20dca64a3ef2d6e4d5d615a3fd418ad3bde77a47ec8a23d984a12b5b4c74491a",\\n-                "sha256:2a7d351cbd8cfeb19ca00de495e224dea7e7d919659c2841bbb7f420ad03e2d6",\\n-                "sha256:2d7d807855b419fc2ed3e631034685db6079889a1f01d5d9dac950f764da3dad",\\n-                "sha256:2ef54abee730b502252bcdf31b10dacb0a416229b72c18b19e24a4509f273d26",\\n-                "sha256:36bc903cbb393720fad60fc28c10de6acf10dc6cc883f3e24ee4012371399a38",\\n-                "sha256:37205cac2a79194e3750b0af2a5720d95f786a55ce7df90c3af697bfa100eaac",\\n-                "sha256:3c112550557578c26af18a1ccc9e090bfe03832ae994343cfdacd287db6a6ae7",\\n-                "sha256:3dd007d54ee88b46be476e293f48c85048603f5f516008bee124ddd891398ed6",\\n-                "sha256:4296f2b1ce8c86a6aea78613c34bb1a672ea0e3de9c6ba08a960efe0b0a09047",\\n-                "sha256:47ab1e7b91c098ab893b828deafa1203de86d0bc6ab587b160f78fe6c4011f75",\\n-                "sha256:49e3ceeabbfb9d66c3aef5af3a60cc43b85c33df25ce03d0031a608b0a8b2e3f",\\n-                "sha256:4dc8f9fb58f7364b63fd9f85013b780ef83c11857ae79f2feda41e270468dd9b",\\n-                "sha256:4efca8f86c54b22348a5467704e3fec767b2db12fc39c6d963168ab1d3fc9135",\\n-                "sha256:53edb4da6925ad13c07b6d26c2a852bd81e364f95301c66e930ab2aef5b5ddd8",\\n-                "sha256:5855f8438a7d1d458206a2466bf82b0f104a3724bf96a1c781ab731e4201731a",\\n-                "sha256:594c67807fb16238b30c44bdf74f36c02cdf22d1c8cda91ef8a0ed8dabf5620a",\\n-                "sha256:5b6d930f030f8ed98e3e6c98ffa0652bdb82601e7a016ec2ab5d7ff23baa78d1",\\n-                "sha256:5bb28c636d87e840583ee3adeb78172efc47c8b26127267f54a9c0ec251d41a9",\\n-                "sha256:60bf42e36abfaf9aff1f50f52644b336d4f0a3fd6d8a60ca0d054ac9f713a864",\\n-                "sha256:611d1ad9a4288cf3e3c16014564df047fe08410e628f89805e475368bd304914",\\n-                "sha256:6300b8454aa6930a24b9618fbb54b5a68135092bc666f7b06901f897fa5c2fee",\\n-                "sha256:63f3268ba69ace99cab4e3e3b5840b03340efed0948ab8f78d2fd87ee5442a4f",\\n-                "sha256:6557b31b5e2c9ddf0de32a691f2312a32f77cd7681d8af66c2692efdbef84c18",\\n-                "sha256:693ce3f9e70a6cf7d2fb9e6c9d8b204b6b39897a2c4a1aa65728d5ac97dcc1d8",\\n-                "sha256:6a7fae0dd14cf60ad5ff42baa2e95727c3d81ded453457771d02b7d2b3f9c0c2",\\n-                "sha256:6c4ca60fa24e85fe25b912b01e62cb969d69a23a5d5867682dd3e80b5b02581d",\\n-                "sha256:6fcf051089389abe060c9cd7caa212c707e58153afa2c649f00346ce6d260f1b",\\n-                "sha256:7d91275b0245b1da4d4cfa07e0faedd5b0812efc15b702576d103293e252af1b",\\n-                "sha256:89c687013cb1cd489a0f0ac24febe8c7a666e6e221b783e53ac50ebf68e45d86",\\n-                "sha256:8d206346619592c6200148b01a2142798c989edcb9c896f9ac9722a99d4e77e6",\\n-                "sha256:905fec760bd2fa1388bb5b489ee8ee5f7291d692638ea5f67982d968366bef9f",\\n-                "sha256:97383d78eb34da7e1fa37dd273c20ad4320929af65d156e35a5e2d89566d9dfb",\\n-                "sha256:984d76483eb32f1bcb536dc27e4ad56bba4baa70be32fa87152832cdd9db0833",\\n-                "sha256:99df47edb6bda1249d3e80fdabb1dab8c08ef3975f69aed437cb69d0a5de1e28",\\n-                "sha256:9f02365d4e99430a12647f09b6cc8bab61a6564363f313126f775eb4f6ef798e",\\n-                "sha256:a30e67a65b53ea0a5e62fe23682cfe22712e01f453b95233b25502f7c61cb415",\\n-                "sha256:ab3ef638ace319fa26553db0624c4699e31a28bb2a835c5faca8f8acf6a5a902",\\n-                "sha256:aca6377c0cb8a8253e493c6b451565ac77e98c2951c45f913e0b52facdcff83f",\\n-                "sha256:add36cb2dbb8b736611303cd3bfcee00afd96471b09cda130da3581cbdc56a6d",\\n-                "sha256:b2f4bf27480f5e5e8ce285a8c8fd176c0b03e93dcc6646477d4630e83440c6a9",\\n-                "sha256:b7f2d075102dc8c794cbde1947378051c4e5180d52d276987b8d28a3bd58c17d",\\n-                "sha256:baa1a4e8f868845af802979fcdbf0bb11f94f1cb7ced4c4b8a351bb60d108145",\\n-                "sha256:be98f628055368795d818ebf93da628541e10b75b41c559fdf36d104c5787066",\\n-                "sha256:bf5d821ffabf0ef3533c39c518f3357b171a1651c1ff6827325e4489b0e46c3c",\\n-                "sha256:c47adbc92fc1bb2b3274c4b3a43ae0e4573d9fbff4f54cd484555edbf030baf1",\\n-                "sha256:cdfba22ea2f0029c9261a4bd07e830a8da012291fbe44dc794e488b6c9bb353a",\\n-                "sha256:d6c7ebd4e944c85e2c3421e612a7057a2f48d478d79e61800d81468a8d842207",\\n-                "sha256:d7f9850398e85aba693bb640262d3611788b1f29a79f0c93c565694658f4071f",\\n-                "sha256:d8446c54dc28c01e5a2dbac5a25f071f6653e6e40f3a8818e8b45d790fe6ef53",\\n-                "sha256:deb993cacb280823246a026e3b2d81c493c53de6acfd5e6bfe31ab3402bb37dd",\\n-                "sha256:e0f138900af21926a02425cf736db95be9f4af72ba1bb21453432a07f6082134",\\n-                "sha256:e9936f0b261d4df76ad22f8fee3ae83b60d7c3e871292cd42f40b81b70afae85",\\n-                "sha256:f0567c4dc99f264f49fe27da5f735f414c4e7e7dd850cfd8e69f0862d7c74ea9",\\n-                "sha256:f5653a225f31e113b152e56f154ccbe59eeb1c7487b39b9d9f9cdb58e6c79dc5",\\n-                "sha256:f826e31d18b516f653fe296d967d700fddad5901ae07c622bb3705955e1faa94",\\n-                "sha256:f8ba0e8349a38d3001fae7eadded3f6606f0da5d748ee53cc1dab1d6527b9509",\\n-                "sha256:f9081981fe268bd86831e5c75f7de206ef275defcb82bc70740ae6dc507aee51",\\n-                "sha256:fa130dd50c57d53368c9d59395cb5526eda596d3ffe36666cd81a44d56e48872"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==2.0.1"\\n-        },\\n-        "matplotlib": {\\n-            "hashes": [\\n-                "sha256:1de0bb6cbfe460725f0e97b88daa8643bcf9571c18ba90bb8e41432aaeca91d6",\\n-                "sha256:1e850163579a8936eede29fad41e202b25923a0a8d5ffd08ce50fc0a97dcdc93",\\n-                "sha256:215e2a30a2090221a9481db58b770ce56b8ef46f13224ae33afe221b14b24dc1",\\n-                "sha256:348e6032f666ffd151b323342f9278b16b95d4a75dfacae84a11d2829a7816ae",\\n-                "sha256:3d2eb9c1cc254d0ffa90bc96fde4b6005d09c2228f99dfd493a4219c1af99644",\\n-                "sha256:3e477db76c22929e4c6876c44f88d790aacdf3c3f8f3a90cb1975c0bf37825b0",\\n-                "sha256:451cc89cb33d6652c509fc6b588dc51c41d7246afdcc29b8624e256b7663ed1f",\\n-                "sha256:46b1a60a04e6d884f0250d5cc8dc7bd21a9a96c584a7acdaab44698a44710bab",\\n-                "sha256:5f571b92a536206f7958f7cb2d367ff6c9a1fa8229dc35020006e4cdd1ca0acd",\\n-                "sha256:672960dd114e342b7c610bf32fb99d14227f29919894388b41553217457ba7ef",\\n-                "sha256:7310e353a4a35477c7f032409966920197d7df3e757c7624fd842f3eeb307d3d",\\n-                "sha256:746a1df55749629e26af7f977ea426817ca9370ad1569436608dc48d1069b87c",\\n-                "sha256:7c155437ae4fd366e2700e2716564d1787700687443de46bcb895fe0f84b761d",\\n-                "sha256:9265ae0fb35e29f9b8cc86c2ab0a2e3dcddc4dd9de4b85bf26c0f63fe5c1c2ca",\\n-                "sha256:94bdd1d55c20e764d8aea9d471d2ae7a7b2c84445e0fa463f02e20f9730783e1",\\n-                "sha256:9a79e5dd7bb797aa611048f5b70588b23c5be05b63eefd8a0d152ac77c4243db",\\n-                "sha256:a17f0a10604fac7627ec82820439e7db611722e80c408a726cd00d8c974c2fb3",\\n-                "sha256:a1acb72f095f1d58ecc2538ed1b8bca0b57df313b13db36ed34b8cdf1868e674",\\n-                "sha256:aa49571d8030ad0b9ac39708ee77bd2a22f87815e12bdee52ecaffece9313ed8",\\n-                "sha256:c24c05f645aef776e8b8931cb81e0f1632d229b42b6d216e30836e2e145a2b40",\\n-                "sha256:cf3a7e54eff792f0815dbbe9b85df2f13d739289c93d346925554f71d484be78",\\n-                "sha256:d738acfdfb65da34c91acbdb56abed46803db39af259b7f194dc96920360dbe4",\\n-                "sha256:e15fa23d844d54e7b3b7243afd53b7567ee71c721f592deb0727ee85e668f96a",\\n-                "sha256:ed4a9e6dcacba56b17a0a9ac22ae2c72a35b7f0ef0693aa68574f0b2df607a89",\\n-                "sha256:f44149a0ef5b4991aaef12a93b8e8d66d6412e762745fea1faa61d98524e0ba9"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==3.3.4"\\n-        },\\n-        "mtcnn-pytorch": {\\n-            "hashes": [\\n-                "sha256:ab2c2f5721afe11d961892f1b34aa508424f114c21aad031cc0c7ea3ceb15ad5"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.0.2"\\n-        },\\n-        "mxnet": {\\n-            "hashes": [\\n-                "sha256:1a1439cf9c6d4001755fc87816e67b0840dba15e711de01e818dd200714316b0",\\n-                "sha256:272435960a316c66c24f8774179344294618357be67d3ced8b3c87adbf41e121",\\n-                "sha256:4a34868f8f73eb90db4404b449fac8ccda53a3bc938276a43ab196a0ed35e7a1",\\n-                "sha256:a6f1d2c381977dc2594b880eb43470447438f6a31321e76142831168821c30d0",\\n-                "sha256:c223c63916a2bd843df32a3f1cc1aecd668c5c85dd2de7cbe166d99a4334cff9",\\n-                "sha256:e6049408be3ac915674ef52e9490ffce0c5e04fd82e550eb618a0f0a75a1b96d"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.5.0"\\n-        },\\n-        "mxnet-mkl": {\\n-            "hashes": [\\n-                "sha256:0ce9bf4a35312a6ef2f8276370969dea2317423487a170a90cd742cb5d9f5f4c",\\n-                "sha256:0dfd7eb60332858fe3f0d79542d6c38fb8c18c28bc5265de2b84ee00f74c8c80",\\n-                "sha256:723318d28d07afaedc685e059b045d37bf6d35134b157f36dd16c6830b94dae2",\\n-                "sha256:7d9a5bdff9cefb6435460da5e90dbf7072ac0d5fb877c8fb7d65c7618cec7ad7",\\n-                "sha256:91c02710ca76921ef1a364a8b309dd2b4bd5a0909332bad9e42f8089b95451c8",\\n-                "sha256:cab8fb56c6bd952b30866575b3632dc9c7a8d22da273017401745b2d01524d49"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.5.0"\\n-        },\\n-        "numpy": {\\n-            "hashes": [\\n-                "sha256:0cdbbaa30ae69281b18dd995d3079c4e552ad6d5426977f66b9a2a95f11f552a",\\n-                "sha256:2b0cca1049bd39d1879fa4d598624cafe82d35529c72de1b3d528d68031cdd95",\\n-                "sha256:31d3fe5b673e99d33d70cfee2ea8fe8dccd60f265c3ed990873a88647e3dd288",\\n-                "sha256:34dd4922aab246c39bf5df03ca653d6265e65971deca6784c956bf356bca6197",\\n-                "sha256:384e2dfa03da7c8d54f8f934f61b6a5e4e1ebb56a65b287567629d6c14578003",\\n-                "sha256:392e2ea22b41a22c0289a88053204b616181288162ba78e6823e1760309d5277",\\n-                "sha256:4341a39fc085f31a583be505eabf00e17c619b469fef78dc7e8241385bfddaa4",\\n-                "sha256:45080f065dcaa573ebecbfe13cdd86e8c0a68c4e999aa06bd365374ea7137706",\\n-                "sha256:485cb1eb4c9962f4cd042fed9424482ec1d83fee5dc2ef3f2552ac47852cb259",\\n-                "sha256:575cefd28d3e0da85b0864506ae26b06483ee4a906e308be5a7ad11083f9d757",\\n-                "sha256:62784b35df7de7ca4d0d81c5b6af5983f48c5cdef32fc3635b445674e56e3266",\\n-                "sha256:69c152f7c11bf3b4fc11bc4cc62eb0334371c0db6844ebace43b7c815b602805",\\n-                "sha256:6ccfdcefd287f252cf1ea7a3f1656070da330c4a5658e43ad223269165cdf977",\\n-                "sha256:7298fbd73c0b3eff1d53dc9b9bdb7add8797bb55eeee38c8ccd7906755ba28af",\\n-                "sha256:79463d918d1bf3aeb9186e3df17ddb0baca443f41371df422f99ee94f4f2bbfe",\\n-                "sha256:8bbee788d82c0ac656536de70e817af09b7694f5326b0ef08e5c1014fcb96bb3",\\n-                "sha256:a863957192855c4c57f60a75a1ac06ce5362ad18506d362dd807e194b4baf3ce",\\n-                "sha256:ae602ba425fb2b074e16d125cdce4f0194903da935b2e7fe284ebecca6d92e76",\\n-                "sha256:b13faa258b20fa66d29011f99fdf498641ca74a0a6d9266bc27d83c70fea4a6a",\\n-                "sha256:c2c39d69266621dd7464e2bb740d6eb5abc64ddc339cc97aa669f3bb4d75c103",\\n-                "sha256:e9c88f173d31909d881a60f08a8494e63f1aff2a4052476b24d4f50e82c47e24",\\n-                "sha256:f1a29267ac29fff0913de0f11f3a9edfcd3f39595f467026c29376fad243ebe3",\\n-                "sha256:f69dde0c5a137d887676a8129373e44366055cf19d1b434e853310c7a1e68f93"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.16.1"\\n-        },\\n-        "opencv-python": {\\n-            "hashes": [\\n-                "sha256:0dc82a3d8630c099d2f3ac1b1aabee164e8188db54a786abb7a4e27eba309440",\\n-                "sha256:5af8ba35a4fcb8913ffb86e92403e9a656a4bff4a645d196987468f0f8947875",\\n-                "sha256:6e32af22e3202748bd233ed8f538741876191863882eba44e332d1a34993165b",\\n-                "sha256:c5bfae41ad4031e66bb10ec4a0a2ffd3e514d092652781e8b1ac98d1b59f1158",\\n-                "sha256:dbdc84a9b4ea2cbae33861652d25093944b9959279200b7ae0badd32439f74de",\\n-                "sha256:e6e448b62afc95c5b58f97e87ef84699e6607fe5c58730a03301c52496005cae",\\n-                "sha256:f482e78de6e7b0b060ff994ffd859bddc3f7f382bb2019ef157b0ea8ca8712f5"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==4.6.0.66"\\n-        },\\n-        "opt-einsum": {\\n-            "hashes": [\\n-                "sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147",\\n-                "sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"\\n-            ],\\n-            "markers": "python_version >= \\\'3.5\\\'",\\n-            "version": "==3.3.0"\\n-        },\\n-        "pillow": {\\n-            "hashes": [\\n-                "sha256:066f3999cb3b070a95c3652712cffa1a748cd02d60ad7b4e485c3748a04d9d76",\\n-                "sha256:0a0956fdc5defc34462bb1c765ee88d933239f9a94bc37d132004775241a7585",\\n-                "sha256:0b052a619a8bfcf26bd8b3f48f45283f9e977890263e4571f2393ed8898d331b",\\n-                "sha256:1394a6ad5abc838c5cd8a92c5a07535648cdf6d09e8e2d6df916dfa9ea86ead8",\\n-                "sha256:1bc723b434fbc4ab50bb68e11e93ce5fb69866ad621e3c2c9bdb0cd70e345f55",\\n-                "sha256:244cf3b97802c34c41905d22810846802a3329ddcb93ccc432870243211c79fc",\\n-                "sha256:25a49dc2e2f74e65efaa32b153527fc5ac98508d502fa46e74fa4fd678ed6645",\\n-                "sha256:2e4440b8f00f504ee4b53fe30f4e381aae30b0568193be305256b1462216feff",\\n-                "sha256:3862b7256046fcd950618ed22d1d60b842e3a40a48236a5498746f21189afbbc",\\n-                "sha256:3eb1ce5f65908556c2d8685a8f0a6e989d887ec4057326f6c22b24e8a172c66b",\\n-                "sha256:3f97cfb1e5a392d75dd8b9fd274d205404729923840ca94ca45a0af57e13dbe6",\\n-                "sha256:493cb4e415f44cd601fcec11c99836f707bb714ab03f5ed46ac25713baf0ff20",\\n-                "sha256:4acc0985ddf39d1bc969a9220b51d94ed51695d455c228d8ac29fcdb25810e6e",\\n-                "sha256:5503c86916d27c2e101b7f71c2ae2cddba01a2cf55b8395b0255fd33fa4d1f1a",\\n-                "sha256:5b7bb9de00197fb4261825c15551adf7605cf14a80badf1761d61e59da347779",\\n-                "sha256:5e9ac5f66616b87d4da618a20ab0a38324dbe88d8a39b55be8964eb520021e02",\\n-                "sha256:620582db2a85b2df5f8a82ddeb52116560d7e5e6b055095f04ad828d1b0baa39",\\n-                "sha256:62cc1afda735a8d109007164714e73771b499768b9bb5afcbbee9d0ff374b43f",\\n-                "sha256:70ad9e5c6cb9b8487280a02c0ad8a51581dcbbe8484ce058477692a27c151c0a",\\n-                "sha256:72b9e656e340447f827885b8d7a15fc8c4e68d410dc2297ef6787eec0f0ea409",\\n-                "sha256:72cbcfd54df6caf85cc35264c77ede902452d6df41166010262374155947460c",\\n-                "sha256:792e5c12376594bfcb986ebf3855aa4b7c225754e9a9521298e460e92fb4a488",\\n-                "sha256:7b7017b61bbcdd7f6363aeceb881e23c46583739cb69a3ab39cb384f6ec82e5b",\\n-                "sha256:81f8d5c81e483a9442d72d182e1fb6dcb9723f289a57e8030811bac9ea3fef8d",\\n-                "sha256:82aafa8d5eb68c8463b6e9baeb4f19043bb31fefc03eb7b216b51e6a9981ae09",\\n-                "sha256:84c471a734240653a0ec91dec0996696eea227eafe72a33bd06c92697728046b",\\n-                "sha256:8c803ac3c28bbc53763e6825746f05cc407b20e4a69d0122e526a582e3b5e153",\\n-                "sha256:93ce9e955cc95959df98505e4608ad98281fff037350d8c2671c9aa86bcf10a9",\\n-                "sha256:9a3e5ddc44c14042f0844b8cf7d2cd455f6cc80fd7f5eefbe657292cf601d9ad",\\n-                "sha256:a4901622493f88b1a29bd30ec1a2f683782e57c3c16a2dbc7f2595ba01f639df",\\n-                "sha256:a5a4532a12314149d8b4e4ad8ff09dde7427731fcfa5917ff16d0291f13609df",\\n-                "sha256:b8831cb7332eda5dc89b21a7bce7ef6ad305548820595033a4b03cf3091235ed",\\n-                "sha256:b8e2f83c56e141920c39464b852de3719dfbfb6e3c99a2d8da0edf4fb33176ed",\\n-                "sha256:c70e94281588ef053ae8998039610dbd71bc509e4acbc77ab59d7d2937b10698",\\n-                "sha256:c8a17b5d948f4ceeceb66384727dde11b240736fddeda54ca740b9b8b1556b29",\\n-                "sha256:d82cdb63100ef5eedb8391732375e6d05993b765f72cb34311fab92103314649",\\n-                "sha256:d89363f02658e253dbd171f7c3716a5d340a24ee82d38aab9183f7fdf0cdca49",\\n-                "sha256:d99ec152570e4196772e7a8e4ba5320d2d27bf22fdf11743dd882936ed64305b",\\n-                "sha256:ddc4d832a0f0b4c52fff973a0d44b6c99839a9d016fe4e6a1cb8f3eea96479c2",\\n-                "sha256:e3dacecfbeec9a33e932f00c6cd7996e62f53ad46fbe677577394aaa90ee419a",\\n-                "sha256:eb9fc393f3c61f9054e1ed26e6fe912c7321af2f41ff49d3f83d05bacf22cc78"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==8.4.0"\\n-        },\\n-        "protobuf": {\\n-            "hashes": [\\n-                "sha256:010be24d5a44be7b0613750ab40bc8b8cedc796db468eae6c779b395f50d1fa1",\\n-                "sha256:0469bc66160180165e4e29de7f445e57a34ab68f49357392c5b2f54c656ab25e",\\n-                "sha256:0c0714b025ec057b5a7600cb66ce7c693815f897cfda6d6efb58201c472e3437",\\n-                "sha256:11478547958c2dfea921920617eb457bc26867b0d1aa065ab05f35080c5d9eb6",\\n-                "sha256:14082457dc02be946f60b15aad35e9f5c69e738f80ebbc0900a19bc83734a5a4",\\n-                "sha256:2b2d2913bcda0e0ec9a784d194bc490f5dc3d9d71d322d070b11a0ade32ff6ba",\\n-                "sha256:30a15015d86b9c3b8d6bf78d5b8c7749f2512c29f168ca259c9d7727604d0e39",\\n-                "sha256:30f5370d50295b246eaa0296533403961f7e64b03ea12265d6dfce3a391d8992",\\n-                "sha256:347b393d4dd06fb93a77620781e11c058b3b0a5289262f094379ada2920a3730",\\n-                "sha256:4bc98de3cdccfb5cd769620d5785b92c662b6bfad03a202b83799b6ed3fa1fa7",\\n-                "sha256:5057c64052a1f1dd7d4450e9aac25af6bf36cfbfb3a1cd89d16393a036c49157",\\n-                "sha256:559670e006e3173308c9254d63facb2c03865818f22204037ab76f7a0ff70b5f",\\n-                "sha256:5a0d7539a1b1fb7e76bf5faa0b44b30f812758e989e59c40f77a7dab320e79b9",\\n-                "sha256:5f5540d57a43042389e87661c6eaa50f47c19c6176e8cf1c4f287aeefeccb5c4",\\n-                "sha256:7a552af4dc34793803f4e735aabe97ffc45962dfd3a237bdde242bff5a3de684",\\n-                "sha256:84a04134866861b11556a82dd91ea6daf1f4925746b992f277b84013a7cc1229",\\n-                "sha256:878b4cd080a21ddda6ac6d1e163403ec6eea2e206cf225982ae04567d39be7b0",\\n-                "sha256:90b0d02163c4e67279ddb6dc25e063db0130fc299aefabb5d481053509fae5c8",\\n-                "sha256:91d5f1e139ff92c37e0ff07f391101df77e55ebb97f46bbc1535298d72019462",\\n-                "sha256:a8ce5ae0de28b51dff886fb922012dad885e66176663950cb2344c0439ecb473",\\n-                "sha256:aa3b82ca1f24ab5326dcf4ea00fcbda703e986b22f3d27541654f749564d778b",\\n-                "sha256:bb6776bd18f01ffe9920e78e03a8676530a5d6c5911934c6a1ac6eb78973ecb6",\\n-                "sha256:bbf5cea5048272e1c60d235c7bd12ce1b14b8a16e76917f371c718bd3005f045",\\n-                "sha256:c0ccd3f940fe7f3b35a261b1dd1b4fc850c8fde9f74207015431f174be5976b3",\\n-                "sha256:d0b635cefebd7a8a0f92020562dead912f81f401af7e71f16bf9506ff3bdbb38"\\n-            ],\\n-            "markers": "python_version >= \\\'3.5\\\'",\\n-            "version": "==3.19.6"\\n-        },\\n-        "pycparser": {\\n-            "hashes": [\\n-                "sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9",\\n-                "sha256:e644fdec12f7872f86c58ff790da456218b10f863970249516d60a5eaca77206"\\n-            ],\\n-            "version": "==2.21"\\n-        },\\n-        "pyparsing": {\\n-            "hashes": [\\n-                "sha256:2b020ecf7d21b687f219b71ecad3631f644a47f01403fa1d1036b0c6416d70fb",\\n-                "sha256:5026bae9a10eeaefb61dab2f09052b9f4307d44aee4eda64b309723d8d206bbc"\\n-            ],\\n-            "markers": "python_full_version >= \\\'3.6.8\\\'",\\n-            "version": "==3.0.9"\\n-        },\\n-        "python-dateutil": {\\n-            "hashes": [\\n-                "sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86",\\n-                "sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3\\\'",\\n-            "version": "==2.8.2"\\n-        },\\n-        "requests": {\\n-            "hashes": [\\n-                "sha256:6a1b267aa90cac58ac3a765d067950e7dbbf75b1da07e895d1f594193a40a38b",\\n-                "sha256:9c443e7324ba5b85070c4a818ade28bfabedf16ea10206da1132edaa6dda237e"\\n-            ],\\n-            "version": "==2.18.4"\\n-        },\\n-        "scikit-learn": {\\n-            "hashes": [\\n-                "sha256:1ac81293d261747c25ea5a0ee8cd2bb1f3b5ba9ec05421a7f9f0feb4eb7c4116",\\n-                "sha256:289361cf003d90b007f5066b27fcddc2d71324c82f1c88e316fedacb0dfdd516",\\n-                "sha256:3a14d0abd4281fc3fd2149c486c3ec7cedad848b8d5f7b6f61522029d65a29f8",\\n-                "sha256:5083a5e50d9d54548e4ada829598ae63a05651dd2bb319f821ffd9e8388384a6",\\n-                "sha256:777cdd5c077b7ca9cb381396c81990cf41d2fa8350760d3cad3b4c460a7db644",\\n-                "sha256:8bf2ff63da820d09b96b18e88f9625228457bff8df4618f6b087e12442ef9e15",\\n-                "sha256:8d319b71c449627d178f21c57614e21747e54bb3fc9602b6f42906c3931aa320",\\n-                "sha256:928050b65781fea9542dfe9bfe02d8c4f5530baa8472ec60782ea77347d2c836",\\n-                "sha256:92c903613ff50e22aa95d589f9fff5deb6f34e79f7f21f609680087f137bb524",\\n-                "sha256:ae322235def5ce8fae645b439e332e6f25d34bb90d6a6c8e261f17eb476457b7",\\n-                "sha256:c1cd6b29eb1fd1cc672ac5e4a8be5f6ea936d094a3dc659ada0746d6fac750b1",\\n-                "sha256:c41a6e2685d06bcdb0d26533af2540f54884d40db7e48baed6a5bcbf1a7cc642",\\n-                "sha256:d07fcb0c0acbc043faa0e7cf4d2037f71193de3fb04fb8ed5c259b089af1cf5c",\\n-                "sha256:d146d5443cda0a41f74276e42faf8c7f283fef49e8a853b832885239ef544e05",\\n-                "sha256:eb2b7bed0a26ba5ce3700e15938b28a4f4513578d3e54a2156c29df19ac5fd01",\\n-                "sha256:eb9b8ebf59eddd8b96366428238ab27d05a19e89c5516ce294abc35cea75d003"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==0.21.3"\\n-        },\\n-        "scipy": {\\n-            "hashes": [\\n-                "sha256:0611ee97296265af4a21164a5323f8c1b4e8e15c582d3dfa7610825900136bb7",\\n-                "sha256:08237eda23fd8e4e54838258b124f1cd141379a5f281b0a234ca99b38918c07a",\\n-                "sha256:0e645dbfc03f279e1946cf07c9c754c2a1859cb4a41c5f70b25f6b3a586b6dbd",\\n-                "sha256:0e9bb7efe5f051ea7212555b290e784b82f21ffd0f655405ac4f87e288b730b3",\\n-                "sha256:108c16640849e5827e7d51023efb3bd79244098c3f21e4897a1007720cb7ce37",\\n-                "sha256:340ef70f5b0f4e2b4b43c8c8061165911bc6b2ad16f8de85d9774545e2c47463",\\n-                "sha256:3ad73dfc6f82e494195144bd3a129c7241e761179b7cb5c07b9a0ede99c686f3",\\n-                "sha256:3b243c77a822cd034dad53058d7c2abf80062aa6f4a32e9799c95d6391558631",\\n-                "sha256:404a00314e85eca9d46b80929571b938e97a143b4f2ddc2b2b3c91a4c4ead9c5",\\n-                "sha256:423b3ff76957d29d1cce1bc0d62ebaf9a3fdfaf62344e3fdec14619bb7b5ad3a",\\n-                "sha256:42d9149a2fff7affdd352d157fa5717033767857c11bd55aa4a519a44343dfef",\\n-                "sha256:625f25a6b7d795e8830cb70439453c9f163e6870e710ec99eba5722775b318f3",\\n-                "sha256:698c6409da58686f2df3d6f815491fd5b4c2de6817a45379517c92366eea208f",\\n-                "sha256:729f8f8363d32cebcb946de278324ab43d28096f36593be6281ca1ee86ce6559",\\n-                "sha256:8190770146a4c8ed5d330d5b5ad1c76251c63349d25c96b3094875b930c44692",\\n-                "sha256:878352408424dffaa695ffedf2f9f92844e116686923ed9aa8626fc30d32cfd1",\\n-                "sha256:8b984f0821577d889f3c7ca8445564175fb4ac7c7f9659b7c60bef95b2b70e76",\\n-                "sha256:8f841bbc21d3dad2111a94c490fb0a591b8612ffea86b8e5571746ae76a3deac",\\n-                "sha256:c22b27371b3866c92796e5d7907e914f0e58a36d3222c5d436ddd3f0e354227a",\\n-                "sha256:d0cdd5658b49a722783b8b4f61a6f1f9c75042d0e29a30ccb6cacc9b25f6d9e2",\\n-                "sha256:d40dc7f494b06dcee0d303e51a00451b2da6119acbeaccf8369f2d29e28917ac",\\n-                "sha256:d8491d4784aceb1f100ddb8e31239c54e4afab8d607928a9f7ef2469ec35ae01",\\n-                "sha256:dfc5080c38dde3f43d8fbb9c0539a7839683475226cf83e4b24363b227dfe552",\\n-                "sha256:e24e22c8d98d3c704bb3410bce9b69e122a8de487ad3dbfe9985d154e5c03a40",\\n-                "sha256:e7a01e53163818d56eabddcafdc2090e9daba178aad05516b20c6591c4811020",\\n-                "sha256:ee677635393414930541a096fc8e61634304bb0153e4e02b75685b11eba14cae",\\n-                "sha256:f0521af1b722265d824d6ad055acfe9bd3341765735c44b5a4d0069e189a0f40",\\n-                "sha256:f25c281f12c0da726c6ed00535ca5d1622ec755c30a3f8eafef26cf43fede694"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.1.0"\\n-        },\\n-        "setuptools": {\\n-            "hashes": [\\n-                "sha256:22c7348c6d2976a52632c67f7ab0cdf40147db7789f9aed18734643fe9cf3373",\\n-                "sha256:4ce92f1e1f8f01233ee9952c04f6b81d1e02939d6e1b488428154974a4d0783e"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==59.6.0"\\n-        },\\n-        "six": {\\n-            "hashes": [\\n-                "sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926",\\n-                "sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3\\\'",\\n-            "version": "==1.16.0"\\n-        },\\n-        "tensorboard": {\\n-            "hashes": [\\n-                "sha256:4cad2c65f6011e51609b463014c014fd7c6ddd9c1263af1d4f18dd97ed88c2bc",\\n-                "sha256:612b789386aa1b2c4804e1961273b37f8e4dd97613f98bc90ff0402d24627f50"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3\\\'",\\n-            "version": "==1.15.0"\\n-        },\\n-        "tensorboardx": {\\n-            "hashes": [\\n-                "sha256:13fe0abba27f407778a7321937190eedaf12bc8c544d9a4e294fcf0ba177fd76",\\n-                "sha256:f52e59b38b4cdf83384f3fce067bcaf2d2847619f9f533394df0de3b5a71ab8e"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.8"\\n-        },\\n-        "tensorflow": {\\n-            "hashes": [\\n-                "sha256:0a01def34c28298970dc83776dd43877fd59e43fddd8e960d01b6eb849ba9938",\\n-                "sha256:1afb2e573fe666eb0dd6a45dbb7679de622a318fcc1fc401fb7819068f2d858b",\\n-                "sha256:20bbda24b022c7cedd569bb36a6d620b68e0ea37afc91645c918971c72881388",\\n-                "sha256:26b86d32f20dba79da30cb87d67a708678cc6c47f6e596c8783433e282e91f00",\\n-                "sha256:5910cdde20a9143760d3b49f6d03a6a09ab5221e2f871eb51dbd422b80da07fc",\\n-                "sha256:6265f7d86a66754b875e252effa01f7d54aa4464df3b71182d4be8b269b8a148",\\n-                "sha256:79fbb4a38c6e6417d5eef8ccae00b16053750c5d61092ec7caabe4cea6870dc2",\\n-                "sha256:8e4bb1c361a9b350497741c1d0a556d9592b5ce52c9b073d88dd053c182f3d15",\\n-                "sha256:a214d7bbdae4093bab9a4c582bbf9c4464274cc9db99f94bb42b88c5c961a452",\\n-                "sha256:af57e0e16adb4d6ccd387954c1d70e34cc4925b74da9135d2b83ca7d3dd9d102",\\n-                "sha256:d1694e25887bc0d0c31d6c7d9c92c8bf9e0499085c7e5a9dbaacf675c44027a8"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.15"\\n-        },\\n-        "tensorflow-estimator": {\\n-            "hashes": [\\n-                "sha256:8853bfb7c3c96fbdc80b3d66c37a10af6ccbcd235dc87474764270c02a0f86b9"\\n-            ],\\n-            "version": "==1.15.1"\\n-        },\\n-        "termcolor": {\\n-            "hashes": [\\n-                "sha256:1d6d69ce66211143803fbc56652b41d73b4a400a2891d7bf7a1cdf4c02de613b"\\n-            ],\\n-            "version": "==1.1.0"\\n-        },\\n-        "torch": {\\n-            "hashes": [\\n-                "sha256:258a0729fb77a3457d5822d84b536057cd119b08049a8d3c41dc3dcdeb48d56e",\\n-                "sha256:3592d3dd62b32760c82624e7586222747fe2281240e8653970b35f1d6d4a434c",\\n-                "sha256:376fc18407add20daa6bbaaffc5a5e06d733abe53bcbd60ef2532bfed34bc091",\\n-                "sha256:3eee3cf53c1f8fb3f1fe107a22025a8501fc6440d14e09599ba7153002531f84",\\n-                "sha256:5b68e9108bd7ebd99eee941686046c517cfaac5331f757bcf440fe02f2e3ced1",\\n-                "sha256:65fd02ed889c63fd82bf1a440c5a94c1310c29f3e6f9f62add416d34da355d97",\\n-                "sha256:6a81f886823bbd15edc2dc0908fa214070df61c9f7ab8831f0a03630275cca5a",\\n-                "sha256:6da1b877880435440a5aa9678ef0f01986d4886416844db1d97ebfb7fd1778d0",\\n-                "sha256:8f3fd2e3ffc3bb867133fdf7fbcc8a0bb2e62a5c0696396f51856f5abf9045a8",\\n-                "sha256:901b52787baeb2e9e1357ca7037da0028bc6ad743f530e0040ae96ef8e27156c",\\n-                "sha256:935e5ac804c5093c79f23a7e6ca5b912c166071aa9d8b4a0a3d6a85126d6a47b",\\n-                "sha256:97b7b0c667e8b0dd1fc70137a36e0a4841ec10ef850bda60500ad066bef3e2de",\\n-                "sha256:9ef4c004f9e5168bd1c1930c6aff25fed5b097de81db6271ffbb2e4fb8b89319",\\n-                "sha256:ab77a9f838874f295ed5410c0686fa22547456e0116efb281c66ef5f9d46fe28",\\n-                "sha256:b07ef01e36b716d0d65ca60c4db0ac9d094a0e797d9b55290da4dcda91463b6c",\\n-                "sha256:d43bc3f3a2d89ae185ef96d903c935c335219231e57685658648396984e2a67a",\\n-                "sha256:ef99b8cca5f9358119b07956915faf6e7906f433ab4a603c160ae9de88918371",\\n-                "sha256:f281438ee99bd72ad65c0bba1026a32e45c3b636bc067fc145ad291e9ea2faab",\\n-                "sha256:fbaf18c1b3e0b31af194a9d853e3739464cf982d279df9d34dd18f1c2a471878"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==1.10.2"\\n-        },\\n-        "torchvision": {\\n-            "hashes": [\\n-                "sha256:23ded32f6e6b6048f2da7b9a8933d122038d050641e385b8857ef206e0613d3b",\\n-                "sha256:4380da0565bd729338f965b4465367dc35a64bdf9e0654a6ae72cf9bdb46dce7",\\n-                "sha256:49315de81faa7e874dbcff58d188eb4d7d0327bc9bb630f2da9d7abe79583708",\\n-                "sha256:5f8b6acd544fca8642099c921cea590cd711ecd6b30c73a335c10a525421450e",\\n-                "sha256:60932ebfa4cbcdba707821ea365f7ca89cde679c087e96f947d99035132dbd7e",\\n-                "sha256:621b49722f97489725660ef0152a447031b21314af67b49a48ee48513f073690",\\n-                "sha256:65cbd363ec17a74d47d0e5237c258a98d0cd548e79455a37d9699418f9413512",\\n-                "sha256:7715c602d4ef7d66ae37002ccbf1d341fdafefd3331e2dabc9341483eb0bd2b2",\\n-                "sha256:b2edd39450c7628e46b53b370a80727aacebfce83458faa4209f6346a39ca845",\\n-                "sha256:c19d3d72987e6c0cbd90e37a80b51f93765c9cbc9954074b6396f8f2ea80a7d4",\\n-                "sha256:dd1c29ea41b6f7f12477aaf2e359781023a6e127d9a5642b6e96978b56b13674",\\n-                "sha256:e54e0f392c3a20206e5f1d09c7eef208e680099c242e4f86b7e51217df812f32"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==0.4.1"\\n-        },\\n-        "tqdm": {\\n-            "hashes": [\\n-                "sha256:1be3e4e3198f2d0e47b928e9d9a8ec1b63525db29095cec1467f4c5a4ea8ebf9",\\n-                "sha256:7e39a30e3d34a7a6539378e39d7490326253b7ee354878a92255656dc4284457"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==4.35.0"\\n-        },\\n-        "typing-extensions": {\\n-            "hashes": [\\n-                "sha256:1a9462dcc3347a79b1f1c0271fbe79e844580bb598bafa1ed208b94da3cdcd42",\\n-                "sha256:21c85e0fe4b9a155d0799430b0ad741cdce7e359660ccbd8b530613e8df88ce2"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==4.1.1"\\n-        },\\n-        "urllib3": {\\n-            "hashes": [\\n-                "sha256:06330f386d6e4b195fbfc736b297f58c5a892e4440e54d294d7004e3a9bbea1b",\\n-                "sha256:cc44da8e1145637334317feebd728bd869a35285b93cbb4cca2577da7e62db4f"\\n-            ],\\n-            "version": "==1.22"\\n-        },\\n-        "werkzeug": {\\n-            "hashes": [\\n-                "sha256:87ae4e5b5366da2347eb3116c0e6c681a0e939a33b2805e2c0cbd282664932c4",\\n-                "sha256:a13b74dd3c45f758d4ebdb224be8f1ab8ef58b3c0ffc1783a8c7d9f4f50227e6"\\n-            ],\\n-            "index": "pypi",\\n-            "version": "==0.15.5"\\n-        },\\n-        "wheel": {\\n-            "hashes": [\\n-                "sha256:4bdcd7d840138086126cd09254dc6195fb4fc6f01c050a1d7236f2630db1d22a",\\n-                "sha256:e9a504e793efbca1b8e0e9cb979a249cf4a0a7b5b8c9e8b65a5e39d49529c1c4"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3, 3.4\\\'",\\n-            "version": "==0.37.1"\\n-        },\\n-        "wrapt": {\\n-            "hashes": [\\n-                "sha256:00b6d4ea20a906c0ca56d84f93065b398ab74b927a7a3dbd470f6fc503f95dc3",\\n-                "sha256:01c205616a89d09827986bc4e859bcabd64f5a0662a7fe95e0d359424e0e071b",\\n-                "sha256:02b41b633c6261feff8ddd8d11c711df6842aba629fdd3da10249a53211a72c4",\\n-                "sha256:07f7a7d0f388028b2df1d916e94bbb40624c59b48ecc6cbc232546706fac74c2",\\n-                "sha256:11871514607b15cfeb87c547a49bca19fde402f32e2b1c24a632506c0a756656",\\n-                "sha256:1b376b3f4896e7930f1f772ac4b064ac12598d1c38d04907e696cc4d794b43d3",\\n-                "sha256:21ac0156c4b089b330b7666db40feee30a5d52634cc4560e1905d6529a3897ff",\\n-                "sha256:257fd78c513e0fb5cdbe058c27a0624c9884e735bbd131935fd49e9fe719d310",\\n-                "sha256:2b39d38039a1fdad98c87279b48bc5dce2c0ca0d73483b12cb72aa9609278e8a",\\n-                "sha256:2cf71233a0ed05ccdabe209c606fe0bac7379fdcf687f39b944420d2a09fdb57",\\n-                "sha256:2fe803deacd09a233e4762a1adcea5db5d31e6be577a43352936179d14d90069",\\n-                "sha256:3232822c7d98d23895ccc443bbdf57c7412c5a65996c30442ebe6ed3df335383",\\n-                "sha256:34aa51c45f28ba7f12accd624225e2b1e5a3a45206aa191f6f9aac931d9d56fe",\\n-                "sha256:36f582d0c6bc99d5f39cd3ac2a9062e57f3cf606ade29a0a0d6b323462f4dd87",\\n-                "sha256:380a85cf89e0e69b7cfbe2ea9f765f004ff419f34194018a6827ac0e3edfed4d",\\n-                "sha256:40e7bc81c9e2b2734ea4bc1aceb8a8f0ceaac7c5299bc5d69e37c44d9081d43b",\\n-                "sha256:43ca3bbbe97af00f49efb06e352eae40434ca9d915906f77def219b88e85d907",\\n-                "sha256:4fcc4649dc762cddacd193e6b55bc02edca674067f5f98166d7713b193932b7f",\\n-                "sha256:5a0f54ce2c092aaf439813735584b9537cad479575a09892b8352fea5e988dc0",\\n-                "sha256:5a9a0d155deafd9448baff28c08e150d9b24ff010e899311ddd63c45c2445e28",\\n-                "sha256:5b02d65b9ccf0ef6c34cba6cf5bf2aab1bb2f49c6090bafeecc9cd81ad4ea1c1",\\n-                "sha256:60db23fa423575eeb65ea430cee741acb7c26a1365d103f7b0f6ec412b893853",\\n-                "sha256:642c2e7a804fcf18c222e1060df25fc210b9c58db7c91416fb055897fc27e8cc",\\n-                "sha256:6a9a25751acb379b466ff6be78a315e2b439d4c94c1e99cb7266d40a537995d3",\\n-                "sha256:6b1a564e6cb69922c7fe3a678b9f9a3c54e72b469875aa8018f18b4d1dd1adf3",\\n-                "sha256:6d323e1554b3d22cfc03cd3243b5bb815a51f5249fdcbb86fda4bf62bab9e164",\\n-                "sha256:6e743de5e9c3d1b7185870f480587b75b1cb604832e380d64f9504a0535912d1",\\n-                "sha256:709fe01086a55cf79d20f741f39325018f4df051ef39fe921b1ebe780a66184c",\\n-                "sha256:7b7c050ae976e286906dd3f26009e117eb000fb2cf3533398c5ad9ccc86867b1",\\n-                "sha256:7d2872609603cb35ca513d7404a94d6d608fc13211563571117046c9d2bcc3d7",\\n-                "sha256:7ef58fb89674095bfc57c4069e95d7a31cfdc0939e2a579882ac7d55aadfd2a1",\\n-                "sha256:80bb5c256f1415f747011dc3604b59bc1f91c6e7150bd7db03b19170ee06b320",\\n-                "sha256:81b19725065dcb43df02b37e03278c011a09e49757287dca60c5aecdd5a0b8ed",\\n-                "sha256:833b58d5d0b7e5b9832869f039203389ac7cbf01765639c7309fd50ef619e0b1",\\n-                "sha256:88bd7b6bd70a5b6803c1abf6bca012f7ed963e58c68d76ee20b9d751c74a3248",\\n-                "sha256:8ad85f7f4e20964db4daadcab70b47ab05c7c1cf2a7c1e51087bfaa83831854c",\\n-                "sha256:8c0ce1e99116d5ab21355d8ebe53d9460366704ea38ae4d9f6933188f327b456",\\n-                "sha256:8d649d616e5c6a678b26d15ece345354f7c2286acd6db868e65fcc5ff7c24a77",\\n-                "sha256:903500616422a40a98a5a3c4ff4ed9d0066f3b4c951fa286018ecdf0750194ef",\\n-                "sha256:9736af4641846491aedb3c3f56b9bc5568d92b0692303b5a305301a95dfd38b1",\\n-                "sha256:988635d122aaf2bdcef9e795435662bcd65b02f4f4c1ae37fbee7401c440b3a7",\\n-                "sha256:9cca3c2cdadb362116235fdbd411735de4328c61425b0aa9f872fd76d02c4e86",\\n-                "sha256:9e0fd32e0148dd5dea6af5fee42beb949098564cc23211a88d799e434255a1f4",\\n-                "sha256:9f3e6f9e05148ff90002b884fbc2a86bd303ae847e472f44ecc06c2cd2fcdb2d",\\n-                "sha256:a85d2b46be66a71bedde836d9e41859879cc54a2a04fad1191eb50c2066f6e9d",\\n-                "sha256:a9a52172be0b5aae932bef82a79ec0a0ce87288c7d132946d645eba03f0ad8a8",\\n-                "sha256:aa31fdcc33fef9eb2552cbcbfee7773d5a6792c137b359e82879c101e98584c5",\\n-                "sha256:b014c23646a467558be7da3d6b9fa409b2c567d2110599b7cf9a0c5992b3b471",\\n-                "sha256:b21bb4c09ffabfa0e85e3a6b623e19b80e7acd709b9f91452b8297ace2a8ab00",\\n-                "sha256:b5901a312f4d14c59918c221323068fad0540e34324925c8475263841dbdfe68",\\n-                "sha256:b9b7a708dd92306328117d8c4b62e2194d00c365f18eff11a9b53c6f923b01e3",\\n-                "sha256:d1967f46ea8f2db647c786e78d8cc7e4313dbd1b0aca360592d8027b8508e24d",\\n-                "sha256:d52a25136894c63de15a35bc0bdc5adb4b0e173b9c0d07a2be9d3ca64a332735",\\n-                "sha256:d77c85fedff92cf788face9bfa3ebaa364448ebb1d765302e9af11bf449ca36d",\\n-                "sha256:d79d7d5dc8a32b7093e81e97dad755127ff77bcc899e845f41bf71747af0c569",\\n-                "sha256:dbcda74c67263139358f4d188ae5faae95c30929281bc6866d00573783c422b7",\\n-                "sha256:ddaea91abf8b0d13443f6dac52e89051a5063c7d014710dcb4d4abb2ff811a59",\\n-                "sha256:dee0ce50c6a2dd9056c20db781e9c1cfd33e77d2d569f5d1d9321c641bb903d5",\\n-                "sha256:dee60e1de1898bde3b238f18340eec6148986da0455d8ba7848d50470a7a32fb",\\n-                "sha256:e2f83e18fe2f4c9e7db597e988f72712c0c3676d337d8b101f6758107c42425b",\\n-                "sha256:e3fb1677c720409d5f671e39bac6c9e0e422584e5f518bfd50aa4cbbea02433f",\\n-                "sha256:ee2b1b1769f6707a8a445162ea16dddf74285c3964f605877a20e38545c3c462",\\n-                "sha256:ee6acae74a2b91865910eef5e7de37dc6895ad96fa23603d1d27ea69df545015",\\n-                "sha256:ef3f72c9666bba2bab70d2a8b79f2c6d2c1a42a7f7e2b0ec83bb2f9e383950af"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3, 3.4\\\'",\\n-            "version": "==1.14.1"\\n-        },\\n-        "zipp": {\\n-            "hashes": [\\n-                "sha256:71c644c5369f4a6e07636f0aa966270449561fcea2e3d6747b8d23efaa9d7832",\\n-                "sha256:9fe5ea21568a0a70e50f273397638d39b03353731e6cbbb3fd8502a33fec40bc"\\n-            ],\\n-            "markers": "python_version >= \\\'3.6\\\'",\\n-            "version": "==3.6.0"\\n-        },\\n-        "zope.event": {\\n-            "hashes": [\\n-                "sha256:73d9e3ef750cca14816a9c322c7250b0d7c9dbc337df5d1b807ff8d3d0b9e97c",\\n-                "sha256:81d98813046fc86cc4136e3698fee628a3282f9c320db18658c21749235fce80"\\n-            ],\\n-            "version": "==4.6"\\n-        },\\n-        "zope.interface": {\\n-            "hashes": [\\n-                "sha256:008b0b65c05993bb08912f644d140530e775cf1c62a072bf9340c2249e613c32",\\n-                "sha256:0217a9615531c83aeedb12e126611b1b1a3175013bbafe57c702ce40000eb9a0",\\n-                "sha256:0fb497c6b088818e3395e302e426850f8236d8d9f4ef5b2836feae812a8f699c",\\n-                "sha256:17ebf6e0b1d07ed009738016abf0d0a0f80388e009d0ac6e0ead26fc162b3b9c",\\n-                "sha256:311196634bb9333aa06f00fc94f59d3a9fddd2305c2c425d86e406ddc6f2260d",\\n-                "sha256:3218ab1a7748327e08ef83cca63eea7cf20ea7e2ebcb2522072896e5e2fceedf",\\n-                "sha256:404d1e284eda9e233c90128697c71acffd55e183d70628aa0bbb0e7a3084ed8b",\\n-                "sha256:4087e253bd3bbbc3e615ecd0b6dd03c4e6a1e46d152d3be6d2ad08fbad742dcc",\\n-                "sha256:40f4065745e2c2fa0dff0e7ccd7c166a8ac9748974f960cd39f63d2c19f9231f",\\n-                "sha256:5334e2ef60d3d9439c08baedaf8b84dc9bb9522d0dacbc10572ef5609ef8db6d",\\n-                "sha256:604cdba8f1983d0ab78edc29aa71c8df0ada06fb147cea436dc37093a0100a4e",\\n-                "sha256:6373d7eb813a143cb7795d3e42bd8ed857c82a90571567e681e1b3841a390d16",\\n-                "sha256:655796a906fa3ca67273011c9805c1e1baa047781fca80feeb710328cdbed87f",\\n-                "sha256:65c3c06afee96c654e590e046c4a24559e65b0a87dbff256cd4bd6f77e1a33f9",\\n-                "sha256:696f3d5493eae7359887da55c2afa05acc3db5fc625c49529e84bd9992313296",\\n-                "sha256:6e972493cdfe4ad0411fd9abfab7d4d800a7317a93928217f1a5de2bb0f0d87a",\\n-                "sha256:7579960be23d1fddecb53898035a0d112ac858c3554018ce615cefc03024e46d",\\n-                "sha256:765d703096ca47aa5d93044bf701b00bbce4d903a95b41fff7c3796e747b1f1d",\\n-                "sha256:7e66f60b0067a10dd289b29dceabd3d0e6d68be1504fc9d0bc209cf07f56d189",\\n-                "sha256:8a2ffadefd0e7206adc86e492ccc60395f7edb5680adedf17a7ee4205c530df4",\\n-                "sha256:959697ef2757406bff71467a09d940ca364e724c534efbf3786e86eee8591452",\\n-                "sha256:9d783213fab61832dbb10d385a319cb0e45451088abd45f95b5bb88ed0acca1a",\\n-                "sha256:a16025df73d24795a0bde05504911d306307c24a64187752685ff6ea23897cb0",\\n-                "sha256:a2ad597c8c9e038a5912ac3cf166f82926feff2f6e0dabdab956768de0a258f5",\\n-                "sha256:bfee1f3ff62143819499e348f5b8a7f3aa0259f9aca5e0ddae7391d059dce671",\\n-                "sha256:d169ccd0756c15bbb2f1acc012f5aab279dffc334d733ca0d9362c5beaebe88e",\\n-                "sha256:d514c269d1f9f5cd05ddfed15298d6c418129f3f064765295659798349c43e6f",\\n-                "sha256:d692374b578360d36568dd05efb8a5a67ab6d1878c29c582e37ddba80e66c396",\\n-                "sha256:dbaeb9cf0ea0b3bc4b36fae54a016933d64c6d52a94810a63c00f440ecb37dd7",\\n-                "sha256:dc26c8d44472e035d59d6f1177eb712888447f5799743da9c398b0339ed90b1b",\\n-                "sha256:e1574980b48c8c74f83578d1e77e701f8439a5d93f36a5a0af31337467c08fcf",\\n-                "sha256:e74a578172525c20d7223eac5f8ad187f10940dac06e40113d62f14f3adb1e8f",\\n-                "sha256:e945de62917acbf853ab968d8916290548df18dd62c739d862f359ecd25842a6",\\n-                "sha256:f0980d44b8aded808bec5059018d64692f0127f10510eca71f2f0ace8fb11188",\\n-                "sha256:f98d4bd7bbb15ca701d19b93263cc5edfd480c3475d163f137385f49e5b3a3a7",\\n-                "sha256:fb68d212efd057596dee9e6582daded9f8ef776538afdf5feceb3059df2d2e7b"\\n-            ],\\n-            "markers": "python_version >= \\\'2.7\\\' and python_version not in \\\'3.0, 3.1, 3.2, 3.3, 3.4\\\'",\\n-            "version": "==5.5.2"\\n-        }\\n-    },\\n-    "develop": {}\\n-}\\ndiff --git a/README.md b/README.md\\ndeleted file mode 100644\\nindex 624ca00..0000000\\n--- a/README.md\\n+++ /dev/null\\n@@ -1,170 +0,0 @@\\n-# Face Recognition using ARCFACE-Pytorch\\n-\\n-## Introduction\\n- This repo contains face_verify.py and app.py which is able to perform the following task -\\n- - Detect faces from an image, video or in webcam and perform face recogntion.\\n- - app.py was used to deploy the project.\\n- \\n-## Required Files\\n-- requirements.txt\\n-- pretrained model [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate) or [Mobilefacenet @ OneDrive](https://onedrive.live.com/?authkey=%21AIweh1IfiuF9vm4&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21836&parId=root&o=OneUp).\\n-- Custom dataset\\n-- Newly Trained model (facebank.pth and names.npy)\\n-\\n-\\n-## User Instruction\\n-After downloading the project first you have to install the following libraries.\\n-### Installation\\n-You can install all the dependencies at once by running the following command from your terminal.\\n-``` python\\n-    $ pip install -r requirements.txt\\n-```\\n-##### For the installation of torch using "pip" run the following command\\n-\\n-``` python\\n-    $ pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\\n-```\\n-### Project Setup\\n-\\n-#### pre-trained model\\n-Although i provided the pretrained model in the <b> work_space/model </b> and <b> work_space/save </b> folder, if you want to download the models you can follow the following url:\\n-\\n-- [IR-SE50 @ BaiduNetdisk](https://pan.baidu.com/s/12BUjjwy1uUTEF9HCx5qvoQ)\\n-- [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate)\\n-- [Mobilefacenet @ BaiduNetDisk](https://pan.baidu.com/s/1hqNNkcAjQOSxUjofboN6qg)\\n-- [Mobilefacenet @ OneDrive](https://onedrive.live.com/?authkey=%21AIweh1IfiuF9vm4&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21836&parId=root&o=OneUp)\\n-\\n-I have used the <b>IR-SE50</b> as the pretrained model to train with my custom dataset. You need to copy the pretrained model and save it under the <b> work_space/save </b> folder as <b> model_final.pth</b>\\n-\\n-#### Newly trained model\\n-In the <b> data/facebank </b> you will find a trained model named <b> "facebank.pth" </b> which contains the related weights and "names.npy" contains the corresponding labels of the users that are avialable in the facebank folder. For instance in this case\\n-the <b> facebank </b> folder will look like this :-\\n-\\n-    facebank/\\n-                ---> Chandler\\n-                ---> Joey\\n-                ---> Monica\\n-                ---> Phoebe\\n-                ---> Pias\\n-                ---> Rachel\\n-                ---> Raihan\\n-                ---> Ross\\n-                ---> Samiur\\n-                ---> Shakil\\n-                ---> facebank.pth\\n-                ---> names.npy\\n-\\n-If you have the "facebank.pth" and "names.npy" files in the <b>data/facebank</b> you can execute the following command to see the demo.\\n-\\n-```python\\n-    $ python app.py\\n- ```\\n- and go to the following url from your web browser.\\n- ```url\\n-http://localhost:5000\\n-```\\n-\\n-\\n-<hr>\\n-Note: If you want to run the inference on a video, download a video of related persons (Person that you trained the model with) and replace 0 in the line number 43 of <b> face_verify.py </b> with the path of your video file. For this code you can run the inference on any video of <b> Friends</b> tv series.\\n-<hr>\\n-\\n-#### Now if you want to train with your custom dataset, you need to follow the following steps.\\n-\\n-#### Dataset preparation \\n-\\n-First organize your images within the following manner- \\n-\\n-    data/\\n-        raw/\\n-             name1/\\n-                 photo1.jpg\\n-                 photo2.jpg\\n-                 ...\\n-             name2/\\n-                 photo1.jpg\\n-                 photo2.jpg\\n-                 ...\\n-             .....\\n-now run the following command\\n-```python\\n-$ python .\\\\create-dataset\\\\align_dataset_mtcnn.py data/raw/ data/processed --image_size 112\\n-```\\n-\\n-You will see a new folder inside the data directory named <b> "processed" </b> which will hold all the images that contains only faces of each user. If more than 1 image appears in any folder for a person, average embedding will be calculated. \\n-\\n-After executing the script new images for each user in the processed folder will look something like this.\\n-<p align="center"> \\n-<b> Cropped Images of faces </b>\\n-    <img src ="http://muizzer07.pythonanywhere.com/media/files/Picture1.png">\\n-</p> \\n-\\n-Copy all the folders of the users under the <b>data/processed</b> folder and paste in the <b>data/facebank</b> folder.\\n-\\n-\\n-Now to train with your dataset, you need to set <b> args.update == True </b> in line 35 of face_verify.py . After training you will get a new facebank.pth and names.npy in your data/facebank folder which will now only holds the weights and labels of your newly trained dataset. Once the training is done you need to reset <b> args.update==False</b>.\\n-However, if this doesn\\\'t work change the code in following manner-\\n-#### Old Code \\n-```python\\n-    if args.update:\\n-        targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\\n-        print(\\\'facebank updated\\\')\\n-    else:\\n-        targets, names = load_facebank(conf)\\n-        print(\\\'facebank loaded\\\')\\n-```\\n-#### New Code \\n-Only keep the follwing lines for training, once the training is done just replace it with the old code.\\n-```python\\n-        targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\\n-        print(\\\'facebank updated\\\')\\n-````\\n-Or you can simply pass a command line arguement such as below if there is new data to train.\\n-```python\\n-   $python face_verify.py -u\\n-```\\n-Here the -u parse the command to update the facebank.pth and names.npy.\\n-\\n-Now you are ready to test the systen with your newly trained users by running-\\n-\\n-```python\\n-    $ python app.py\\n-```\\n-\\n-### Note: You can train with custom dataset as many time as you want, you will only require any of the pre-trained model to train with your custom dataset to generate the <b>facebank.pth</b> and <b>names.npy</b>. Once you get this two files you are ready to test the face recogniton.\\n-\\n-\\n-<hr>\\n-\\n-### Retrain the pre-trained model\\n-\\n- If you want to build a new pre-trained model like [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate) and reproduce the result, you will need the large files which contains several dataset of faces under the <b> data/faces_emore </b>.\\n- \\n- To get these files, first you need to download the [MS1M](https://arxiv.org/abs/1607.08221) dataset from any of the following url-\\n-- [emore dataset @ Dropbox](https://www.dropbox.com/s/wpx6tqjf0y5mf6r/faces_ms1m-refine-v2_112x112.zip?dl=0)\\n-- [emore dataset @ BaiduDrive](https://pan.baidu.com/s/1eXohwNBHbbKXh5KHyItVhQ)\\n-\\n-After unzipping the downloaded file execute the following command. It will take few hours depending on your system configuration.\\n-\\n-```python\\n-    $ python prepare_data.py\\n-```\\n-After that you will see the following files to the "data/faces_emore" folder. \\n-\\n-    faces_emore/\\n-                ---> agedb_30\\n-                ---> calfw\\n-                ---> cfp_ff\\n-                ---> cfp_fp\\n-                ---> cfp_fp\\n-                ---> cplfw\\n-                ---> imgs\\n-                ---> lfw\\n-                ---> vgg2_fp\\n-\\n-To know the further training procedure you can see the details in this [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch) repository.\\n-\\n-## References\\n-- [Arcface](https://arxiv.org/pdf/1801.07698.pdf)\\n-- [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)\\n-- [The one with Face Recognition.](https://towardsdatascience.com/s01e01-3eb397d458d)\\ndiff --git a/app.py b/app.py\\ndeleted file mode 100644\\nindex c84f09b..0000000\\n--- a/app.py\\n+++ /dev/null\\n@@ -1,31 +0,0 @@\\n-# Flask utils\\n-from flask import Flask, redirect, url_for, request, render_template, Response\\n-from werkzeug.utils import secure_filename\\n-from gevent.pywsgi import WSGIServer\\n-from face_verify import faceRec\\n-\\n-camera = faceRec()\\n-\\n-app = Flask(__name__)\\n-\\n-@app.route("/")\\n-def main():\\n-    return render_template("index.html")\\n-\\n-def gen(camera):\\n-    while True:\\n-        frame = camera.main()\\n-        if frame != "":\\n-            global_frame = frame\\n-            yield (b\\\'--frame\\\\r\\\\n\\\'\\n-                    b\\\'Content-Type: image/jpeg\\\\r\\\\n\\\\r\\\\n\\\' + frame + b\\\'\\\\r\\\\n\\\\r\\\\n\\\')\\n-\\n-@app.route(\\\'/video_feed\\\')\\n-def video_feed():\\n-    return Response(gen(camera), mimetype=\\\'multipart/x-mixed-replace; boundary=frame\\\')\\n-\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    # Serve the app with gevent\\n-    http_server = WSGIServer((\\\'localhost\\\', 5000), app)\\n-    http_server.serve_forever()\\n\\\\ No newline at end of file\\ndiff --git a/config.py b/config.py\\ndeleted file mode 100644\\nindex 656f459..0000000\\n--- a/config.py\\n+++ /dev/null\\n@@ -1,52 +0,0 @@\\n-from easydict import EasyDict as edict\\n-from pathlib import Path\\n-import torch\\n-from torch.nn import CrossEntropyLoss\\n-from torchvision import transforms as trans\\n-\\n-def get_config(training = True):\\n-    conf = edict()\\n-    conf.data_path = Path(\\\'data\\\')\\n-    conf.work_path = Path(\\\'work_space/\\\')\\n-    conf.model_path = conf.work_path/\\\'models\\\'\\n-    conf.log_path = conf.work_path/\\\'log\\\'\\n-    conf.save_path = conf.work_path/\\\'save\\\'\\n-    conf.input_size = [112,112]\\n-    conf.embedding_size = 512\\n-    conf.use_mobilfacenet = False\\n-    conf.facebank_path = conf.data_path/\\\'facebank\\\'\\n-    conf.net_depth = 50\\n-    conf.drop_ratio = 0.6\\n-    conf.net_mode = \\\'ir_se\\\' # or \\\'ir\\\'\\n-    conf.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\\n-    conf.test_transform = trans.Compose([\\n-                    trans.ToTensor(),\\n-                    trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\n-                ])\\n-    conf.data_mode = \\\'emore\\\'\\n-    conf.vgg_folder = conf.data_path/\\\'faces_vgg_112x112\\\'\\n-    conf.ms1m_folder = conf.data_path/\\\'faces_ms1m_112x112\\\'\\n-    conf.emore_folder = conf.data_path/\\\'faces_emore\\\'\\n-    conf.batch_size = 100 # irse net depth 50 \\n-#   conf.batch_size = 200 # mobilefacenet\\n-#--------------------Training Config ------------------------    \\n-    if training:        \\n-        conf.log_path = conf.work_path/\\\'log\\\'\\n-        conf.save_path = conf.work_path/\\\'save\\\'\\n-    #     conf.weight_decay = 5e-4\\n-        conf.lr = 1e-3\\n-        conf.milestones = [12,15,18]\\n-        conf.momentum = 0.9\\n-        conf.pin_memory = True\\n-#         conf.num_workers = 4 # when batchsize is 200\\n-        conf.num_workers = 3\\n-        conf.ce_loss = CrossEntropyLoss()    \\n-#--------------------Inference Config ------------------------\\n-    else:\\n-        conf.facebank_path = conf.data_path/\\\'facebank\\\'\\n-        conf.threshold = 0.9\\n-        conf.face_limit = 10 \\n-        #when inference, at maximum detect 10 faces in one image,\\n-        conf.min_face_size = 35\\n-        # the larger this value, the faster deduction, comes with tradeoff in small faces\\n-    return conf\\ndiff --git a/create-dataset/align/__init__.py b/create-dataset/align/__init__.py\\ndeleted file mode 100644\\nindex e69de29..0000000\\ndiff --git a/create-dataset/align/align_dataset_mtcnn.py b/create-dataset/align/align_dataset_mtcnn.py\\ndeleted file mode 100644\\nindex 7d5e735..0000000\\n--- a/create-dataset/align/align_dataset_mtcnn.py\\n+++ /dev/null\\n@@ -1,159 +0,0 @@\\n-"""Performs face alignment and stores face thumbnails in the output directory."""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from scipy import misc\\n-import sys\\n-import os\\n-import argparse\\n-import tensorflow as tf\\n-import numpy as np\\n-import facenet\\n-import align.detect_face\\n-import random\\n-from time import sleep\\n-\\n-def main(args):\\n-    sleep(random.random())\\n-    output_dir = os.path.expanduser(args.output_dir)\\n-    if not os.path.exists(output_dir):\\n-        os.makedirs(output_dir)\\n-    # Store some git revision info in a text file in the log directory\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\n-    facenet.store_revision_info(src_path, output_dir, \\\' \\\'.join(sys.argv))\\n-    dataset = facenet.get_dataset(args.input_dir)\\n-    \\n-    print(\\\'Creating networks and loading parameters\\\')\\n-    \\n-    with tf.Graph().as_default():\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-        with sess.as_default():\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\\n-    \\n-    minsize = 20 # minimum size of face\\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\\\'s threshold\\n-    factor = 0.709 # scale factor\\n-\\n-    # Add a random key to the filename to allow alignment using multiple processes\\n-    random_key = np.random.randint(0, high=99999)\\n-    bounding_boxes_filename = os.path.join(output_dir, \\\'bounding_boxes_%05d.txt\\\' % random_key)\\n-    \\n-    with open(bounding_boxes_filename, "w") as text_file:\\n-        nrof_images_total = 0\\n-        nrof_successfully_aligned = 0\\n-        if args.random_order:\\n-            random.shuffle(dataset)\\n-        for cls in dataset:\\n-            output_class_dir = os.path.join(output_dir, cls.name)\\n-            if not os.path.exists(output_class_dir):\\n-                os.makedirs(output_class_dir)\\n-                if args.random_order:\\n-                    random.shuffle(cls.image_paths)\\n-            for image_path in cls.image_paths:\\n-                nrof_images_total += 1\\n-                filename = os.path.splitext(os.path.split(image_path)[1])[0]\\n-                output_filename = os.path.join(output_class_dir, filename+\\\'.png\\\')\\n-                print(image_path)\\n-                if not os.path.exists(output_filename):\\n-                    try:\\n-                        img = misc.imread(image_path)\\n-                    except (IOError, ValueError, IndexError) as e:\\n-                        errorMessage = \\\'{}: {}\\\'.format(image_path, e)\\n-                        print(errorMessage)\\n-                    else:\\n-                        if img.ndim<2:\\n-                            print(\\\'Unable to align "%s"\\\' % image_path)\\n-                            text_file.write(\\\'%s\\\\n\\\' % (output_filename))\\n-                            continue\\n-                        if img.ndim == 2:\\n-                            img = facenet.to_rgb(img)\\n-                        img = img[:,:,0:3]\\n-    \\n-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\\n-                        nrof_faces = bounding_boxes.shape[0]\\n-                        if nrof_faces>0:\\n-                            det = bounding_boxes[:,0:4]\\n-                            det_arr = []\\n-                            img_size = np.asarray(img.shape)[0:2]\\n-                            if nrof_faces>1:\\n-                                if args.detect_multiple_faces:\\n-                                    for i in range(nrof_faces):\\n-                                        det_arr.append(np.squeeze(det[i]))\\n-                                else:\\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\\n-                                    img_center = img_size / 2\\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\\n-                                    det_arr.append(det[index,:])\\n-                            else:\\n-                                det_arr.append(np.squeeze(det))\\n-\\n-                            for i, det in enumerate(det_arr):\\n-                                det = np.squeeze(det)\\n-                                bb = np.zeros(4, dtype=np.int32)\\n-                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\\n-                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\\n-                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\\n-                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\\\'bilinear\\\')\\n-                                nrof_successfully_aligned += 1\\n-                                filename_base, file_extension = os.path.splitext(output_filename)\\n-                                if args.detect_multiple_faces:\\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\\n-                                else:\\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\\n-                                misc.imsave(output_filename_n, scaled)\\n-                                text_file.write(\\\'%s %d %d %d %d\\\\n\\\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\\n-                        else:\\n-                            print(\\\'Unable to align "%s"\\\' % image_path)\\n-                            text_file.write(\\\'%s\\\\n\\\' % (output_filename))\\n-                            \\n-    print(\\\'Total number of images: %d\\\' % nrof_images_total)\\n-    print(\\\'Number of successfully aligned images: %d\\\' % nrof_successfully_aligned)\\n-            \\n-\\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'input_dir\\\', type=str, help=\\\'Directory with unaligned images.\\\')\\n-    parser.add_argument(\\\'output_dir\\\', type=str, help=\\\'Directory with aligned face thumbnails.\\\')\\n-    parser.add_argument(\\\'--image_size\\\', type=int,\\n-        help=\\\'Image size (height, width) in pixels.\\\', default=182)\\n-    parser.add_argument(\\\'--margin\\\', type=int,\\n-        help=\\\'Margin for the crop around the bounding box (height, width) in pixels.\\\', default=44)\\n-    parser.add_argument(\\\'--random_order\\\', \\n-        help=\\\'Shuffles the order of images to enable alignment using multiple processes.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--gpu_memory_fraction\\\', type=float,\\n-        help=\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\', default=1.0)\\n-    parser.add_argument(\\\'--detect_multiple_faces\\\', type=bool,\\n-                        help=\\\'Detect and align multiple faces per image.\\\', default=False)\\n-    return parser.parse_args(argv)\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/create-dataset/align/det1.npy b/create-dataset/align/det1.npy\\ndeleted file mode 100644\\nindex 7c05a2c..0000000\\nBinary files a/create-dataset/align/det1.npy and /dev/null differ\\ndiff --git a/create-dataset/align/det2.npy b/create-dataset/align/det2.npy\\ndeleted file mode 100644\\nindex 85d5bf0..0000000\\nBinary files a/create-dataset/align/det2.npy and /dev/null differ\\ndiff --git a/create-dataset/align/det3.npy b/create-dataset/align/det3.npy\\ndeleted file mode 100644\\nindex 90d5ba9..0000000\\nBinary files a/create-dataset/align/det3.npy and /dev/null differ\\ndiff --git a/create-dataset/align/detect_face.py b/create-dataset/align/detect_face.py\\ndeleted file mode 100644\\nindex 7f98ca7..0000000\\n--- a/create-dataset/align/detect_face.py\\n+++ /dev/null\\n@@ -1,781 +0,0 @@\\n-""" Tensorflow implementation of the face detection / alignment algorithm found at\\n-https://github.com/kpzhang93/MTCNN_face_detection_alignment\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-from six import string_types, iteritems\\n-\\n-import numpy as np\\n-import tensorflow as tf\\n-#from math import floor\\n-import cv2\\n-import os\\n-\\n-def layer(op):\\n-    """Decorator for composable network layers."""\\n-\\n-    def layer_decorated(self, *args, **kwargs):\\n-        # Automatically set a name if not provided.\\n-        name = kwargs.setdefault(\\\'name\\\', self.get_unique_name(op.__name__))\\n-        # Figure out the layer inputs.\\n-        if len(self.terminals) == 0:\\n-            raise RuntimeError(\\\'No input variables found for layer %s.\\\' % name)\\n-        elif len(self.terminals) == 1:\\n-            layer_input = self.terminals[0]\\n-        else:\\n-            layer_input = list(self.terminals)\\n-        # Perform the operation and get the output.\\n-        layer_output = op(self, layer_input, *args, **kwargs)\\n-        # Add to layer LUT.\\n-        self.layers[name] = layer_output\\n-        # This output is now the input for the next layer.\\n-        self.feed(layer_output)\\n-        # Return self for chained calls.\\n-        return self\\n-\\n-    return layer_decorated\\n-\\n-class Network(object):\\n-\\n-    def __init__(self, inputs, trainable=True):\\n-        # The input nodes for this network\\n-        self.inputs = inputs\\n-        # The current list of terminal nodes\\n-        self.terminals = []\\n-        # Mapping from layer names to layers\\n-        self.layers = dict(inputs)\\n-        # If true, the resulting variables are set as trainable\\n-        self.trainable = trainable\\n-\\n-        self.setup()\\n-\\n-    def setup(self):\\n-        """Construct the network. """\\n-        raise NotImplementedError(\\\'Must be implemented by the subclass.\\\')\\n-\\n-    def load(self, data_path, session, ignore_missing=False):\\n-        """Load network weights.\\n-        data_path: The path to the numpy-serialized network weights\\n-        session: The current TensorFlow session\\n-        ignore_missing: If true, serialized weights for missing layers are ignored.\\n-        """\\n-        data_dict = np.load(data_path, encoding=\\\'latin1\\\').item() #pylint: disable=no-member\\n-\\n-        for op_name in data_dict:\\n-            with tf.variable_scope(op_name, reuse=True):\\n-                for param_name, data in iteritems(data_dict[op_name]):\\n-                    try:\\n-                        var = tf.get_variable(param_name)\\n-                        session.run(var.assign(data))\\n-                    except ValueError:\\n-                        if not ignore_missing:\\n-                            raise\\n-\\n-    def feed(self, *args):\\n-        """Set the input(s) for the next operation by replacing the terminal nodes.\\n-        The arguments can be either layer names or the actual layers.\\n-        """\\n-        assert len(args) != 0\\n-        self.terminals = []\\n-        for fed_layer in args:\\n-            if isinstance(fed_layer, string_types):\\n-                try:\\n-                    fed_layer = self.layers[fed_layer]\\n-                except KeyError:\\n-                    raise KeyError(\\\'Unknown layer name fed: %s\\\' % fed_layer)\\n-            self.terminals.append(fed_layer)\\n-        return self\\n-\\n-    def get_output(self):\\n-        """Returns the current network output."""\\n-        return self.terminals[-1]\\n-\\n-    def get_unique_name(self, prefix):\\n-        """Returns an index-suffixed unique name for the given prefix.\\n-        This is used for auto-generating layer names based on the type-prefix.\\n-        """\\n-        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\\n-        return \\\'%s_%d\\\' % (prefix, ident)\\n-\\n-    def make_var(self, name, shape):\\n-        """Creates a new TensorFlow variable."""\\n-        return tf.get_variable(name, shape, trainable=self.trainable)\\n-\\n-    def validate_padding(self, padding):\\n-        """Verifies that the padding is one of the supported ones."""\\n-        assert padding in (\\\'SAME\\\', \\\'VALID\\\')\\n-\\n-    @layer\\n-    def conv(self,\\n-             inp,\\n-             k_h,\\n-             k_w,\\n-             c_o,\\n-             s_h,\\n-             s_w,\\n-             name,\\n-             relu=True,\\n-             padding=\\\'SAME\\\',\\n-             group=1,\\n-             biased=True):\\n-        # Verify that the padding is acceptable\\n-        self.validate_padding(padding)\\n-        # Get the number of channels in the input\\n-        c_i = int(inp.get_shape()[-1])\\n-        # Verify that the grouping parameter is valid\\n-        assert c_i % group == 0\\n-        assert c_o % group == 0\\n-        # Convolution for a given input and kernel\\n-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\\n-        with tf.variable_scope(name) as scope:\\n-            kernel = self.make_var(\\\'weights\\\', shape=[k_h, k_w, c_i // group, c_o])\\n-            # This is the common-case. Convolve the input without any further complications.\\n-            output = convolve(inp, kernel)\\n-            # Add the biases\\n-            if biased:\\n-                biases = self.make_var(\\\'biases\\\', [c_o])\\n-                output = tf.nn.bias_add(output, biases)\\n-            if relu:\\n-                # ReLU non-linearity\\n-                output = tf.nn.relu(output, name=scope.name)\\n-            return output\\n-\\n-    @layer\\n-    def prelu(self, inp, name):\\n-        with tf.variable_scope(name):\\n-            i = int(inp.get_shape()[-1])\\n-            alpha = self.make_var(\\\'alpha\\\', shape=(i,))\\n-            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\\n-        return output\\n-\\n-    @layer\\n-    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\\\'SAME\\\'):\\n-        self.validate_padding(padding)\\n-        return tf.nn.max_pool(inp,\\n-                              ksize=[1, k_h, k_w, 1],\\n-                              strides=[1, s_h, s_w, 1],\\n-                              padding=padding,\\n-                              name=name)\\n-\\n-    @layer\\n-    def fc(self, inp, num_out, name, relu=True):\\n-        with tf.variable_scope(name):\\n-            input_shape = inp.get_shape()\\n-            if input_shape.ndims == 4:\\n-                # The input is spatial. Vectorize it first.\\n-                dim = 1\\n-                for d in input_shape[1:].as_list():\\n-                    dim *= int(d)\\n-                feed_in = tf.reshape(inp, [-1, dim])\\n-            else:\\n-                feed_in, dim = (inp, input_shape[-1].value)\\n-            weights = self.make_var(\\\'weights\\\', shape=[dim, num_out])\\n-            biases = self.make_var(\\\'biases\\\', [num_out])\\n-            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\\n-            fc = op(feed_in, weights, biases, name=name)\\n-            return fc\\n-\\n-\\n-    """\\n-    Multi dimensional softmax,\\n-    refer to https://github.com/tensorflow/tensorflow/issues/210\\n-    compute softmax along the dimension of target\\n-    the native softmax only supports batch_size x dimension\\n-    """\\n-    @layer\\n-    def softmax(self, target, axis, name=None):\\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\\n-        target_exp = tf.exp(target-max_axis)\\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\\n-        softmax = tf.div(target_exp, normalize, name)\\n-        return softmax\\n-    \\n-class PNet(Network):\\n-    def setup(self):\\n-        (self.feed(\\\'data\\\') #pylint: disable=no-value-for-parameter, no-member\\n-             .conv(3, 3, 10, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv1\\\')\\n-             .prelu(name=\\\'PReLU1\\\')\\n-             .max_pool(2, 2, 2, 2, name=\\\'pool1\\\')\\n-             .conv(3, 3, 16, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv2\\\')\\n-             .prelu(name=\\\'PReLU2\\\')\\n-             .conv(3, 3, 32, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv3\\\')\\n-             .prelu(name=\\\'PReLU3\\\')\\n-             .conv(1, 1, 2, 1, 1, relu=False, name=\\\'conv4-1\\\')\\n-             .softmax(3,name=\\\'prob1\\\'))\\n-\\n-        (self.feed(\\\'PReLU3\\\') #pylint: disable=no-value-for-parameter\\n-             .conv(1, 1, 4, 1, 1, relu=False, name=\\\'conv4-2\\\'))\\n-        \\n-class RNet(Network):\\n-    def setup(self):\\n-        (self.feed(\\\'data\\\') #pylint: disable=no-value-for-parameter, no-member\\n-             .conv(3, 3, 28, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv1\\\')\\n-             .prelu(name=\\\'prelu1\\\')\\n-             .max_pool(3, 3, 2, 2, name=\\\'pool1\\\')\\n-             .conv(3, 3, 48, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv2\\\')\\n-             .prelu(name=\\\'prelu2\\\')\\n-             .max_pool(3, 3, 2, 2, padding=\\\'VALID\\\', name=\\\'pool2\\\')\\n-             .conv(2, 2, 64, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv3\\\')\\n-             .prelu(name=\\\'prelu3\\\')\\n-             .fc(128, relu=False, name=\\\'conv4\\\')\\n-             .prelu(name=\\\'prelu4\\\')\\n-             .fc(2, relu=False, name=\\\'conv5-1\\\')\\n-             .softmax(1,name=\\\'prob1\\\'))\\n-\\n-        (self.feed(\\\'prelu4\\\') #pylint: disable=no-value-for-parameter\\n-             .fc(4, relu=False, name=\\\'conv5-2\\\'))\\n-\\n-class ONet(Network):\\n-    def setup(self):\\n-        (self.feed(\\\'data\\\') #pylint: disable=no-value-for-parameter, no-member\\n-             .conv(3, 3, 32, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv1\\\')\\n-             .prelu(name=\\\'prelu1\\\')\\n-             .max_pool(3, 3, 2, 2, name=\\\'pool1\\\')\\n-             .conv(3, 3, 64, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv2\\\')\\n-             .prelu(name=\\\'prelu2\\\')\\n-             .max_pool(3, 3, 2, 2, padding=\\\'VALID\\\', name=\\\'pool2\\\')\\n-             .conv(3, 3, 64, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv3\\\')\\n-             .prelu(name=\\\'prelu3\\\')\\n-             .max_pool(2, 2, 2, 2, name=\\\'pool3\\\')\\n-             .conv(2, 2, 128, 1, 1, padding=\\\'VALID\\\', relu=False, name=\\\'conv4\\\')\\n-             .prelu(name=\\\'prelu4\\\')\\n-             .fc(256, relu=False, name=\\\'conv5\\\')\\n-             .prelu(name=\\\'prelu5\\\')\\n-             .fc(2, relu=False, name=\\\'conv6-1\\\')\\n-             .softmax(1, name=\\\'prob1\\\'))\\n-\\n-        (self.feed(\\\'prelu5\\\') #pylint: disable=no-value-for-parameter\\n-             .fc(4, relu=False, name=\\\'conv6-2\\\'))\\n-\\n-        (self.feed(\\\'prelu5\\\') #pylint: disable=no-value-for-parameter\\n-             .fc(10, relu=False, name=\\\'conv6-3\\\'))\\n-\\n-def create_mtcnn(sess, model_path):\\n-    if not model_path:\\n-        model_path,_ = os.path.split(os.path.realpath(__file__))\\n-\\n-    with tf.variable_scope(\\\'pnet\\\'):\\n-        data = tf.placeholder(tf.float32, (None,None,None,3), \\\'input\\\')\\n-        pnet = PNet({\\\'data\\\':data})\\n-        pnet.load(os.path.join(model_path, \\\'det1.npy\\\'), sess)\\n-    with tf.variable_scope(\\\'rnet\\\'):\\n-        data = tf.placeholder(tf.float32, (None,24,24,3), \\\'input\\\')\\n-        rnet = RNet({\\\'data\\\':data})\\n-        rnet.load(os.path.join(model_path, \\\'det2.npy\\\'), sess)\\n-    with tf.variable_scope(\\\'onet\\\'):\\n-        data = tf.placeholder(tf.float32, (None,48,48,3), \\\'input\\\')\\n-        onet = ONet({\\\'data\\\':data})\\n-        onet.load(os.path.join(model_path, \\\'det3.npy\\\'), sess)\\n-        \\n-    pnet_fun = lambda img : sess.run((\\\'pnet/conv4-2/BiasAdd:0\\\', \\\'pnet/prob1:0\\\'), feed_dict={\\\'pnet/input:0\\\':img})\\n-    rnet_fun = lambda img : sess.run((\\\'rnet/conv5-2/conv5-2:0\\\', \\\'rnet/prob1:0\\\'), feed_dict={\\\'rnet/input:0\\\':img})\\n-    onet_fun = lambda img : sess.run((\\\'onet/conv6-2/conv6-2:0\\\', \\\'onet/conv6-3/conv6-3:0\\\', \\\'onet/prob1:0\\\'), feed_dict={\\\'onet/input:0\\\':img})\\n-    return pnet_fun, rnet_fun, onet_fun\\n-\\n-def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\\n-    """Detects faces in an image, and returns bounding boxes and points for them.\\n-    img: input image\\n-    minsize: minimum faces\\\' size\\n-    pnet, rnet, onet: caffemodel\\n-    threshold: threshold=[th1, th2, th3], th1-3 are three steps\\\'s threshold\\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\\n-    """\\n-    factor_count=0\\n-    total_boxes=np.empty((0,9))\\n-    points=np.empty(0)\\n-    h=img.shape[0]\\n-    w=img.shape[1]\\n-    minl=np.amin([h, w])\\n-    m=12.0/minsize\\n-    minl=minl*m\\n-    # create scale pyramid\\n-    scales=[]\\n-    while minl>=12:\\n-        scales += [m*np.power(factor, factor_count)]\\n-        minl = minl*factor\\n-        factor_count += 1\\n-\\n-    # first stage\\n-    for scale in scales:\\n-        hs=int(np.ceil(h*scale))\\n-        ws=int(np.ceil(w*scale))\\n-        im_data = imresample(img, (hs, ws))\\n-        im_data = (im_data-127.5)*0.0078125\\n-        img_x = np.expand_dims(im_data, 0)\\n-        img_y = np.transpose(img_x, (0,2,1,3))\\n-        out = pnet(img_y)\\n-        out0 = np.transpose(out[0], (0,2,1,3))\\n-        out1 = np.transpose(out[1], (0,2,1,3))\\n-        \\n-        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\\n-        \\n-        # inter-scale nms\\n-        pick = nms(boxes.copy(), 0.5, \\\'Union\\\')\\n-        if boxes.size>0 and pick.size>0:\\n-            boxes = boxes[pick,:]\\n-            total_boxes = np.append(total_boxes, boxes, axis=0)\\n-\\n-    numbox = total_boxes.shape[0]\\n-    if numbox>0:\\n-        pick = nms(total_boxes.copy(), 0.7, \\\'Union\\\')\\n-        total_boxes = total_boxes[pick,:]\\n-        regw = total_boxes[:,2]-total_boxes[:,0]\\n-        regh = total_boxes[:,3]-total_boxes[:,1]\\n-        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\\n-        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\\n-        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\\n-        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\\n-        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\\n-        total_boxes = rerec(total_boxes.copy())\\n-        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\\n-\\n-    numbox = total_boxes.shape[0]\\n-    if numbox>0:\\n-        # second stage\\n-        tempimg = np.zeros((24,24,3,numbox))\\n-        for k in range(0,numbox):\\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\\n-                tempimg[:,:,:,k] = imresample(tmp, (24, 24))\\n-            else:\\n-                return np.empty()\\n-        tempimg = (tempimg-127.5)*0.0078125\\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\\n-        out = rnet(tempimg1)\\n-        out0 = np.transpose(out[0])\\n-        out1 = np.transpose(out[1])\\n-        score = out1[1,:]\\n-        ipass = np.where(score>threshold[1])\\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\\n-        mv = out0[:,ipass[0]]\\n-        if total_boxes.shape[0]>0:\\n-            pick = nms(total_boxes, 0.7, \\\'Union\\\')\\n-            total_boxes = total_boxes[pick,:]\\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\\n-            total_boxes = rerec(total_boxes.copy())\\n-\\n-    numbox = total_boxes.shape[0]\\n-    if numbox>0:\\n-        # third stage\\n-        total_boxes = np.fix(total_boxes).astype(np.int32)\\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\\n-        tempimg = np.zeros((48,48,3,numbox))\\n-        for k in range(0,numbox):\\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\\n-                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\\n-            else:\\n-                return np.empty()\\n-        tempimg = (tempimg-127.5)*0.0078125\\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\\n-        out = onet(tempimg1)\\n-        out0 = np.transpose(out[0])\\n-        out1 = np.transpose(out[1])\\n-        out2 = np.transpose(out[2])\\n-        score = out2[1,:]\\n-        points = out1\\n-        ipass = np.where(score>threshold[2])\\n-        points = points[:,ipass[0]]\\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\\n-        mv = out0[:,ipass[0]]\\n-\\n-        w = total_boxes[:,2]-total_boxes[:,0]+1\\n-        h = total_boxes[:,3]-total_boxes[:,1]+1\\n-        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\\n-        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\\n-        if total_boxes.shape[0]>0:\\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\\n-            pick = nms(total_boxes.copy(), 0.7, \\\'Min\\\')\\n-            total_boxes = total_boxes[pick,:]\\n-            points = points[:,pick]\\n-                \\n-    return total_boxes, points\\n-\\n-\\n-def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\\n-    """Detects faces in a list of images\\n-    images: list containing input images\\n-    detection_window_size_ratio: ratio of minimum face size to smallest image dimension\\n-    pnet, rnet, onet: caffemodel\\n-    threshold: threshold=[th1 th2 th3], th1-3 are three steps\\\'s threshold [0-1]\\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\\n-    """\\n-    all_scales = [None] * len(images)\\n-    images_with_boxes = [None] * len(images)\\n-\\n-    for i in range(len(images)):\\n-        images_with_boxes[i] = {\\\'total_boxes\\\': np.empty((0, 9))}\\n-\\n-    # create scale pyramid\\n-    for index, img in enumerate(images):\\n-        all_scales[index] = []\\n-        h = img.shape[0]\\n-        w = img.shape[1]\\n-        minsize = int(detection_window_size_ratio * np.minimum(w, h))\\n-        factor_count = 0\\n-        minl = np.amin([h, w])\\n-        if minsize <= 12:\\n-            minsize = 12\\n-\\n-        m = 12.0 / minsize\\n-        minl = minl * m\\n-        while minl >= 12:\\n-            all_scales[index].append(m * np.power(factor, factor_count))\\n-            minl = minl * factor\\n-            factor_count += 1\\n-\\n-    # # # # # # # # # # # # #\\n-    # first stage - fast proposal network (pnet) to obtain face candidates\\n-    # # # # # # # # # # # # #\\n-\\n-    images_obj_per_resolution = {}\\n-\\n-    # TODO: use some type of rounding to number module 8 to increase probability that pyramid images will have the same resolution across input images\\n-\\n-    for index, scales in enumerate(all_scales):\\n-        h = images[index].shape[0]\\n-        w = images[index].shape[1]\\n-\\n-        for scale in scales:\\n-            hs = int(np.ceil(h * scale))\\n-            ws = int(np.ceil(w * scale))\\n-\\n-            if (ws, hs) not in images_obj_per_resolution:\\n-                images_obj_per_resolution[(ws, hs)] = []\\n-\\n-            im_data = imresample(images[index], (hs, ws))\\n-            im_data = (im_data - 127.5) * 0.0078125\\n-            img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\\n-            images_obj_per_resolution[(ws, hs)].append({\\\'scale\\\': scale, \\\'image\\\': img_y, \\\'index\\\': index})\\n-\\n-    for resolution in images_obj_per_resolution:\\n-        images_per_resolution = [i[\\\'image\\\'] for i in images_obj_per_resolution[resolution]]\\n-        outs = pnet(images_per_resolution)\\n-\\n-        for index in range(len(outs[0])):\\n-            scale = images_obj_per_resolution[resolution][index][\\\'scale\\\']\\n-            image_index = images_obj_per_resolution[resolution][index][\\\'index\\\']\\n-            out0 = np.transpose(outs[0][index], (1, 0, 2))\\n-            out1 = np.transpose(outs[1][index], (1, 0, 2))\\n-\\n-            boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\\n-\\n-            # inter-scale nms\\n-            pick = nms(boxes.copy(), 0.5, \\\'Union\\\')\\n-            if boxes.size > 0 and pick.size > 0:\\n-                boxes = boxes[pick, :]\\n-                images_with_boxes[image_index][\\\'total_boxes\\\'] = np.append(images_with_boxes[image_index][\\\'total_boxes\\\'],\\n-                                                                          boxes,\\n-                                                                          axis=0)\\n-\\n-    for index, image_obj in enumerate(images_with_boxes):\\n-        numbox = image_obj[\\\'total_boxes\\\'].shape[0]\\n-        if numbox > 0:\\n-            h = images[index].shape[0]\\n-            w = images[index].shape[1]\\n-            pick = nms(image_obj[\\\'total_boxes\\\'].copy(), 0.7, \\\'Union\\\')\\n-            image_obj[\\\'total_boxes\\\'] = image_obj[\\\'total_boxes\\\'][pick, :]\\n-            regw = image_obj[\\\'total_boxes\\\'][:, 2] - image_obj[\\\'total_boxes\\\'][:, 0]\\n-            regh = image_obj[\\\'total_boxes\\\'][:, 3] - image_obj[\\\'total_boxes\\\'][:, 1]\\n-            qq1 = image_obj[\\\'total_boxes\\\'][:, 0] + image_obj[\\\'total_boxes\\\'][:, 5] * regw\\n-            qq2 = image_obj[\\\'total_boxes\\\'][:, 1] + image_obj[\\\'total_boxes\\\'][:, 6] * regh\\n-            qq3 = image_obj[\\\'total_boxes\\\'][:, 2] + image_obj[\\\'total_boxes\\\'][:, 7] * regw\\n-            qq4 = image_obj[\\\'total_boxes\\\'][:, 3] + image_obj[\\\'total_boxes\\\'][:, 8] * regh\\n-            image_obj[\\\'total_boxes\\\'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj[\\\'total_boxes\\\'][:, 4]]))\\n-            image_obj[\\\'total_boxes\\\'] = rerec(image_obj[\\\'total_boxes\\\'].copy())\\n-            image_obj[\\\'total_boxes\\\'][:, 0:4] = np.fix(image_obj[\\\'total_boxes\\\'][:, 0:4]).astype(np.int32)\\n-            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\\\'total_boxes\\\'].copy(), w, h)\\n-\\n-            numbox = image_obj[\\\'total_boxes\\\'].shape[0]\\n-            tempimg = np.zeros((24, 24, 3, numbox))\\n-\\n-            if numbox > 0:\\n-                for k in range(0, numbox):\\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\\n-                        tempimg[:, :, :, k] = imresample(tmp, (24, 24))\\n-                    else:\\n-                        return np.empty()\\n-\\n-                tempimg = (tempimg - 127.5) * 0.0078125\\n-                image_obj[\\\'rnet_input\\\'] = np.transpose(tempimg, (3, 1, 0, 2))\\n-\\n-    # # # # # # # # # # # # #\\n-    # second stage - refinement of face candidates with rnet\\n-    # # # # # # # # # # # # #\\n-\\n-    bulk_rnet_input = np.empty((0, 24, 24, 3))\\n-    for index, image_obj in enumerate(images_with_boxes):\\n-        if \\\'rnet_input\\\' in image_obj:\\n-            bulk_rnet_input = np.append(bulk_rnet_input, image_obj[\\\'rnet_input\\\'], axis=0)\\n-\\n-    out = rnet(bulk_rnet_input)\\n-    out0 = np.transpose(out[0])\\n-    out1 = np.transpose(out[1])\\n-    score = out1[1, :]\\n-\\n-    i = 0\\n-    for index, image_obj in enumerate(images_with_boxes):\\n-        if \\\'rnet_input\\\' not in image_obj:\\n-            continue\\n-\\n-        rnet_input_count = image_obj[\\\'rnet_input\\\'].shape[0]\\n-        score_per_image = score[i:i + rnet_input_count]\\n-        out0_per_image = out0[:, i:i + rnet_input_count]\\n-\\n-        ipass = np.where(score_per_image > threshold[1])\\n-        image_obj[\\\'total_boxes\\\'] = np.hstack([image_obj[\\\'total_boxes\\\'][ipass[0], 0:4].copy(),\\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\\n-\\n-        mv = out0_per_image[:, ipass[0]]\\n-\\n-        if image_obj[\\\'total_boxes\\\'].shape[0] > 0:\\n-            h = images[index].shape[0]\\n-            w = images[index].shape[1]\\n-            pick = nms(image_obj[\\\'total_boxes\\\'], 0.7, \\\'Union\\\')\\n-            image_obj[\\\'total_boxes\\\'] = image_obj[\\\'total_boxes\\\'][pick, :]\\n-            image_obj[\\\'total_boxes\\\'] = bbreg(image_obj[\\\'total_boxes\\\'].copy(), np.transpose(mv[:, pick]))\\n-            image_obj[\\\'total_boxes\\\'] = rerec(image_obj[\\\'total_boxes\\\'].copy())\\n-\\n-            numbox = image_obj[\\\'total_boxes\\\'].shape[0]\\n-\\n-            if numbox > 0:\\n-                tempimg = np.zeros((48, 48, 3, numbox))\\n-                image_obj[\\\'total_boxes\\\'] = np.fix(image_obj[\\\'total_boxes\\\']).astype(np.int32)\\n-                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\\\'total_boxes\\\'].copy(), w, h)\\n-\\n-                for k in range(0, numbox):\\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\\n-                        tempimg[:, :, :, k] = imresample(tmp, (48, 48))\\n-                    else:\\n-                        return np.empty()\\n-                tempimg = (tempimg - 127.5) * 0.0078125\\n-                image_obj[\\\'onet_input\\\'] = np.transpose(tempimg, (3, 1, 0, 2))\\n-\\n-        i += rnet_input_count\\n-\\n-    # # # # # # # # # # # # #\\n-    # third stage - further refinement and facial landmarks positions with onet\\n-    # # # # # # # # # # # # #\\n-\\n-    bulk_onet_input = np.empty((0, 48, 48, 3))\\n-    for index, image_obj in enumerate(images_with_boxes):\\n-        if \\\'onet_input\\\' in image_obj:\\n-            bulk_onet_input = np.append(bulk_onet_input, image_obj[\\\'onet_input\\\'], axis=0)\\n-\\n-    out = onet(bulk_onet_input)\\n-\\n-    out0 = np.transpose(out[0])\\n-    out1 = np.transpose(out[1])\\n-    out2 = np.transpose(out[2])\\n-    score = out2[1, :]\\n-    points = out1\\n-\\n-    i = 0\\n-    ret = []\\n-    for index, image_obj in enumerate(images_with_boxes):\\n-        if \\\'onet_input\\\' not in image_obj:\\n-            ret.append(None)\\n-            continue\\n-\\n-        onet_input_count = image_obj[\\\'onet_input\\\'].shape[0]\\n-\\n-        out0_per_image = out0[:, i:i + onet_input_count]\\n-        score_per_image = score[i:i + onet_input_count]\\n-        points_per_image = points[:, i:i + onet_input_count]\\n-\\n-        ipass = np.where(score_per_image > threshold[2])\\n-        points_per_image = points_per_image[:, ipass[0]]\\n-\\n-        image_obj[\\\'total_boxes\\\'] = np.hstack([image_obj[\\\'total_boxes\\\'][ipass[0], 0:4].copy(),\\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\\n-        mv = out0_per_image[:, ipass[0]]\\n-\\n-        w = image_obj[\\\'total_boxes\\\'][:, 2] - image_obj[\\\'total_boxes\\\'][:, 0] + 1\\n-        h = image_obj[\\\'total_boxes\\\'][:, 3] - image_obj[\\\'total_boxes\\\'][:, 1] + 1\\n-        points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\\n-            image_obj[\\\'total_boxes\\\'][:, 0], (5, 1)) - 1\\n-        points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\\n-            image_obj[\\\'total_boxes\\\'][:, 1], (5, 1)) - 1\\n-\\n-        if image_obj[\\\'total_boxes\\\'].shape[0] > 0:\\n-            image_obj[\\\'total_boxes\\\'] = bbreg(image_obj[\\\'total_boxes\\\'].copy(), np.transpose(mv))\\n-            pick = nms(image_obj[\\\'total_boxes\\\'].copy(), 0.7, \\\'Min\\\')\\n-            image_obj[\\\'total_boxes\\\'] = image_obj[\\\'total_boxes\\\'][pick, :]\\n-            points_per_image = points_per_image[:, pick]\\n-\\n-            ret.append((image_obj[\\\'total_boxes\\\'], points_per_image))\\n-        else:\\n-            ret.append(None)\\n-\\n-        i += onet_input_count\\n-\\n-    return ret\\n-\\n-\\n-# function [boundingbox] = bbreg(boundingbox,reg)\\n-def bbreg(boundingbox,reg):\\n-    """Calibrate bounding boxes"""\\n-    if reg.shape[1]==1:\\n-        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\\n-\\n-    w = boundingbox[:,2]-boundingbox[:,0]+1\\n-    h = boundingbox[:,3]-boundingbox[:,1]+1\\n-    b1 = boundingbox[:,0]+reg[:,0]*w\\n-    b2 = boundingbox[:,1]+reg[:,1]*h\\n-    b3 = boundingbox[:,2]+reg[:,2]*w\\n-    b4 = boundingbox[:,3]+reg[:,3]*h\\n-    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\\n-    return boundingbox\\n- \\n-def generateBoundingBox(imap, reg, scale, t):\\n-    """Use heatmap to generate bounding boxes"""\\n-    stride=2\\n-    cellsize=12\\n-\\n-    imap = np.transpose(imap)\\n-    dx1 = np.transpose(reg[:,:,0])\\n-    dy1 = np.transpose(reg[:,:,1])\\n-    dx2 = np.transpose(reg[:,:,2])\\n-    dy2 = np.transpose(reg[:,:,3])\\n-    y, x = np.where(imap >= t)\\n-    if y.shape[0]==1:\\n-        dx1 = np.flipud(dx1)\\n-        dy1 = np.flipud(dy1)\\n-        dx2 = np.flipud(dx2)\\n-        dy2 = np.flipud(dy2)\\n-    score = imap[(y,x)]\\n-    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\\n-    if reg.size==0:\\n-        reg = np.empty((0,3))\\n-    bb = np.transpose(np.vstack([y,x]))\\n-    q1 = np.fix((stride*bb+1)/scale)\\n-    q2 = np.fix((stride*bb+cellsize-1+1)/scale)\\n-    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\\n-    return boundingbox, reg\\n- \\n-# function pick = nms(boxes,threshold,type)\\n-def nms(boxes, threshold, method):\\n-    if boxes.size==0:\\n-        return np.empty((0,3))\\n-    x1 = boxes[:,0]\\n-    y1 = boxes[:,1]\\n-    x2 = boxes[:,2]\\n-    y2 = boxes[:,3]\\n-    s = boxes[:,4]\\n-    area = (x2-x1+1) * (y2-y1+1)\\n-    I = np.argsort(s)\\n-    pick = np.zeros_like(s, dtype=np.int16)\\n-    counter = 0\\n-    while I.size>0:\\n-        i = I[-1]\\n-        pick[counter] = i\\n-        counter += 1\\n-        idx = I[0:-1]\\n-        xx1 = np.maximum(x1[i], x1[idx])\\n-        yy1 = np.maximum(y1[i], y1[idx])\\n-        xx2 = np.minimum(x2[i], x2[idx])\\n-        yy2 = np.minimum(y2[i], y2[idx])\\n-        w = np.maximum(0.0, xx2-xx1+1)\\n-        h = np.maximum(0.0, yy2-yy1+1)\\n-        inter = w * h\\n-        if method is \\\'Min\\\':\\n-            o = inter / np.minimum(area[i], area[idx])\\n-        else:\\n-            o = inter / (area[i] + area[idx] - inter)\\n-        I = I[np.where(o<=threshold)]\\n-    pick = pick[0:counter]\\n-    return pick\\n-\\n-# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\\n-def pad(total_boxes, w, h):\\n-    """Compute the padding coordinates (pad the bounding boxes to square)"""\\n-    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\\n-    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\\n-    numbox = total_boxes.shape[0]\\n-\\n-    dx = np.ones((numbox), dtype=np.int32)\\n-    dy = np.ones((numbox), dtype=np.int32)\\n-    edx = tmpw.copy().astype(np.int32)\\n-    edy = tmph.copy().astype(np.int32)\\n-\\n-    x = total_boxes[:,0].copy().astype(np.int32)\\n-    y = total_boxes[:,1].copy().astype(np.int32)\\n-    ex = total_boxes[:,2].copy().astype(np.int32)\\n-    ey = total_boxes[:,3].copy().astype(np.int32)\\n-\\n-    tmp = np.where(ex>w)\\n-    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\\n-    ex[tmp] = w\\n-    \\n-    tmp = np.where(ey>h)\\n-    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\\n-    ey[tmp] = h\\n-\\n-    tmp = np.where(x<1)\\n-    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\\n-    x[tmp] = 1\\n-\\n-    tmp = np.where(y<1)\\n-    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\\n-    y[tmp] = 1\\n-    \\n-    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\\n-\\n-# function [bboxA] = rerec(bboxA)\\n-def rerec(bboxA):\\n-    """Convert bboxA to square."""\\n-    h = bboxA[:,3]-bboxA[:,1]\\n-    w = bboxA[:,2]-bboxA[:,0]\\n-    l = np.maximum(w, h)\\n-    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\\n-    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\\n-    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\\n-    return bboxA\\n-\\n-def imresample(img, sz):\\n-    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\\n-    return im_data\\n-\\n-    # This method is kept for debugging purpose\\n-#     h=img.shape[0]\\n-#     w=img.shape[1]\\n-#     hs, ws = sz\\n-#     dx = float(w) / ws\\n-#     dy = float(h) / hs\\n-#     im_data = np.zeros((hs,ws,3))\\n-#     for a1 in range(0,hs):\\n-#         for a2 in range(0,ws):\\n-#             for a3 in range(0,3):\\n-#                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\n-#     return im_data\\n-\\ndiff --git a/create-dataset/align_dataset_mtcnn.py b/create-dataset/align_dataset_mtcnn.py\\ndeleted file mode 100644\\nindex 2d8110e..0000000\\n--- a/create-dataset/align_dataset_mtcnn.py\\n+++ /dev/null\\n@@ -1,160 +0,0 @@\\n-"""Performs face alignment and stores face thumbnails in the output directory."""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from scipy import misc\\n-import sys\\n-import os\\n-import argparse\\n-import tensorflow as tf\\n-import numpy as np\\n-import facenet\\n-import align.detect_face\\n-import random\\n-import imageio\\n-from time import sleep\\n-\\n-def main(args):\\n-    sleep(random.random())\\n-    output_dir = os.path.expanduser(args.output_dir)\\n-    if not os.path.exists(output_dir):\\n-        os.makedirs(output_dir)\\n-    # Store some git revision info in a text file in the log directory\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\n-    facenet.store_revision_info(src_path, output_dir, \\\' \\\'.join(sys.argv))\\n-    dataset = facenet.get_dataset(args.input_dir)\\n-    \\n-    print(\\\'Creating networks and loading parameters\\\')\\n-    \\n-    with tf.Graph().as_default():\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-        with sess.as_default():\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\\n-    \\n-    minsize = 20 # minimum size of face\\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\\\'s threshold\\n-    factor = 0.709 # scale factor\\n-\\n-    # Add a random key to the filename to allow alignment using multiple processes\\n-    random_key = np.random.randint(0, high=99999)\\n-    bounding_boxes_filename = os.path.join(output_dir, \\\'bounding_boxes_%05d.txt\\\' % random_key)\\n-    \\n-    with open(bounding_boxes_filename, "w") as text_file:\\n-        nrof_images_total = 0\\n-        nrof_successfully_aligned = 0\\n-        if args.random_order:\\n-            random.shuffle(dataset)\\n-        for cls in dataset:\\n-            output_class_dir = os.path.join(output_dir, cls.name)\\n-            if not os.path.exists(output_class_dir):\\n-                os.makedirs(output_class_dir)\\n-                if args.random_order:\\n-                    random.shuffle(cls.image_paths)\\n-            for image_path in cls.image_paths:\\n-                nrof_images_total += 1\\n-                filename = os.path.splitext(os.path.split(image_path)[1])[0]\\n-                output_filename = os.path.join(output_class_dir, filename+\\\'.png\\\')\\n-                print(image_path)\\n-                if not os.path.exists(output_filename):\\n-                    try:\\n-                        img = imageio.imread(image_path)\\n-                    except (IOError, ValueError, IndexError) as e:\\n-                        errorMessage = \\\'{}: {}\\\'.format(image_path, e)\\n-                        print(errorMessage)\\n-                    else:\\n-                        if img.ndim<2:\\n-                            print(\\\'Unable to align "%s"\\\' % image_path)\\n-                            text_file.write(\\\'%s\\\\n\\\' % (output_filename))\\n-                            continue\\n-                        if img.ndim == 2:\\n-                            img = facenet.to_rgb(img)\\n-                        img = img[:,:,0:3]\\n-    \\n-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\\n-                        nrof_faces = bounding_boxes.shape[0]\\n-                        if nrof_faces>0:\\n-                            det = bounding_boxes[:,0:4]\\n-                            det_arr = []\\n-                            img_size = np.asarray(img.shape)[0:2]\\n-                            if nrof_faces>1:\\n-                                if args.detect_multiple_faces:\\n-                                    for i in range(nrof_faces):\\n-                                        det_arr.append(np.squeeze(det[i]))\\n-                                else:\\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\\n-                                    img_center = img_size / 2\\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\\n-                                    det_arr.append(det[index,:])\\n-                            else:\\n-                                det_arr.append(np.squeeze(det))\\n-\\n-                            for i, det in enumerate(det_arr):\\n-                                det = np.squeeze(det)\\n-                                bb = np.zeros(4, dtype=np.int32)\\n-                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\\n-                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\\n-                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\\n-                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\\\'bilinear\\\')\\n-                                nrof_successfully_aligned += 1\\n-                                filename_base, file_extension = os.path.splitext(output_filename)\\n-                                if args.detect_multiple_faces:\\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\\n-                                else:\\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\\n-                                misc.imsave(output_filename_n, scaled)\\n-                                text_file.write(\\\'%s %d %d %d %d\\\\n\\\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\\n-                        else:\\n-                            print(\\\'Unable to align "%s"\\\' % image_path)\\n-                            text_file.write(\\\'%s\\\\n\\\' % (output_filename))\\n-                            \\n-    print(\\\'Total number of images: %d\\\' % nrof_images_total)\\n-    print(\\\'Number of successfully aligned images: %d\\\' % nrof_successfully_aligned)\\n-            \\n-\\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'input_dir\\\', type=str, help=\\\'Directory with unaligned images.\\\')\\n-    parser.add_argument(\\\'output_dir\\\', type=str, help=\\\'Directory with aligned face thumbnails.\\\')\\n-    parser.add_argument(\\\'--image_size\\\', type=int,\\n-        help=\\\'Image size (height, width) in pixels.\\\', default=182)\\n-    parser.add_argument(\\\'--margin\\\', type=int,\\n-        help=\\\'Margin for the crop around the bounding box (height, width) in pixels.\\\', default=44)\\n-    parser.add_argument(\\\'--random_order\\\', \\n-        help=\\\'Shuffles the order of images to enable alignment using multiple processes.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--gpu_memory_fraction\\\', type=float,\\n-        help=\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\', default=1.0)\\n-    parser.add_argument(\\\'--detect_multiple_faces\\\', type=bool,\\n-                        help=\\\'Detect and align multiple faces per image.\\\', default=False)\\n-    return parser.parse_args(argv)\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/create-dataset/facenet.py b/create-dataset/facenet.py\\ndeleted file mode 100644\\nindex 0e05676..0000000\\n--- a/create-dataset/facenet.py\\n+++ /dev/null\\n@@ -1,571 +0,0 @@\\n-"""Functions for building the face recognition network.\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-# pylint: disable=missing-docstring\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import os\\n-from subprocess import Popen, PIPE\\n-import tensorflow as tf\\n-import numpy as np\\n-from scipy import misc\\n-from sklearn.model_selection import KFold\\n-from scipy import interpolate\\n-from tensorflow.python.training import training\\n-import random\\n-import re\\n-from tensorflow.python.platform import gfile\\n-import math\\n-from six import iteritems\\n-\\n-def triplet_loss(anchor, positive, negative, alpha):\\n-    """Calculate the triplet loss according to the FaceNet paper\\n-    \\n-    Args:\\n-      anchor: the embeddings for the anchor images.\\n-      positive: the embeddings for the positive images.\\n-      negative: the embeddings for the negative images.\\n-  \\n-    Returns:\\n-      the triplet loss according to the FaceNet paper as a float tensor.\\n-    """\\n-    with tf.variable_scope(\\\'triplet_loss\\\'):\\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\\n-        \\n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\\n-      \\n-    return loss\\n-  \\n-def center_loss(features, label, alfa, nrof_classes):\\n-    """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\\n-       (http://ydwen.github.io/papers/WenECCV16.pdf)\\n-    """\\n-    nrof_features = features.get_shape()[1]\\n-    centers = tf.get_variable(\\\'centers\\\', [nrof_classes, nrof_features], dtype=tf.float32,\\n-        initializer=tf.constant_initializer(0), trainable=False)\\n-    label = tf.reshape(label, [-1])\\n-    centers_batch = tf.gather(centers, label)\\n-    diff = (1 - alfa) * (centers_batch - features)\\n-    centers = tf.scatter_sub(centers, label, diff)\\n-    with tf.control_dependencies([centers]):\\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\\n-    return loss, centers\\n-\\n-def get_image_paths_and_labels(dataset):\\n-    image_paths_flat = []\\n-    labels_flat = []\\n-    for i in range(len(dataset)):\\n-        image_paths_flat += dataset[i].image_paths\\n-        labels_flat += [i] * len(dataset[i].image_paths)\\n-    return image_paths_flat, labels_flat\\n-\\n-def shuffle_examples(image_paths, labels):\\n-    shuffle_list = list(zip(image_paths, labels))\\n-    random.shuffle(shuffle_list)\\n-    image_paths_shuff, labels_shuff = zip(*shuffle_list)\\n-    return image_paths_shuff, labels_shuff\\n-\\n-def random_rotate_image(image):\\n-    angle = np.random.uniform(low=-10.0, high=10.0)\\n-    return misc.imrotate(image, angle, \\\'bicubic\\\')\\n-  \\n-# 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\\n-RANDOM_ROTATE = 1\\n-RANDOM_CROP = 2\\n-RANDOM_FLIP = 4\\n-FIXED_STANDARDIZATION = 8\\n-FLIP = 16\\n-def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\\n-    images_and_labels_list = []\\n-    for _ in range(nrof_preprocess_threads):\\n-        filenames, label, control = input_queue.dequeue()\\n-        images = []\\n-        for filename in tf.unstack(filenames):\\n-            file_contents = tf.read_file(filename)\\n-            image = tf.image.decode_image(file_contents, 3)\\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \\n-                            lambda:tf.identity(image))\\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \\n-                            lambda:tf.random_crop(image, image_size + (3,)), \\n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\\n-                            lambda:tf.image.random_flip_left_right(image),\\n-                            lambda:tf.identity(image))\\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\\n-                            lambda:tf.image.per_image_standardization(image))\\n-            image = tf.cond(get_control_flag(control[0], FLIP),\\n-                            lambda:tf.image.flip_left_right(image),\\n-                            lambda:tf.identity(image))\\n-            #pylint: disable=no-member\\n-            image.set_shape(image_size + (3,))\\n-            images.append(image)\\n-        images_and_labels_list.append([images, label])\\n-\\n-    image_batch, label_batch = tf.train.batch_join(\\n-        images_and_labels_list, batch_size=batch_size_placeholder, \\n-        shapes=[image_size + (3,), ()], enqueue_many=True,\\n-        capacity=4 * nrof_preprocess_threads * 100,\\n-        allow_smaller_final_batch=True)\\n-    \\n-    return image_batch, label_batch\\n-\\n-def get_control_flag(control, field):\\n-    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\\n-  \\n-def _add_loss_summaries(total_loss):\\n-    """Add summaries for losses.\\n-  \\n-    Generates moving average for all losses and associated summaries for\\n-    visualizing the performance of the network.\\n-  \\n-    Args:\\n-      total_loss: Total loss from loss().\\n-    Returns:\\n-      loss_averages_op: op for generating moving averages of losses.\\n-    """\\n-    # Compute the moving average of all individual losses and the total loss.\\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\\\'avg\\\')\\n-    losses = tf.get_collection(\\\'losses\\\')\\n-    loss_averages_op = loss_averages.apply(losses + [total_loss])\\n-  \\n-    # Attach a scalar summmary to all individual losses and the total loss; do the\\n-    # same for the averaged version of the losses.\\n-    for l in losses + [total_loss]:\\n-        # Name each loss as \\\'(raw)\\\' and name the moving average version of the loss\\n-        # as the original loss name.\\n-        tf.summary.scalar(l.op.name +\\\' (raw)\\\', l)\\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\\n-  \\n-    return loss_averages_op\\n-\\n-def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\\n-    # Generate moving averages of all losses and associated summaries.\\n-    loss_averages_op = _add_loss_summaries(total_loss)\\n-\\n-    # Compute gradients.\\n-    with tf.control_dependencies([loss_averages_op]):\\n-        if optimizer==\\\'ADAGRAD\\\':\\n-            opt = tf.train.AdagradOptimizer(learning_rate)\\n-        elif optimizer==\\\'ADADELTA\\\':\\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\\n-        elif optimizer==\\\'ADAM\\\':\\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\\n-        elif optimizer==\\\'RMSPROP\\\':\\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\\n-        elif optimizer==\\\'MOM\\\':\\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\\n-        else:\\n-            raise ValueError(\\\'Invalid optimization algorithm\\\')\\n-    \\n-        grads = opt.compute_gradients(total_loss, update_gradient_vars)\\n-        \\n-    # Apply gradients.\\n-    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\\n-  \\n-    # Add histograms for trainable variables.\\n-    if log_histograms:\\n-        for var in tf.trainable_variables():\\n-            tf.summary.histogram(var.op.name, var)\\n-   \\n-    # Add histograms for gradients.\\n-    if log_histograms:\\n-        for grad, var in grads:\\n-            if grad is not None:\\n-                tf.summary.histogram(var.op.name + \\\'/gradients\\\', grad)\\n-  \\n-    # Track the moving averages of all trainable variables.\\n-    variable_averages = tf.train.ExponentialMovingAverage(\\n-        moving_average_decay, global_step)\\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\\n-  \\n-    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\\n-        train_op = tf.no_op(name=\\\'train\\\')\\n-  \\n-    return train_op\\n-\\n-def prewhiten(x):\\n-    mean = np.mean(x)\\n-    std = np.std(x)\\n-    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\\n-    y = np.multiply(np.subtract(x, mean), 1/std_adj)\\n-    return y  \\n-\\n-def crop(image, random_crop, image_size):\\n-    if image.shape[1]>image_size:\\n-        sz1 = int(image.shape[1]//2)\\n-        sz2 = int(image_size//2)\\n-        if random_crop:\\n-            diff = sz1-sz2\\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\\n-        else:\\n-            (h, v) = (0,0)\\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\\n-    return image\\n-  \\n-def flip(image, random_flip):\\n-    if random_flip and np.random.choice([True, False]):\\n-        image = np.fliplr(image)\\n-    return image\\n-\\n-def to_rgb(img):\\n-    w, h = img.shape\\n-    ret = np.empty((w, h, 3), dtype=np.uint8)\\n-    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\\n-    return ret\\n-  \\n-def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\\n-    nrof_samples = len(image_paths)\\n-    images = np.zeros((nrof_samples, image_size, image_size, 3))\\n-    for i in range(nrof_samples):\\n-        img = misc.imread(image_paths[i])\\n-        if img.ndim == 2:\\n-            img = to_rgb(img)\\n-        if do_prewhiten:\\n-            img = prewhiten(img)\\n-        img = crop(img, do_random_crop, image_size)\\n-        img = flip(img, do_random_flip)\\n-        images[i,:,:,:] = img\\n-    return images\\n-\\n-def get_label_batch(label_data, batch_size, batch_index):\\n-    nrof_examples = np.size(label_data, 0)\\n-    j = batch_index*batch_size % nrof_examples\\n-    if j+batch_size<=nrof_examples:\\n-        batch = label_data[j:j+batch_size]\\n-    else:\\n-        x1 = label_data[j:nrof_examples]\\n-        x2 = label_data[0:nrof_examples-j]\\n-        batch = np.vstack([x1,x2])\\n-    batch_int = batch.astype(np.int64)\\n-    return batch_int\\n-\\n-def get_batch(image_data, batch_size, batch_index):\\n-    nrof_examples = np.size(image_data, 0)\\n-    j = batch_index*batch_size % nrof_examples\\n-    if j+batch_size<=nrof_examples:\\n-        batch = image_data[j:j+batch_size,:,:,:]\\n-    else:\\n-        x1 = image_data[j:nrof_examples,:,:,:]\\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\\n-        batch = np.vstack([x1,x2])\\n-    batch_float = batch.astype(np.float32)\\n-    return batch_float\\n-\\n-def get_triplet_batch(triplets, batch_index, batch_size):\\n-    ax, px, nx = triplets\\n-    a = get_batch(ax, int(batch_size/3), batch_index)\\n-    p = get_batch(px, int(batch_size/3), batch_index)\\n-    n = get_batch(nx, int(batch_size/3), batch_index)\\n-    batch = np.vstack([a, p, n])\\n-    return batch\\n-\\n-def get_learning_rate_from_file(filename, epoch):\\n-    with open(filename, \\\'r\\\') as f:\\n-        for line in f.readlines():\\n-            line = line.split(\\\'#\\\', 1)[0]\\n-            if line:\\n-                par = line.strip().split(\\\':\\\')\\n-                e = int(par[0])\\n-                if par[1]==\\\'-\\\':\\n-                    lr = -1\\n-                else:\\n-                    lr = float(par[1])\\n-                if e <= epoch:\\n-                    learning_rate = lr\\n-                else:\\n-                    return learning_rate\\n-\\n-class ImageClass():\\n-    "Stores the paths to images for a given class"\\n-    def __init__(self, name, image_paths):\\n-        self.name = name\\n-        self.image_paths = image_paths\\n-  \\n-    def __str__(self):\\n-        return self.name + \\\', \\\' + str(len(self.image_paths)) + \\\' images\\\'\\n-  \\n-    def __len__(self):\\n-        return len(self.image_paths)\\n-  \\n-def get_dataset(path, has_class_directories=True):\\n-    dataset = []\\n-    path_exp = os.path.expanduser(path)\\n-    classes = [path for path in os.listdir(path_exp) \\\\\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\\n-    classes.sort()\\n-    nrof_classes = len(classes)\\n-    for i in range(nrof_classes):\\n-        class_name = classes[i]\\n-        facedir = os.path.join(path_exp, class_name)\\n-        image_paths = get_image_paths(facedir)\\n-        dataset.append(ImageClass(class_name, image_paths))\\n-  \\n-    return dataset\\n-\\n-def get_image_paths(facedir):\\n-    image_paths = []\\n-    if os.path.isdir(facedir):\\n-        images = os.listdir(facedir)\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\n-    return image_paths\\n-  \\n-def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\n-    if mode==\\\'SPLIT_CLASSES\\\':\\n-        nrof_classes = len(dataset)\\n-        class_indices = np.arange(nrof_classes)\\n-        np.random.shuffle(class_indices)\\n-        split = int(round(nrof_classes*(1-split_ratio)))\\n-        train_set = [dataset[i] for i in class_indices[0:split]]\\n-        test_set = [dataset[i] for i in class_indices[split:-1]]\\n-    elif mode==\\\'SPLIT_IMAGES\\\':\\n-        train_set = []\\n-        test_set = []\\n-        for cls in dataset:\\n-            paths = cls.image_paths\\n-            np.random.shuffle(paths)\\n-            nrof_images_in_class = len(paths)\\n-            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\\n-            if split==nrof_images_in_class:\\n-                split = nrof_images_in_class-1\\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\\n-                train_set.append(ImageClass(cls.name, paths[:split]))\\n-                test_set.append(ImageClass(cls.name, paths[split:]))\\n-    else:\\n-        raise ValueError(\\\'Invalid train/test split mode "%s"\\\' % mode)\\n-    return train_set, test_set\\n-\\n-def load_model(model, input_map=None):\\n-    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\\n-    #  or if it is a protobuf file with a frozen graph\\n-    model_exp = os.path.expanduser(model)\\n-    if (os.path.isfile(model_exp)):\\n-        print(\\\'Model filename: %s\\\' % model_exp)\\n-        with gfile.FastGFile(model_exp,\\\'rb\\\') as f:\\n-            graph_def = tf.GraphDef()\\n-            graph_def.ParseFromString(f.read())\\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\\\'\\\')\\n-    else:\\n-        print(\\\'Model directory: %s\\\' % model_exp)\\n-        meta_file, ckpt_file = get_model_filenames(model_exp)\\n-        \\n-        print(\\\'Metagraph file: %s\\\' % meta_file)\\n-        print(\\\'Checkpoint file: %s\\\' % ckpt_file)\\n-      \\n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\\n-    \\n-def get_model_filenames(model_dir):\\n-    files = os.listdir(model_dir)\\n-    meta_files = [s for s in files if s.endswith(\\\'.meta\\\')]\\n-    if len(meta_files)==0:\\n-        raise ValueError(\\\'No meta file found in the model directory (%s)\\\' % model_dir)\\n-    elif len(meta_files)>1:\\n-        raise ValueError(\\\'There should not be more than one meta file in the model directory (%s)\\\' % model_dir)\\n-    meta_file = meta_files[0]\\n-    ckpt = tf.train.get_checkpoint_state(model_dir)\\n-    if ckpt and ckpt.model_checkpoint_path:\\n-        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\\n-        return meta_file, ckpt_file\\n-\\n-    meta_files = [s for s in files if \\\'.ckpt\\\' in s]\\n-    max_step = -1\\n-    for f in files:\\n-        step_str = re.match(r\\\'(^model-[\\\\w\\\\- ]+.ckpt-(\\\\d+))\\\', f)\\n-        if step_str is not None and len(step_str.groups())>=2:\\n-            step = int(step_str.groups()[1])\\n-            if step > max_step:\\n-                max_step = step\\n-                ckpt_file = step_str.groups()[0]\\n-    return meta_file, ckpt_file\\n-  \\n-def distance(embeddings1, embeddings2, distance_metric=0):\\n-    if distance_metric==0:\\n-        # Euclidian distance\\n-        diff = np.subtract(embeddings1, embeddings2)\\n-        dist = np.sum(np.square(diff),1)\\n-    elif distance_metric==1:\\n-        # Distance based on cosine similarity\\n-        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\\n-        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\\n-        similarity = dot / norm\\n-        dist = np.arccos(similarity) / math.pi\\n-    else:\\n-        raise \\\'Undefined distance metric %d\\\' % distance_metric \\n-        \\n-    return dist\\n-\\n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\n-    nrof_thresholds = len(thresholds)\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\n-    \\n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\\n-    accuracy = np.zeros((nrof_folds))\\n-    \\n-    indices = np.arange(nrof_pairs)\\n-    \\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\n-        if subtract_mean:\\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\\n-        else:\\n-          mean = 0.0\\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\\n-        \\n-        # Find the best threshold for the fold\\n-        acc_train = np.zeros((nrof_thresholds))\\n-        for threshold_idx, threshold in enumerate(thresholds):\\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\\n-        best_threshold_index = np.argmax(acc_train)\\n-        for threshold_idx, threshold in enumerate(thresholds):\\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\\n-          \\n-        tpr = np.mean(tprs,0)\\n-        fpr = np.mean(fprs,0)\\n-    return tpr, fpr, accuracy\\n-\\n-def calculate_accuracy(threshold, dist, actual_issame):\\n-    predict_issame = np.less(dist, threshold)\\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\\n-  \\n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\\n-    acc = float(tp+tn)/dist.size\\n-    return tpr, fpr, acc\\n-\\n-\\n-  \\n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\n-    nrof_thresholds = len(thresholds)\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\n-    \\n-    val = np.zeros(nrof_folds)\\n-    far = np.zeros(nrof_folds)\\n-    \\n-    indices = np.arange(nrof_pairs)\\n-    \\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\n-        if subtract_mean:\\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\\n-        else:\\n-          mean = 0.0\\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\\n-      \\n-        # Find the threshold that gives FAR = far_target\\n-        far_train = np.zeros(nrof_thresholds)\\n-        for threshold_idx, threshold in enumerate(thresholds):\\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\\n-        if np.max(far_train)>=far_target:\\n-            f = interpolate.interp1d(far_train, thresholds, kind=\\\'slinear\\\')\\n-            threshold = f(far_target)\\n-        else:\\n-            threshold = 0.0\\n-    \\n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\\n-  \\n-    val_mean = np.mean(val)\\n-    far_mean = np.mean(far)\\n-    val_std = np.std(val)\\n-    return val_mean, val_std, far_mean\\n-\\n-\\n-def calculate_val_far(threshold, dist, actual_issame):\\n-    predict_issame = np.less(dist, threshold)\\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n-    n_same = np.sum(actual_issame)\\n-    n_diff = np.sum(np.logical_not(actual_issame))\\n-    val = float(true_accept) / float(n_same)\\n-    far = float(false_accept) / float(n_diff)\\n-    return val, far\\n-\\n-def store_revision_info(src_path, output_dir, arg_string):\\n-    try:\\n-        # Get git hash\\n-        cmd = [\\\'git\\\', \\\'rev-parse\\\', \\\'HEAD\\\']\\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\\n-        (stdout, _) = gitproc.communicate()\\n-        git_hash = stdout.strip()\\n-    except OSError as e:\\n-        git_hash = \\\' \\\'.join(cmd) + \\\': \\\' +  e.strerror\\n-  \\n-    try:\\n-        # Get local changes\\n-        cmd = [\\\'git\\\', \\\'diff\\\', \\\'HEAD\\\']\\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\\n-        (stdout, _) = gitproc.communicate()\\n-        git_diff = stdout.strip()\\n-    except OSError as e:\\n-        git_diff = \\\' \\\'.join(cmd) + \\\': \\\' +  e.strerror\\n-    \\n-    # Store a text file in the log directory\\n-    rev_info_filename = os.path.join(output_dir, \\\'revision_info.txt\\\')\\n-    with open(rev_info_filename, "w") as text_file:\\n-        text_file.write(\\\'arguments: %s\\\\n--------------------\\\\n\\\' % arg_string)\\n-        text_file.write(\\\'tensorflow version: %s\\\\n--------------------\\\\n\\\' % tf.__version__)  # @UndefinedVariable\\n-        text_file.write(\\\'git hash: %s\\\\n--------------------\\\\n\\\' % git_hash)\\n-        text_file.write(\\\'%s\\\' % git_diff)\\n-\\n-def list_variables(filename):\\n-    reader = training.NewCheckpointReader(filename)\\n-    variable_map = reader.get_variable_to_shape_map()\\n-    names = sorted(variable_map.keys())\\n-    return names\\n-\\n-def put_images_on_grid(images, shape=(16,8)):\\n-    nrof_images = images.shape[0]\\n-    img_size = images.shape[1]\\n-    bw = 3\\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\\n-    for i in range(shape[1]):\\n-        x_start = i*(img_size+bw)+bw\\n-        for j in range(shape[0]):\\n-            img_index = i*shape[0]+j\\n-            if img_index>=nrof_images:\\n-                break\\n-            y_start = j*(img_size+bw)+bw\\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\\n-        if img_index>=nrof_images:\\n-            break\\n-    return img\\n-\\n-def write_arguments_to_file(args, filename):\\n-    with open(filename, \\\'w\\\') as f:\\n-        for key, value in iteritems(vars(args)):\\n-            f.write(\\\'%s: %s\\\\n\\\' % (key, str(value)))\\ndiff --git a/data/__init__.py b/data/__init__.py\\ndeleted file mode 100644\\nindex e69de29..0000000\\ndiff --git a/data/data_pipe.py b/data/data_pipe.py\\ndeleted file mode 100644\\nindex bd67f02..0000000\\n--- a/data/data_pipe.py\\n+++ /dev/null\\n@@ -1,122 +0,0 @@\\n-from pathlib import Path\\n-from torch.utils.data import Dataset, ConcatDataset, DataLoader\\n-from torchvision import transforms as trans\\n-from torchvision.datasets import ImageFolder\\n-from PIL import Image, ImageFile\\n-ImageFile.LOAD_TRUNCATED_IMAGES = True\\n-import numpy as np\\n-import cv2\\n-import bcolz\\n-import pickle\\n-import torch\\n-import mxnet as mx\\n-from tqdm import tqdm\\n-\\n-def de_preprocess(tensor):\\n-    return tensor*0.5 + 0.5\\n-    \\n-def get_train_dataset(imgs_folder):\\n-    train_transform = trans.Compose([\\n-        trans.RandomHorizontalFlip(),\\n-        trans.ToTensor(),\\n-        trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\n-    ])\\n-    ds = ImageFolder(imgs_folder, train_transform)\\n-    class_num = ds[-1][1] + 1\\n-    return ds, class_num\\n-\\n-def get_train_loader(conf):\\n-    if conf.data_mode in [\\\'ms1m\\\', \\\'concat\\\']:\\n-        ms1m_ds, ms1m_class_num = get_train_dataset(conf.ms1m_folder/\\\'imgs\\\')\\n-        print(\\\'ms1m loader generated\\\')\\n-    if conf.data_mode in [\\\'vgg\\\', \\\'concat\\\']:\\n-        vgg_ds, vgg_class_num = get_train_dataset(conf.vgg_folder/\\\'imgs\\\')\\n-        print(\\\'vgg loader generated\\\')        \\n-    if conf.data_mode == \\\'vgg\\\':\\n-        ds = vgg_ds\\n-        class_num = vgg_class_num\\n-    elif conf.data_mode == \\\'ms1m\\\':\\n-        ds = ms1m_ds\\n-        class_num = ms1m_class_num\\n-    elif conf.data_mode == \\\'concat\\\':\\n-        for i,(url,label) in enumerate(vgg_ds.imgs):\\n-            vgg_ds.imgs[i] = (url, label + ms1m_class_num)\\n-        ds = ConcatDataset([ms1m_ds,vgg_ds])\\n-        class_num = vgg_class_num + ms1m_class_num\\n-    elif conf.data_mode == \\\'emore\\\':\\n-        ds, class_num = get_train_dataset(conf.emore_folder/\\\'imgs\\\')\\n-    loader = DataLoader(ds, batch_size=conf.batch_size, shuffle=True, pin_memory=conf.pin_memory, num_workers=conf.num_workers)\\n-    return loader, class_num \\n-    \\n-def load_bin(path, rootdir, transform, image_size=[112,112]):\\n-    if not rootdir.exists():\\n-        rootdir.mkdir()\\n-    bins, issame_list = pickle.load(open(path, \\\'rb\\\'), encoding=\\\'bytes\\\')\\n-    data = bcolz.fill([len(bins), 3, image_size[0], image_size[1]], dtype=np.float32, rootdir=rootdir, mode=\\\'w\\\')\\n-    for i in range(len(bins)):\\n-        _bin = bins[i]\\n-        img = mx.image.imdecode(_bin).asnumpy()\\n-        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\n-        img = Image.fromarray(img.astype(np.uint8))\\n-        data[i, ...] = transform(img)\\n-        i += 1\\n-        if i % 1000 == 0:\\n-            print(\\\'loading bin\\\', i)\\n-    print(data.shape)\\n-    np.save(str(rootdir)+\\\'_list\\\', np.array(issame_list))\\n-    return data, issame_list\\n-\\n-def get_val_pair(path, name):\\n-    carray = bcolz.carray(rootdir = path/name, mode=\\\'r\\\')\\n-    issame = np.load(path/\\\'{}_list.npy\\\'.format(name))\\n-    return carray, issame\\n-\\n-def get_val_data(data_path):\\n-    agedb_30, agedb_30_issame = get_val_pair(data_path, \\\'agedb_30\\\')\\n-    cfp_fp, cfp_fp_issame = get_val_pair(data_path, \\\'cfp_fp\\\')\\n-    lfw, lfw_issame = get_val_pair(data_path, \\\'lfw\\\')\\n-    return agedb_30, cfp_fp, lfw, agedb_30_issame, cfp_fp_issame, lfw_issame\\n-\\n-def load_mx_rec(rec_path):\\n-    save_path = rec_path/\\\'imgs\\\'\\n-    if not save_path.exists():\\n-        save_path.mkdir()\\n-    imgrec = mx.recordio.MXIndexedRecordIO(str(rec_path/\\\'train.idx\\\'), str(rec_path/\\\'train.rec\\\'), \\\'r\\\')\\n-    img_info = imgrec.read_idx(0)\\n-    header,_ = mx.recordio.unpack(img_info)\\n-    max_idx = int(header.label[0])\\n-    for idx in tqdm(range(1,max_idx)):\\n-        img_info = imgrec.read_idx(idx)\\n-        header, img = mx.recordio.unpack_img(img_info)\\n-        label = int(header.label)\\n-        img = Image.fromarray(img)\\n-        label_path = save_path/str(label)\\n-        if not label_path.exists():\\n-            label_path.mkdir()\\n-        img.save(label_path/\\\'{}.jpg\\\'.format(idx), quality=95)\\n-\\n-# class train_dataset(Dataset):\\n-#     def __init__(self, imgs_bcolz, label_bcolz, h_flip=True):\\n-#         self.imgs = bcolz.carray(rootdir = imgs_bcolz)\\n-#         self.labels = bcolz.carray(rootdir = label_bcolz)\\n-#         self.h_flip = h_flip\\n-#         self.length = len(self.imgs) - 1\\n-#         if h_flip:\\n-#             self.transform = trans.Compose([\\n-#                 trans.ToPILImage(),\\n-#                 trans.RandomHorizontalFlip(),\\n-#                 trans.ToTensor(),\\n-#                 trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\n-#             ])\\n-#         self.class_num = self.labels[-1] + 1\\n-        \\n-#     def __len__(self):\\n-#         return self.length\\n-    \\n-#     def __getitem__(self, index):\\n-#         img = torch.tensor(self.imgs[index+1], dtype=torch.float)\\n-#         label = torch.tensor(self.labels[index+1], dtype=torch.long)\\n-#         if self.h_flip:\\n-#             img = de_preprocess(img)\\n-#             img = self.transform(img)\\n-#         return img, label\\n\\\\ No newline at end of file\\ndiff --git a/data/facebank/Chandler/Chandler.02.png b/data/facebank/Chandler/Chandler.02.png\\ndeleted file mode 100644\\nindex e824825..0000000\\nBinary files a/data/facebank/Chandler/Chandler.02.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.03.png b/data/facebank/Chandler/Chandler.03.png\\ndeleted file mode 100644\\nindex c8ad160..0000000\\nBinary files a/data/facebank/Chandler/Chandler.03.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.04.png b/data/facebank/Chandler/Chandler.04.png\\ndeleted file mode 100644\\nindex cd13e74..0000000\\nBinary files a/data/facebank/Chandler/Chandler.04.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.05.png b/data/facebank/Chandler/Chandler.05.png\\ndeleted file mode 100644\\nindex 6ac06fb..0000000\\nBinary files a/data/facebank/Chandler/Chandler.05.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.06.png b/data/facebank/Chandler/Chandler.06.png\\ndeleted file mode 100644\\nindex e598bd4..0000000\\nBinary files a/data/facebank/Chandler/Chandler.06.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.07.png b/data/facebank/Chandler/Chandler.07.png\\ndeleted file mode 100644\\nindex 6a3d942..0000000\\nBinary files a/data/facebank/Chandler/Chandler.07.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.08.png b/data/facebank/Chandler/Chandler.08.png\\ndeleted file mode 100644\\nindex 15fdae4..0000000\\nBinary files a/data/facebank/Chandler/Chandler.08.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.09.png b/data/facebank/Chandler/Chandler.09.png\\ndeleted file mode 100644\\nindex 962a80d..0000000\\nBinary files a/data/facebank/Chandler/Chandler.09.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.10.png b/data/facebank/Chandler/Chandler.10.png\\ndeleted file mode 100644\\nindex bc7707c..0000000\\nBinary files a/data/facebank/Chandler/Chandler.10.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.11.png b/data/facebank/Chandler/Chandler.11.png\\ndeleted file mode 100644\\nindex 08f3f50..0000000\\nBinary files a/data/facebank/Chandler/Chandler.11.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.17.png b/data/facebank/Chandler/Chandler.17.png\\ndeleted file mode 100644\\nindex 0db2c6c..0000000\\nBinary files a/data/facebank/Chandler/Chandler.17.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler.png b/data/facebank/Chandler/Chandler.png\\ndeleted file mode 100644\\nindex f998d6f..0000000\\nBinary files a/data/facebank/Chandler/Chandler.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler12.png b/data/facebank/Chandler/Chandler12.png\\ndeleted file mode 100644\\nindex 4b5720a..0000000\\nBinary files a/data/facebank/Chandler/Chandler12.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler13.png b/data/facebank/Chandler/Chandler13.png\\ndeleted file mode 100644\\nindex aea4252..0000000\\nBinary files a/data/facebank/Chandler/Chandler13.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler14.png b/data/facebank/Chandler/Chandler14.png\\ndeleted file mode 100644\\nindex 5dfdbf3..0000000\\nBinary files a/data/facebank/Chandler/Chandler14.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler15.png b/data/facebank/Chandler/Chandler15.png\\ndeleted file mode 100644\\nindex 4f1efef..0000000\\nBinary files a/data/facebank/Chandler/Chandler15.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler16.png b/data/facebank/Chandler/Chandler16.png\\ndeleted file mode 100644\\nindex 987c894..0000000\\nBinary files a/data/facebank/Chandler/Chandler16.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler18.png b/data/facebank/Chandler/Chandler18.png\\ndeleted file mode 100644\\nindex dc5576f..0000000\\nBinary files a/data/facebank/Chandler/Chandler18.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler19.png b/data/facebank/Chandler/Chandler19.png\\ndeleted file mode 100644\\nindex e407adc..0000000\\nBinary files a/data/facebank/Chandler/Chandler19.png and /dev/null differ\\ndiff --git a/data/facebank/Chandler/Chandler20.png b/data/facebank/Chandler/Chandler20.png\\ndeleted file mode 100644\\nindex 6fc1ab3..0000000\\nBinary files a/data/facebank/Chandler/Chandler20.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.01.png b/data/facebank/Joey/Joey.01.png\\ndeleted file mode 100644\\nindex 3c685d0..0000000\\nBinary files a/data/facebank/Joey/Joey.01.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.02.png b/data/facebank/Joey/Joey.02.png\\ndeleted file mode 100644\\nindex 9ca5128..0000000\\nBinary files a/data/facebank/Joey/Joey.02.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.03.png b/data/facebank/Joey/Joey.03.png\\ndeleted file mode 100644\\nindex 6d2caca..0000000\\nBinary files a/data/facebank/Joey/Joey.03.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.04.png b/data/facebank/Joey/Joey.04.png\\ndeleted file mode 100644\\nindex f698017..0000000\\nBinary files a/data/facebank/Joey/Joey.04.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.05.png b/data/facebank/Joey/Joey.05.png\\ndeleted file mode 100644\\nindex 11c4831..0000000\\nBinary files a/data/facebank/Joey/Joey.05.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.06.png b/data/facebank/Joey/Joey.06.png\\ndeleted file mode 100644\\nindex 96f7ed3..0000000\\nBinary files a/data/facebank/Joey/Joey.06.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.07.png b/data/facebank/Joey/Joey.07.png\\ndeleted file mode 100644\\nindex 1e165fa..0000000\\nBinary files a/data/facebank/Joey/Joey.07.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.08.png b/data/facebank/Joey/Joey.08.png\\ndeleted file mode 100644\\nindex 09144d8..0000000\\nBinary files a/data/facebank/Joey/Joey.08.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.09.png b/data/facebank/Joey/Joey.09.png\\ndeleted file mode 100644\\nindex 7ed417e..0000000\\nBinary files a/data/facebank/Joey/Joey.09.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.11.png b/data/facebank/Joey/Joey.11.png\\ndeleted file mode 100644\\nindex 580bc53..0000000\\nBinary files a/data/facebank/Joey/Joey.11.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.12.png b/data/facebank/Joey/Joey.12.png\\ndeleted file mode 100644\\nindex f298964..0000000\\nBinary files a/data/facebank/Joey/Joey.12.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.13.png b/data/facebank/Joey/Joey.13.png\\ndeleted file mode 100644\\nindex 0498bc2..0000000\\nBinary files a/data/facebank/Joey/Joey.13.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.14.png b/data/facebank/Joey/Joey.14.png\\ndeleted file mode 100644\\nindex 54b8ddc..0000000\\nBinary files a/data/facebank/Joey/Joey.14.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.15.png b/data/facebank/Joey/Joey.15.png\\ndeleted file mode 100644\\nindex c2a4757..0000000\\nBinary files a/data/facebank/Joey/Joey.15.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.16.png b/data/facebank/Joey/Joey.16.png\\ndeleted file mode 100644\\nindex e3bc9b8..0000000\\nBinary files a/data/facebank/Joey/Joey.16.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.17.png b/data/facebank/Joey/Joey.17.png\\ndeleted file mode 100644\\nindex 4e9312e..0000000\\nBinary files a/data/facebank/Joey/Joey.17.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.18.png b/data/facebank/Joey/Joey.18.png\\ndeleted file mode 100644\\nindex 936d8a5..0000000\\nBinary files a/data/facebank/Joey/Joey.18.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.19.png b/data/facebank/Joey/Joey.19.png\\ndeleted file mode 100644\\nindex bdfb07d..0000000\\nBinary files a/data/facebank/Joey/Joey.19.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.20.png b/data/facebank/Joey/Joey.20.png\\ndeleted file mode 100644\\nindex b78b6bf..0000000\\nBinary files a/data/facebank/Joey/Joey.20.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.02.png b/data/facebank/Monica/Monica.02.png\\ndeleted file mode 100644\\nindex 1a05511..0000000\\nBinary files a/data/facebank/Monica/Monica.02.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.03.png b/data/facebank/Monica/Monica.03.png\\ndeleted file mode 100644\\nindex 1f58157..0000000\\nBinary files a/data/facebank/Monica/Monica.03.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.04.png b/data/facebank/Monica/Monica.04.png\\ndeleted file mode 100644\\nindex 05a254a..0000000\\nBinary files a/data/facebank/Monica/Monica.04.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.05.png b/data/facebank/Monica/Monica.05.png\\ndeleted file mode 100644\\nindex f97be0d..0000000\\nBinary files a/data/facebank/Monica/Monica.05.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.08.png b/data/facebank/Monica/Monica.08.png\\ndeleted file mode 100644\\nindex 4ccbc47..0000000\\nBinary files a/data/facebank/Monica/Monica.08.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.png b/data/facebank/Monica/Monica.png\\ndeleted file mode 100644\\nindex 758c3c6..0000000\\nBinary files a/data/facebank/Monica/Monica.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica10.png b/data/facebank/Monica/Monica10.png\\ndeleted file mode 100644\\nindex 8994753..0000000\\nBinary files a/data/facebank/Monica/Monica10.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica11.png b/data/facebank/Monica/Monica11.png\\ndeleted file mode 100644\\nindex 357b8bc..0000000\\nBinary files a/data/facebank/Monica/Monica11.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica12.png b/data/facebank/Monica/Monica12.png\\ndeleted file mode 100644\\nindex dc9ac29..0000000\\nBinary files a/data/facebank/Monica/Monica12.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica13.png b/data/facebank/Monica/Monica13.png\\ndeleted file mode 100644\\nindex d7c2cf6..0000000\\nBinary files a/data/facebank/Monica/Monica13.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica14.png b/data/facebank/Monica/Monica14.png\\ndeleted file mode 100644\\nindex a528414..0000000\\nBinary files a/data/facebank/Monica/Monica14.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica15.png b/data/facebank/Monica/Monica15.png\\ndeleted file mode 100644\\nindex 5bfe1a9..0000000\\nBinary files a/data/facebank/Monica/Monica15.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica16.png b/data/facebank/Monica/Monica16.png\\ndeleted file mode 100644\\nindex f94e645..0000000\\nBinary files a/data/facebank/Monica/Monica16.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica17.png b/data/facebank/Monica/Monica17.png\\ndeleted file mode 100644\\nindex bf24311..0000000\\nBinary files a/data/facebank/Monica/Monica17.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica18.png b/data/facebank/Monica/Monica18.png\\ndeleted file mode 100644\\nindex b04f261..0000000\\nBinary files a/data/facebank/Monica/Monica18.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica19.png b/data/facebank/Monica/Monica19.png\\ndeleted file mode 100644\\nindex 0eeb0d6..0000000\\nBinary files a/data/facebank/Monica/Monica19.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica20.png b/data/facebank/Monica/Monica20.png\\ndeleted file mode 100644\\nindex e5d4e24..0000000\\nBinary files a/data/facebank/Monica/Monica20.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica6.png b/data/facebank/Monica/Monica6.png\\ndeleted file mode 100644\\nindex 7112741..0000000\\nBinary files a/data/facebank/Monica/Monica6.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica7.png b/data/facebank/Monica/Monica7.png\\ndeleted file mode 100644\\nindex 3567770..0000000\\nBinary files a/data/facebank/Monica/Monica7.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica8.png b/data/facebank/Monica/Monica8.png\\ndeleted file mode 100644\\nindex de723c2..0000000\\nBinary files a/data/facebank/Monica/Monica8.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica9.png b/data/facebank/Monica/Monica9.png\\ndeleted file mode 100644\\nindex c7ae768..0000000\\nBinary files a/data/facebank/Monica/Monica9.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe06.png b/data/facebank/Phoebe/Pheobe06.png\\ndeleted file mode 100644\\nindex 3270a78..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe06.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe07.png b/data/facebank/Phoebe/Pheobe07.png\\ndeleted file mode 100644\\nindex 25cb4fd..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe07.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe08.png b/data/facebank/Phoebe/Pheobe08.png\\ndeleted file mode 100644\\nindex cd5d7e0..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe08.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe09.png b/data/facebank/Phoebe/Pheobe09.png\\ndeleted file mode 100644\\nindex 4df4c47..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe09.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe10.png b/data/facebank/Phoebe/Pheobe10.png\\ndeleted file mode 100644\\nindex 8637633..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe10.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe11.png b/data/facebank/Phoebe/Pheobe11.png\\ndeleted file mode 100644\\nindex 7ad6ed9..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe11.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe12.png b/data/facebank/Phoebe/Pheobe12.png\\ndeleted file mode 100644\\nindex 7ce09dc..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe12.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe13.png b/data/facebank/Phoebe/Pheobe13.png\\ndeleted file mode 100644\\nindex 17b6c11..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe13.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe15.png b/data/facebank/Phoebe/Pheobe15.png\\ndeleted file mode 100644\\nindex 6c47f10..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe15.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe16.png b/data/facebank/Phoebe/Pheobe16.png\\ndeleted file mode 100644\\nindex 56775ea..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe16.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe17.png b/data/facebank/Phoebe/Pheobe17.png\\ndeleted file mode 100644\\nindex 0897eb9..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe17.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe18.png b/data/facebank/Phoebe/Pheobe18.png\\ndeleted file mode 100644\\nindex 2579394..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe18.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe19.png b/data/facebank/Phoebe/Pheobe19.png\\ndeleted file mode 100644\\nindex 111693b..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe19.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe20.png b/data/facebank/Phoebe/Pheobe20.png\\ndeleted file mode 100644\\nindex 82ecd08..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe20.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.02.png b/data/facebank/Phoebe/Phoebe.02.png\\ndeleted file mode 100644\\nindex 5a67bbc..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.02.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.03.png b/data/facebank/Phoebe/Phoebe.03.png\\ndeleted file mode 100644\\nindex 43a1ae2..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.03.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.04.png b/data/facebank/Phoebe/Phoebe.04.png\\ndeleted file mode 100644\\nindex e221b0d..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.04.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.05.png b/data/facebank/Phoebe/Phoebe.05.png\\ndeleted file mode 100644\\nindex 96a6629..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.05.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.png b/data/facebank/Phoebe/Phoebe.png\\ndeleted file mode 100644\\nindex befbda6..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.02.png b/data/facebank/Rachel/Rachel.02.png\\ndeleted file mode 100644\\nindex e62a99a..0000000\\nBinary files a/data/facebank/Rachel/Rachel.02.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.03.png b/data/facebank/Rachel/Rachel.03.png\\ndeleted file mode 100644\\nindex f4ac8fa..0000000\\nBinary files a/data/facebank/Rachel/Rachel.03.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.04.png b/data/facebank/Rachel/Rachel.04.png\\ndeleted file mode 100644\\nindex 1ff0d0c..0000000\\nBinary files a/data/facebank/Rachel/Rachel.04.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.05.png b/data/facebank/Rachel/Rachel.05.png\\ndeleted file mode 100644\\nindex 2d69283..0000000\\nBinary files a/data/facebank/Rachel/Rachel.05.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel01.png b/data/facebank/Rachel/Rachel01.png\\ndeleted file mode 100644\\nindex 91e0fd9..0000000\\nBinary files a/data/facebank/Rachel/Rachel01.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel06.png b/data/facebank/Rachel/Rachel06.png\\ndeleted file mode 100644\\nindex be80ce0..0000000\\nBinary files a/data/facebank/Rachel/Rachel06.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel07.png b/data/facebank/Rachel/Rachel07.png\\ndeleted file mode 100644\\nindex 7f1c0d8..0000000\\nBinary files a/data/facebank/Rachel/Rachel07.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel08.png b/data/facebank/Rachel/Rachel08.png\\ndeleted file mode 100644\\nindex 2295162..0000000\\nBinary files a/data/facebank/Rachel/Rachel08.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel09.png b/data/facebank/Rachel/Rachel09.png\\ndeleted file mode 100644\\nindex 49dfe08..0000000\\nBinary files a/data/facebank/Rachel/Rachel09.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel10.png b/data/facebank/Rachel/Rachel10.png\\ndeleted file mode 100644\\nindex bae25a1..0000000\\nBinary files a/data/facebank/Rachel/Rachel10.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel11.png b/data/facebank/Rachel/Rachel11.png\\ndeleted file mode 100644\\nindex 0e39cef..0000000\\nBinary files a/data/facebank/Rachel/Rachel11.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel12.png b/data/facebank/Rachel/Rachel12.png\\ndeleted file mode 100644\\nindex d0945d5..0000000\\nBinary files a/data/facebank/Rachel/Rachel12.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel13.png b/data/facebank/Rachel/Rachel13.png\\ndeleted file mode 100644\\nindex 9a942f2..0000000\\nBinary files a/data/facebank/Rachel/Rachel13.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel14.png b/data/facebank/Rachel/Rachel14.png\\ndeleted file mode 100644\\nindex 3786268..0000000\\nBinary files a/data/facebank/Rachel/Rachel14.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel15.png b/data/facebank/Rachel/Rachel15.png\\ndeleted file mode 100644\\nindex c8b5b67..0000000\\nBinary files a/data/facebank/Rachel/Rachel15.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel16.png b/data/facebank/Rachel/Rachel16.png\\ndeleted file mode 100644\\nindex 8fa13b5..0000000\\nBinary files a/data/facebank/Rachel/Rachel16.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel17.png b/data/facebank/Rachel/Rachel17.png\\ndeleted file mode 100644\\nindex dcd563d..0000000\\nBinary files a/data/facebank/Rachel/Rachel17.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel18.png b/data/facebank/Rachel/Rachel18.png\\ndeleted file mode 100644\\nindex 5d58bc1..0000000\\nBinary files a/data/facebank/Rachel/Rachel18.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel19.png b/data/facebank/Rachel/Rachel19.png\\ndeleted file mode 100644\\nindex 5f9778a..0000000\\nBinary files a/data/facebank/Rachel/Rachel19.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel20.png b/data/facebank/Rachel/Rachel20.png\\ndeleted file mode 100644\\nindex 7d62102..0000000\\nBinary files a/data/facebank/Rachel/Rachel20.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/Ross03.png b/data/facebank/Ross/Ross03.png\\ndeleted file mode 100644\\nindex d8c2c7d..0000000\\nBinary files a/data/facebank/Ross/Ross03.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/Ross04.png b/data/facebank/Ross/Ross04.png\\ndeleted file mode 100644\\nindex b3ccc1b..0000000\\nBinary files a/data/facebank/Ross/Ross04.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/Ross05.png b/data/facebank/Ross/Ross05.png\\ndeleted file mode 100644\\nindex 95fda3f..0000000\\nBinary files a/data/facebank/Ross/Ross05.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross01.png b/data/facebank/Ross/ross01.png\\ndeleted file mode 100644\\nindex 3459b69..0000000\\nBinary files a/data/facebank/Ross/ross01.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross02.png b/data/facebank/Ross/ross02.png\\ndeleted file mode 100644\\nindex 332c896..0000000\\nBinary files a/data/facebank/Ross/ross02.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross06.png b/data/facebank/Ross/ross06.png\\ndeleted file mode 100644\\nindex 8cc4528..0000000\\nBinary files a/data/facebank/Ross/ross06.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross07.png b/data/facebank/Ross/ross07.png\\ndeleted file mode 100644\\nindex f26cf1d..0000000\\nBinary files a/data/facebank/Ross/ross07.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross08.png b/data/facebank/Ross/ross08.png\\ndeleted file mode 100644\\nindex 76b0a88..0000000\\nBinary files a/data/facebank/Ross/ross08.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross09.png b/data/facebank/Ross/ross09.png\\ndeleted file mode 100644\\nindex 50d07c8..0000000\\nBinary files a/data/facebank/Ross/ross09.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross10.png b/data/facebank/Ross/ross10.png\\ndeleted file mode 100644\\nindex 64651c2..0000000\\nBinary files a/data/facebank/Ross/ross10.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross11.png b/data/facebank/Ross/ross11.png\\ndeleted file mode 100644\\nindex bfc016b..0000000\\nBinary files a/data/facebank/Ross/ross11.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross12.png b/data/facebank/Ross/ross12.png\\ndeleted file mode 100644\\nindex d65cc34..0000000\\nBinary files a/data/facebank/Ross/ross12.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross13.png b/data/facebank/Ross/ross13.png\\ndeleted file mode 100644\\nindex 2671298..0000000\\nBinary files a/data/facebank/Ross/ross13.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross14.png b/data/facebank/Ross/ross14.png\\ndeleted file mode 100644\\nindex c239eea..0000000\\nBinary files a/data/facebank/Ross/ross14.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross15.png b/data/facebank/Ross/ross15.png\\ndeleted file mode 100644\\nindex 7326319..0000000\\nBinary files a/data/facebank/Ross/ross15.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross16.png b/data/facebank/Ross/ross16.png\\ndeleted file mode 100644\\nindex 7786cdd..0000000\\nBinary files a/data/facebank/Ross/ross16.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross17.png b/data/facebank/Ross/ross17.png\\ndeleted file mode 100644\\nindex 0c70f90..0000000\\nBinary files a/data/facebank/Ross/ross17.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross18.png b/data/facebank/Ross/ross18.png\\ndeleted file mode 100644\\nindex 06d916d..0000000\\nBinary files a/data/facebank/Ross/ross18.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross19.png b/data/facebank/Ross/ross19.png\\ndeleted file mode 100644\\nindex 8f79393..0000000\\nBinary files a/data/facebank/Ross/ross19.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross20.png b/data/facebank/Ross/ross20.png\\ndeleted file mode 100644\\nindex 6621203..0000000\\nBinary files a/data/facebank/Ross/ross20.png and /dev/null differ\\ndiff --git a/data/facebank/Suyash/image1.png b/data/facebank/Suyash/image1.png\\ndeleted file mode 100644\\nindex 0d8a120..0000000\\nBinary files a/data/facebank/Suyash/image1.png and /dev/null differ\\ndiff --git a/data/facebank/Suyash/image2.png b/data/facebank/Suyash/image2.png\\ndeleted file mode 100644\\nindex 6d14e96..0000000\\nBinary files a/data/facebank/Suyash/image2.png and /dev/null differ\\ndiff --git a/data/facebank/Suyash/image3.png b/data/facebank/Suyash/image3.png\\ndeleted file mode 100644\\nindex 7266fe4..0000000\\nBinary files a/data/facebank/Suyash/image3.png and /dev/null differ\\ndiff --git a/data/facebank/Suyash/image4.png b/data/facebank/Suyash/image4.png\\ndeleted file mode 100644\\nindex ecfef1a..0000000\\nBinary files a/data/facebank/Suyash/image4.png and /dev/null differ\\ndiff --git a/data/facebank/Suyash/image5.png b/data/facebank/Suyash/image5.png\\ndeleted file mode 100644\\nindex 23495e2..0000000\\nBinary files a/data/facebank/Suyash/image5.png and /dev/null differ\\ndiff --git a/data/facebank/facebank.pth b/data/facebank/facebank.pth\\ndeleted file mode 100644\\nindex 5fdbbbc..0000000\\n--- a/data/facebank/facebank.pth\\n+++ /dev/null\\n@@ -1,3 +0,0 @@\\n-version https://git-lfs.github.com/spec/v1\\n-oid sha256:01da0a10ca9fae085c25f535aed1241340a3bad79a49c1f2ec28b07aaa3307d0\\n-size 15083\\ndiff --git a/data/facebank/names.npy b/data/facebank/names.npy\\ndeleted file mode 100644\\nindex 108dbb2..0000000\\nBinary files a/data/facebank/names.npy and /dev/null differ\\ndiff --git a/face_verify.py b/face_verify.py\\ndeleted file mode 100644\\nindex 7f7d96b..0000000\\n--- a/face_verify.py\\n+++ /dev/null\\n@@ -1,84 +0,0 @@\\n-import cv2\\n-from PIL import Image\\n-import argparse\\n-from pathlib import Path\\n-from multiprocessing import Process, Pipe,Value,Array\\n-import torch\\n-from config import get_config\\n-from mtcnn import MTCNN\\n-from Learner import face_learner\\n-from utils import load_facebank, draw_box_name, prepare_facebank\\n-\\n-\\n-parser = argparse.ArgumentParser(description=\\\'for face verification\\\')\\n-parser.add_argument("-s", "--save", help="whether save",action="store_true")\\n-parser.add_argument(\\\'-th\\\',\\\'--threshold\\\',help=\\\'threshold to decide identical faces\\\',default=1.54, type=float)\\n-parser.add_argument("-u", "--update", help="whether perform update the facebank",action="store_true")\\n-parser.add_argument("-tta", "--tta", help="whether test time augmentation",action="store_true")\\n-parser.add_argument("-c", "--score", help="whether show the confidence score",action="store_true")\\n-args = parser.parse_args()\\n-\\n-conf = get_config(False)\\n-\\n-mtcnn = MTCNN()\\n-print(\\\'arcface loaded\\\')\\n-\\n-learner = face_learner(conf, True)\\n-learner.threshold = args.threshold\\n-if conf.device.type == \\\'cpu\\\':\\n-    learner.load_state(conf, \\\'cpu_final.pth\\\', True, True)\\n-else:\\n-    learner.load_state(conf, \\\'final.pth\\\', True, True)\\n-learner.model.eval()\\n-print(\\\'learner loaded\\\')\\n-\\n-if args.update:\\n-    targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\\n-    print(\\\'facebank updated\\\')\\n-else:\\n-    targets, names = load_facebank(conf)\\n-    print(\\\'facebank loaded\\\')\\n-\\n-# inital camera\\n-\\n-# cap = cv2.VideoCapture(r\\\'C:/Users/suyas/Desktop/Face-Recognition-master/test2.mp4\\\')\\n-cap = cv2.VideoCapture(0)\\n-cap.set(3,500)\\n-cap.set(4,500)\\n-\\n-\\n-class faceRec:\\n-    def __init__(self):\\n-        self.width = 800\\n-        self.height = 800\\n-        self.image = None\\n-    def main(self): \\n-        while cap.isOpened():\\n-            isSuccess,frame = cap.read()\\n-            if isSuccess:            \\n-                try:\\n-    #                 image = Image.fromarray(frame[...,::-1]) #bgr to rgb\\n-                    image = Image.fromarray(frame)\\n-                    bboxes, faces = mtcnn.align_multi(image, conf.face_limit, conf.min_face_size)\\n-                    bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\\n-                    bboxes = bboxes.astype(int)\\n-                    bboxes = bboxes + [-1,-1,1,1] # personal choice    \\n-                    results, score = learner.infer(conf, faces, targets, args.tta)\\n-                    for idx,bbox in enumerate(bboxes):\\n-                        if args.score:\\n-                            frame = draw_box_name(bbox, names[results[idx] + 1] + \\\'_{:.2f}\\\'.format(score[idx]), frame)\\n-                        else:\\n-                            frame = draw_box_name(bbox, names[results[idx] + 1], frame)\\n-                except:\\n-                    pass    \\n-                ret, jpeg = cv2.imencode(\\\'.jpg\\\', frame)\\n-                return jpeg.tostring()\\n-                # cv2.imshow(\\\'Arc Face Recognizer\\\', frame)\\n-\\n-\\n-            if cv2.waitKey(1)&0xFF == ord(\\\'q\\\'):\\n-                break\\n-\\n-        cap.release()\\n-\\n-        cv2.destroyAllWindows()    \\ndiff --git a/model.py b/model.py\\ndeleted file mode 100644\\nindex 6645455..0000000\\n--- a/model.py\\n+++ /dev/null\\n@@ -1,306 +0,0 @@\\n-from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\\n-import torch.nn.functional as F\\n-import torch\\n-from collections import namedtuple\\n-import math\\n-import pdb\\n-\\n-##################################  Original Arcface Model #############################################################\\n-\\n-class Flatten(Module):\\n-    def forward(self, input):\\n-        return input.view(input.size(0), -1)\\n-\\n-def l2_norm(input,axis=1):\\n-    norm = torch.norm(input,2,axis,True)\\n-    output = torch.div(input, norm)\\n-    return output\\n-\\n-class SEModule(Module):\\n-    def __init__(self, channels, reduction):\\n-        super(SEModule, self).__init__()\\n-        self.avg_pool = AdaptiveAvgPool2d(1)\\n-        self.fc1 = Conv2d(\\n-            channels, channels // reduction, kernel_size=1, padding=0 ,bias=False)\\n-        self.relu = ReLU(inplace=True)\\n-        self.fc2 = Conv2d(\\n-            channels // reduction, channels, kernel_size=1, padding=0 ,bias=False)\\n-        self.sigmoid = Sigmoid()\\n-\\n-    def forward(self, x):\\n-        module_input = x\\n-        x = self.avg_pool(x)\\n-        x = self.fc1(x)\\n-        x = self.relu(x)\\n-        x = self.fc2(x)\\n-        x = self.sigmoid(x)\\n-        return module_input * x\\n-\\n-class bottleneck_IR(Module):\\n-    def __init__(self, in_channel, depth, stride):\\n-        super(bottleneck_IR, self).__init__()\\n-        if in_channel == depth:\\n-            self.shortcut_layer = MaxPool2d(1, stride)\\n-        else:\\n-            self.shortcut_layer = Sequential(\\n-                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), BatchNorm2d(depth))\\n-        self.res_layer = Sequential(\\n-            BatchNorm2d(in_channel),\\n-            Conv2d(in_channel, depth, (3, 3), (1, 1), 1 ,bias=False), PReLU(depth),\\n-            Conv2d(depth, depth, (3, 3), stride, 1 ,bias=False), BatchNorm2d(depth))\\n-\\n-    def forward(self, x):\\n-        shortcut = self.shortcut_layer(x)\\n-        res = self.res_layer(x)\\n-        return res + shortcut\\n-\\n-class bottleneck_IR_SE(Module):\\n-    def __init__(self, in_channel, depth, stride):\\n-        super(bottleneck_IR_SE, self).__init__()\\n-        if in_channel == depth:\\n-            self.shortcut_layer = MaxPool2d(1, stride)\\n-        else:\\n-            self.shortcut_layer = Sequential(\\n-                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), \\n-                BatchNorm2d(depth))\\n-        self.res_layer = Sequential(\\n-            BatchNorm2d(in_channel),\\n-            Conv2d(in_channel, depth, (3,3), (1,1),1 ,bias=False),\\n-            PReLU(depth),\\n-            Conv2d(depth, depth, (3,3), stride, 1 ,bias=False),\\n-            BatchNorm2d(depth),\\n-            SEModule(depth,16)\\n-            )\\n-    def forward(self,x):\\n-        shortcut = self.shortcut_layer(x)\\n-        res = self.res_layer(x)\\n-        return res + shortcut\\n-\\n-class Bottleneck(namedtuple(\\\'Block\\\', [\\\'in_channel\\\', \\\'depth\\\', \\\'stride\\\'])):\\n-    \\\'\\\'\\\'A named tuple describing a ResNet block.\\\'\\\'\\\'\\n-    \\n-def get_block(in_channel, depth, num_units, stride = 2):\\n-    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units-1)]\\n-\\n-def get_blocks(num_layers):\\n-    if num_layers == 50:\\n-        blocks = [\\n-            get_block(in_channel=64, depth=64, num_units = 3),\\n-            get_block(in_channel=64, depth=128, num_units=4),\\n-            get_block(in_channel=128, depth=256, num_units=14),\\n-            get_block(in_channel=256, depth=512, num_units=3)\\n-        ]\\n-    elif num_layers == 100:\\n-        blocks = [\\n-            get_block(in_channel=64, depth=64, num_units=3),\\n-            get_block(in_channel=64, depth=128, num_units=13),\\n-            get_block(in_channel=128, depth=256, num_units=30),\\n-            get_block(in_channel=256, depth=512, num_units=3)\\n-        ]\\n-    elif num_layers == 152:\\n-        blocks = [\\n-            get_block(in_channel=64, depth=64, num_units=3),\\n-            get_block(in_channel=64, depth=128, num_units=8),\\n-            get_block(in_channel=128, depth=256, num_units=36),\\n-            get_block(in_channel=256, depth=512, num_units=3)\\n-        ]\\n-    return blocks\\n-\\n-class Backbone(Module):\\n-    def __init__(self, num_layers, drop_ratio, mode=\\\'ir\\\'):\\n-        super(Backbone, self).__init__()\\n-        assert num_layers in [50, 100, 152], \\\'num_layers should be 50,100, or 152\\\'\\n-        assert mode in [\\\'ir\\\', \\\'ir_se\\\'], \\\'mode should be ir or ir_se\\\'\\n-        blocks = get_blocks(num_layers)\\n-        if mode == \\\'ir\\\':\\n-            unit_module = bottleneck_IR\\n-        elif mode == \\\'ir_se\\\':\\n-            unit_module = bottleneck_IR_SE\\n-        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1 ,bias=False), \\n-                                      BatchNorm2d(64), \\n-                                      PReLU(64))\\n-        self.output_layer = Sequential(BatchNorm2d(512), \\n-                                       Dropout(drop_ratio),\\n-                                       Flatten(),\\n-                                       Linear(512 * 7 * 7, 512),\\n-                                       BatchNorm1d(512))\\n-        modules = []\\n-        for block in blocks:\\n-            for bottleneck in block:\\n-                modules.append(\\n-                    unit_module(bottleneck.in_channel,\\n-                                bottleneck.depth,\\n-                                bottleneck.stride))\\n-        self.body = Sequential(*modules)\\n-    \\n-    def forward(self,x):\\n-        x = self.input_layer(x)\\n-        x = self.body(x)\\n-        x = self.output_layer(x)\\n-        return l2_norm(x)\\n-\\n-##################################  MobileFaceNet #############################################################\\n-    \\n-class Conv_block(Module):\\n-    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\\n-        super(Conv_block, self).__init__()\\n-        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\\n-        self.bn = BatchNorm2d(out_c)\\n-        self.prelu = PReLU(out_c)\\n-    def forward(self, x):\\n-        x = self.conv(x)\\n-        x = self.bn(x)\\n-        x = self.prelu(x)\\n-        return x\\n-\\n-class Linear_block(Module):\\n-    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\\n-        super(Linear_block, self).__init__()\\n-        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\\n-        self.bn = BatchNorm2d(out_c)\\n-    def forward(self, x):\\n-        x = self.conv(x)\\n-        x = self.bn(x)\\n-        return x\\n-\\n-class Depth_Wise(Module):\\n-     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\\n-        super(Depth_Wise, self).__init__()\\n-        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\\n-        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\\n-        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\\n-        self.residual = residual\\n-     def forward(self, x):\\n-        if self.residual:\\n-            short_cut = x\\n-        x = self.conv(x)\\n-        x = self.conv_dw(x)\\n-        x = self.project(x)\\n-        if self.residual:\\n-            output = short_cut + x\\n-        else:\\n-            output = x\\n-        return output\\n-\\n-class Residual(Module):\\n-    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\\n-        super(Residual, self).__init__()\\n-        modules = []\\n-        for _ in range(num_block):\\n-            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\\n-        self.model = Sequential(*modules)\\n-    def forward(self, x):\\n-        return self.model(x)\\n-\\n-class MobileFaceNet(Module):\\n-    def __init__(self, embedding_size):\\n-        super(MobileFaceNet, self).__init__()\\n-        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\\n-        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\\n-        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\\n-        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\\n-        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\\n-        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\\n-        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\\n-        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\\n-        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\\n-        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\\n-        self.conv_6_flatten = Flatten()\\n-        self.linear = Linear(512, embedding_size, bias=False)\\n-        self.bn = BatchNorm1d(embedding_size)\\n-    \\n-    def forward(self, x):\\n-        out = self.conv1(x)\\n-\\n-        out = self.conv2_dw(out)\\n-\\n-        out = self.conv_23(out)\\n-\\n-        out = self.conv_3(out)\\n-        \\n-        out = self.conv_34(out)\\n-\\n-        out = self.conv_4(out)\\n-\\n-        out = self.conv_45(out)\\n-\\n-        out = self.conv_5(out)\\n-\\n-        out = self.conv_6_sep(out)\\n-\\n-        out = self.conv_6_dw(out)\\n-\\n-        out = self.conv_6_flatten(out)\\n-\\n-        out = self.linear(out)\\n-\\n-        out = self.bn(out)\\n-        return l2_norm(out)\\n-\\n-##################################  Arcface head #############################################################\\n-\\n-class Arcface(Module):\\n-    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \\n-    def __init__(self, embedding_size=512, classnum=51332,  s=64., m=0.5):\\n-        super(Arcface, self).__init__()\\n-        self.classnum = classnum\\n-        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\\n-        # initial kernel\\n-        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\\n-        self.m = m # the margin value, default is 0.5\\n-        self.s = s # scalar value default is 64, see normface https://arxiv.org/abs/1704.06369\\n-        self.cos_m = math.cos(m)\\n-        self.sin_m = math.sin(m)\\n-        self.mm = self.sin_m * m  # issue 1\\n-        self.threshold = math.cos(math.pi - m)\\n-    def forward(self, embbedings, label):\\n-        # weights norm\\n-        nB = len(embbedings)\\n-        kernel_norm = l2_norm(self.kernel,axis=0)\\n-        # cos(theta+m)\\n-        cos_theta = torch.mm(embbedings,kernel_norm)\\n-#         output = torch.mm(embbedings,kernel_norm)\\n-        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\\n-        cos_theta_2 = torch.pow(cos_theta, 2)\\n-        sin_theta_2 = 1 - cos_theta_2\\n-        sin_theta = torch.sqrt(sin_theta_2)\\n-        cos_theta_m = (cos_theta * self.cos_m - sin_theta * self.sin_m)\\n-        # this condition controls the theta+m should in range [0, pi]\\n-        #      0<=theta+m<=pi\\n-        #     -m<=theta<=pi-m\\n-        cond_v = cos_theta - self.threshold\\n-        cond_mask = cond_v <= 0\\n-        keep_val = (cos_theta - self.mm) # when theta not in [0,pi], use cosface instead\\n-        cos_theta_m[cond_mask] = keep_val[cond_mask]\\n-        output = cos_theta * 1.0 # a little bit hacky way to prevent in_place operation on cos_theta\\n-        idx_ = torch.arange(0, nB, dtype=torch.long)\\n-        output[idx_, label] = cos_theta_m[idx_, label]\\n-        output *= self.s # scale up in order to make softmax work, first introduced in normface\\n-        return output\\n-\\n-##################################  Cosface head #############################################################    \\n-    \\n-class Am_softmax(Module):\\n-    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \\n-    def __init__(self,embedding_size=512,classnum=51332):\\n-        super(Am_softmax, self).__init__()\\n-        self.classnum = classnum\\n-        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\\n-        # initial kernel\\n-        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\\n-        self.m = 0.35 # additive margin recommended by the paper\\n-        self.s = 30. # see normface https://arxiv.org/abs/1704.06369\\n-    def forward(self,embbedings,label):\\n-        kernel_norm = l2_norm(self.kernel,axis=0)\\n-        cos_theta = torch.mm(embbedings,kernel_norm)\\n-        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\\n-        phi = cos_theta - self.m\\n-        label = label.view(-1,1) #size=(B,1)\\n-        index = cos_theta.data * 0.0 #size=(B,Classnum)\\n-        index.scatter_(1,label.data.view(-1,1),1)\\n-        index = index.byte()\\n-        output = cos_theta * 1.0\\n-        output[index] = phi[index] #only change the correct predicted output\\n-        output *= self.s # scale up in order to make softmax work, first introduced in normface\\n-        return output\\n-\\ndiff --git a/mtcnn.py b/mtcnn.py\\ndeleted file mode 100644\\nindex 90a72c2..0000000\\n--- a/mtcnn.py\\n+++ /dev/null\\n@@ -1,151 +0,0 @@\\n-import numpy as np\\n-import torch\\n-from PIL import Image\\n-from torch.autograd import Variable\\n-from mtcnn_pytorch.src.get_nets import PNet, RNet, ONet\\n-from mtcnn_pytorch.src.box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\\n-from mtcnn_pytorch.src.first_stage import run_first_stage\\n-from mtcnn_pytorch.src.align_trans import get_reference_facial_points, warp_and_crop_face\\n-device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\\n-# device = \\\'cpu\\\'\\n-\\n-class MTCNN():\\n-    def __init__(self):\\n-        self.pnet = PNet().to(device)\\n-        self.rnet = RNet().to(device)\\n-        self.onet = ONet().to(device)\\n-        self.pnet.eval()\\n-        self.rnet.eval()\\n-        self.onet.eval()\\n-        self.refrence = get_reference_facial_points(default_square= True)\\n-        \\n-    def align(self, img):\\n-        _, landmarks = self.detect_faces(img)\\n-        facial5points = [[landmarks[0][j],landmarks[0][j+5]] for j in range(5)]\\n-        warped_face = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112,112))\\n-        return Image.fromarray(warped_face)\\n-    \\n-    def align_multi(self, img, limit=None, min_face_size=30.0):\\n-        boxes, landmarks = self.detect_faces(img, min_face_size)\\n-        if limit:\\n-            boxes = boxes[:limit]\\n-            landmarks = landmarks[:limit]\\n-        faces = []\\n-        for landmark in landmarks:\\n-            facial5points = [[landmark[j],landmark[j+5]] for j in range(5)]\\n-            warped_face = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112,112))\\n-            faces.append(Image.fromarray(warped_face))\\n-        return boxes, faces\\n-\\n-    def detect_faces(self, image, min_face_size=20.0,\\n-                    thresholds=[0.9, 0.8, 0.8],\\n-                    nms_thresholds=[0.9, 0.9, 0.8]):\\n-        """\\n-        Arguments:\\n-            image: an instance of PIL.Image.\\n-            min_face_size: a float number.\\n-            thresholds: a list of length 3.\\n-            nms_thresholds: a list of length 3.\\n-\\n-        Returns:\\n-            two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\\n-            bounding boxes and facial landmarks.\\n-        """\\n-\\n-        # BUILD AN IMAGE PYRAMID\\n-        width, height = image.size\\n-        min_length = min(height, width)\\n-\\n-        min_detection_size = 12\\n-        factor = 0.707  # sqrt(0.5)\\n-\\n-        # scales for scaling the image\\n-        scales = []\\n-\\n-        # scales the image so that\\n-        # minimum size that we can detect equals to\\n-        # minimum face size that we want to detect\\n-        m = min_detection_size/min_face_size\\n-        min_length *= m\\n-\\n-        factor_count = 0\\n-        while min_length > min_detection_size:\\n-            scales.append(m*factor**factor_count)\\n-            min_length *= factor\\n-            factor_count += 1\\n-\\n-        # STAGE 1\\n-\\n-        # it will be returned\\n-        bounding_boxes = []\\n-\\n-        with torch.no_grad():\\n-            # run P-Net on different scales\\n-            for s in scales:\\n-                boxes = run_first_stage(image, self.pnet, scale=s, threshold=thresholds[0])\\n-                bounding_boxes.append(boxes)\\n-\\n-            # collect boxes (and offsets, and scores) from different scales\\n-            bounding_boxes = [i for i in bounding_boxes if i is not None]\\n-            bounding_boxes = np.vstack(bounding_boxes)\\n-\\n-            keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\\n-            bounding_boxes = bounding_boxes[keep]\\n-\\n-            # use offsets predicted by pnet to transform bounding boxes\\n-            bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\\n-            # shape [n_boxes, 5]\\n-\\n-            bounding_boxes = convert_to_square(bounding_boxes)\\n-            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\n-\\n-            # STAGE 2\\n-\\n-            img_boxes = get_image_boxes(bounding_boxes, image, size=24)\\n-            img_boxes = torch.FloatTensor(img_boxes).to(device)\\n-\\n-            output = self.rnet(img_boxes)\\n-            offsets = output[0].cpu().data.numpy()  # shape [n_boxes, 4]\\n-            probs = output[1].cpu().data.numpy()  # shape [n_boxes, 2]\\n-\\n-            keep = np.where(probs[:, 1] > thresholds[1])[0]\\n-            bounding_boxes = bounding_boxes[keep]\\n-            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\n-            offsets = offsets[keep]\\n-\\n-            keep = nms(bounding_boxes, nms_thresholds[1])\\n-            bounding_boxes = bounding_boxes[keep]\\n-            bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\\n-            bounding_boxes = convert_to_square(bounding_boxes)\\n-            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\n-\\n-            # STAGE 3\\n-\\n-            img_boxes = get_image_boxes(bounding_boxes, image, size=48)\\n-            if len(img_boxes) == 0: \\n-                return [], []\\n-            img_boxes = torch.FloatTensor(img_boxes).to(device)\\n-            output = self.onet(img_boxes)\\n-            landmarks = output[0].cpu().data.numpy()  # shape [n_boxes, 10]\\n-            offsets = output[1].cpu().data.numpy()  # shape [n_boxes, 4]\\n-            probs = output[2].cpu().data.numpy()  # shape [n_boxes, 2]\\n-\\n-            keep = np.where(probs[:, 1] > thresholds[2])[0]\\n-            bounding_boxes = bounding_boxes[keep]\\n-            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\n-            offsets = offsets[keep]\\n-            landmarks = landmarks[keep]\\n-\\n-            # compute landmark points\\n-            width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\\n-            height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\\n-            xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\\n-            landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\\n-            landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\\n-\\n-            bounding_boxes = calibrate_box(bounding_boxes, offsets)\\n-            keep = nms(bounding_boxes, nms_thresholds[2], mode=\\\'min\\\')\\n-            bounding_boxes = bounding_boxes[keep]\\n-            landmarks = landmarks[keep]\\n-\\n-        return bounding_boxes, landmarks\\ndiff --git a/mtcnn_pytorch/src/__init__.py b/mtcnn_pytorch/src/__init__.py\\ndeleted file mode 100644\\nindex 617ba38..0000000\\n--- a/mtcnn_pytorch/src/__init__.py\\n+++ /dev/null\\n@@ -1,2 +0,0 @@\\n-from .visualization_utils import show_bboxes\\n-from .detector import detect_faces\\ndiff --git a/mtcnn_pytorch/src/align_trans.py b/mtcnn_pytorch/src/align_trans.py\\ndeleted file mode 100644\\nindex 72d4351..0000000\\n--- a/mtcnn_pytorch/src/align_trans.py\\n+++ /dev/null\\n@@ -1,304 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-"""\\n-Created on Mon Apr 24 15:43:29 2017\\n-@author: zhaoy\\n-"""\\n-import numpy as np\\n-import cv2\\n-\\n-# from scipy.linalg import lstsq\\n-# from scipy.ndimage import geometric_transform  # , map_coordinates\\n-\\n-from mtcnn_pytorch.src.matlab_cp2tform import get_similarity_transform_for_cv2\\n-\\n-# reference facial points, a list of coordinates (x,y)\\n-REFERENCE_FACIAL_POINTS = [\\n-    [30.29459953,  51.69630051],\\n-    [65.53179932,  51.50139999],\\n-    [48.02519989,  71.73660278],\\n-    [33.54930115,  92.3655014],\\n-    [62.72990036,  92.20410156]\\n-]\\n-\\n-DEFAULT_CROP_SIZE = (96, 112)\\n-\\n-\\n-class FaceWarpException(Exception):\\n-    def __str__(self):\\n-        return \\\'In File {}:{}\\\'.format(\\n-            __file__, super.__str__(self))\\n-\\n-\\n-def get_reference_facial_points(output_size=None,\\n-                                inner_padding_factor=0.0,\\n-                                outer_padding=(0, 0),\\n-                                default_square=False):\\n-    """\\n-    Function:\\n-    ----------\\n-        get reference 5 key points according to crop settings:\\n-        0. Set default crop_size:\\n-            if default_square: \\n-                crop_size = (112, 112)\\n-            else: \\n-                crop_size = (96, 112)\\n-        1. Pad the crop_size by inner_padding_factor in each side;\\n-        2. Resize crop_size into (output_size - outer_padding*2),\\n-            pad into output_size with outer_padding;\\n-        3. Output reference_5point;\\n-    Parameters:\\n-    ----------\\n-        @output_size: (w, h) or None\\n-            size of aligned face image\\n-        @inner_padding_factor: (w_factor, h_factor)\\n-            padding factor for inner (w, h)\\n-        @outer_padding: (w_pad, h_pad)\\n-            each row is a pair of coordinates (x, y)\\n-        @default_square: True or False\\n-            if True:\\n-                default crop_size = (112, 112)\\n-            else:\\n-                default crop_size = (96, 112);\\n-        !!! make sure, if output_size is not None:\\n-                (output_size - outer_padding) \\n-                = some_scale * (default crop_size * (1.0 + inner_padding_factor))\\n-    Returns:\\n-    ----------\\n-        @reference_5point: 5x2 np.array\\n-            each row is a pair of transformed coordinates (x, y)\\n-    """\\n-    #print(\\\'\\\\n===> get_reference_facial_points():\\\')\\n-\\n-    #print(\\\'---> Params:\\\')\\n-    #print(\\\'            output_size: \\\', output_size)\\n-    #print(\\\'            inner_padding_factor: \\\', inner_padding_factor)\\n-    #print(\\\'            outer_padding:\\\', outer_padding)\\n-    #print(\\\'            default_square: \\\', default_square)\\n-\\n-    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\\n-    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\\n-\\n-    # 0) make the inner region a square\\n-    if default_square:\\n-        size_diff = max(tmp_crop_size) - tmp_crop_size\\n-        tmp_5pts += size_diff / 2\\n-        tmp_crop_size += size_diff\\n-\\n-    #print(\\\'---> default:\\\')\\n-    #print(\\\'              crop_size = \\\', tmp_crop_size)\\n-    #print(\\\'              reference_5pts = \\\', tmp_5pts)\\n-\\n-    if (output_size and\\n-            output_size[0] == tmp_crop_size[0] and\\n-            output_size[1] == tmp_crop_size[1]):\\n-        #print(\\\'output_size == DEFAULT_CROP_SIZE {}: return default reference points\\\'.format(tmp_crop_size))\\n-        return tmp_5pts\\n-\\n-    if (inner_padding_factor == 0 and\\n-            outer_padding == (0, 0)):\\n-        if output_size is None:\\n-            #print(\\\'No paddings to do: return default reference points\\\')\\n-            return tmp_5pts\\n-        else:\\n-            raise FaceWarpException(\\n-                \\\'No paddings to do, output_size must be None or {}\\\'.format(tmp_crop_size))\\n-\\n-    # check output size\\n-    if not (0 <= inner_padding_factor <= 1.0):\\n-        raise FaceWarpException(\\\'Not (0 <= inner_padding_factor <= 1.0)\\\')\\n-\\n-    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\\n-            and output_size is None):\\n-        output_size = tmp_crop_size * \\\\\\n-            (1 + inner_padding_factor * 2).astype(np.int32)\\n-        output_size += np.array(outer_padding)\\n-        #print(\\\'              deduced from paddings, output_size = \\\', output_size)\\n-\\n-    if not (outer_padding[0] < output_size[0]\\n-            and outer_padding[1] < output_size[1]):\\n-        raise FaceWarpException(\\\'Not (outer_padding[0] < output_size[0]\\\'\\n-                                \\\'and outer_padding[1] < output_size[1])\\\')\\n-\\n-    # 1) pad the inner region according inner_padding_factor\\n-    #print(\\\'---> STEP1: pad the inner region according inner_padding_factor\\\')\\n-    if inner_padding_factor > 0:\\n-        size_diff = tmp_crop_size * inner_padding_factor * 2\\n-        tmp_5pts += size_diff / 2\\n-        tmp_crop_size += np.round(size_diff).astype(np.int32)\\n-\\n-    #print(\\\'              crop_size = \\\', tmp_crop_size)\\n-    #print(\\\'              reference_5pts = \\\', tmp_5pts)\\n-\\n-    # 2) resize the padded inner region\\n-    #print(\\\'---> STEP2: resize the padded inner region\\\')\\n-    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\\n-    #print(\\\'              crop_size = \\\', tmp_crop_size)\\n-    #print(\\\'              size_bf_outer_pad = \\\', size_bf_outer_pad)\\n-\\n-    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\\n-        raise FaceWarpException(\\\'Must have (output_size - outer_padding)\\\'\\n-                                \\\'= some_scale * (crop_size * (1.0 + inner_padding_factor)\\\')\\n-\\n-    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\\n-    #print(\\\'              resize scale_factor = \\\', scale_factor)\\n-    tmp_5pts = tmp_5pts * scale_factor\\n-#    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\\n-#    tmp_5pts = tmp_5pts + size_diff / 2\\n-    tmp_crop_size = size_bf_outer_pad\\n-    #print(\\\'              crop_size = \\\', tmp_crop_size)\\n-    #print(\\\'              reference_5pts = \\\', tmp_5pts)\\n-\\n-    # 3) add outer_padding to make output_size\\n-    reference_5point = tmp_5pts + np.array(outer_padding)\\n-    tmp_crop_size = output_size\\n-    #print(\\\'---> STEP3: add outer_padding to make output_size\\\')\\n-    #print(\\\'              crop_size = \\\', tmp_crop_size)\\n-    #print(\\\'              reference_5pts = \\\', tmp_5pts)\\n-\\n-    #print(\\\'===> end get_reference_facial_points\\\\n\\\')\\n-\\n-    return reference_5point\\n-\\n-\\n-def get_affine_transform_matrix(src_pts, dst_pts):\\n-    """\\n-    Function:\\n-    ----------\\n-        get affine transform matrix \\\'tfm\\\' from src_pts to dst_pts\\n-    Parameters:\\n-    ----------\\n-        @src_pts: Kx2 np.array\\n-            source points matrix, each row is a pair of coordinates (x, y)\\n-        @dst_pts: Kx2 np.array\\n-            destination points matrix, each row is a pair of coordinates (x, y)\\n-    Returns:\\n-    ----------\\n-        @tfm: 2x3 np.array\\n-            transform matrix from src_pts to dst_pts\\n-    """\\n-\\n-    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\\n-    n_pts = src_pts.shape[0]\\n-    ones = np.ones((n_pts, 1), src_pts.dtype)\\n-    src_pts_ = np.hstack([src_pts, ones])\\n-    dst_pts_ = np.hstack([dst_pts, ones])\\n-\\n-#    #print((\\\'src_pts_:\\\\n\\\' + str(src_pts_))\\n-#    #print((\\\'dst_pts_:\\\\n\\\' + str(dst_pts_))\\n-\\n-    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\\n-\\n-#    #print((\\\'np.linalg.lstsq return A: \\\\n\\\' + str(A))\\n-#    #print((\\\'np.linalg.lstsq return res: \\\\n\\\' + str(res))\\n-#    #print((\\\'np.linalg.lstsq return rank: \\\\n\\\' + str(rank))\\n-#    #print((\\\'np.linalg.lstsq return s: \\\\n\\\' + str(s))\\n-\\n-    if rank == 3:\\n-        tfm = np.float32([\\n-            [A[0, 0], A[1, 0], A[2, 0]],\\n-            [A[0, 1], A[1, 1], A[2, 1]]\\n-        ])\\n-    elif rank == 2:\\n-        tfm = np.float32([\\n-            [A[0, 0], A[1, 0], 0],\\n-            [A[0, 1], A[1, 1], 0]\\n-        ])\\n-\\n-    return tfm\\n-\\n-\\n-def warp_and_crop_face(src_img,\\n-                       facial_pts,\\n-                       reference_pts=None,\\n-                       crop_size=(96, 112),\\n-                       align_type=\\\'smilarity\\\'):\\n-    """\\n-    Function:\\n-    ----------\\n-        apply affine transform \\\'trans\\\' to uv\\n-    Parameters:\\n-    ----------\\n-        @src_img: 3x3 np.array\\n-            input image\\n-        @facial_pts: could be\\n-            1)a list of K coordinates (x,y)\\n-        or\\n-            2) Kx2 or 2xK np.array\\n-            each row or col is a pair of coordinates (x, y)\\n-        @reference_pts: could be\\n-            1) a list of K coordinates (x,y)\\n-        or\\n-            2) Kx2 or 2xK np.array\\n-            each row or col is a pair of coordinates (x, y)\\n-        or\\n-            3) None\\n-            if None, use default reference facial points\\n-        @crop_size: (w, h)\\n-            output face image size\\n-        @align_type: transform type, could be one of\\n-            1) \\\'similarity\\\': use similarity transform\\n-            2) \\\'cv2_affine\\\': use the first 3 points to do affine transform,\\n-                    by calling cv2.getAffineTransform()\\n-            3) \\\'affine\\\': use all points to do affine transform\\n-    Returns:\\n-    ----------\\n-        @face_img: output face image with size (w, h) = @crop_size\\n-    """\\n-\\n-    if reference_pts is None:\\n-        if crop_size[0] == 96 and crop_size[1] == 112:\\n-            reference_pts = REFERENCE_FACIAL_POINTS\\n-        else:\\n-            default_square = False\\n-            inner_padding_factor = 0\\n-            outer_padding = (0, 0)\\n-            output_size = crop_size\\n-\\n-            reference_pts = get_reference_facial_points(output_size,\\n-                                                        inner_padding_factor,\\n-                                                        outer_padding,\\n-                                                        default_square)\\n-\\n-    ref_pts = np.float32(reference_pts)\\n-    ref_pts_shp = ref_pts.shape\\n-    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\\n-        raise FaceWarpException(\\n-            \\\'reference_pts.shape must be (K,2) or (2,K) and K>2\\\')\\n-\\n-    if ref_pts_shp[0] == 2:\\n-        ref_pts = ref_pts.T\\n-\\n-    src_pts = np.float32(facial_pts)\\n-    src_pts_shp = src_pts.shape\\n-    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\\n-        raise FaceWarpException(\\n-            \\\'facial_pts.shape must be (K,2) or (2,K) and K>2\\\')\\n-\\n-    if src_pts_shp[0] == 2:\\n-        src_pts = src_pts.T\\n-\\n-#    #print(\\\'--->src_pts:\\\\n\\\', src_pts\\n-#    #print(\\\'--->ref_pts\\\\n\\\', ref_pts\\n-\\n-    if src_pts.shape != ref_pts.shape:\\n-        raise FaceWarpException(\\n-            \\\'facial_pts and reference_pts must have the same shape\\\')\\n-\\n-    if align_type is \\\'cv2_affine\\\':\\n-        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\\n-#        #print((\\\'cv2.getAffineTransform() returns tfm=\\\\n\\\' + str(tfm))\\n-    elif align_type is \\\'affine\\\':\\n-        tfm = get_affine_transform_matrix(src_pts, ref_pts)\\n-#        #print((\\\'get_affine_transform_matrix() returns tfm=\\\\n\\\' + str(tfm))\\n-    else:\\n-        tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\\n-#        #print((\\\'get_similarity_transform_for_cv2() returns tfm=\\\\n\\\' + str(tfm))\\n-\\n-#    #print(\\\'--->Transform matrix: \\\'\\n-#    #print((\\\'type(tfm):\\\' + str(type(tfm)))\\n-#    #print((\\\'tfm.dtype:\\\' + str(tfm.dtype))\\n-#    #print( tfm\\n-\\n-    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\\n-\\n-    return face_img\\n\\\\ No newline at end of file\\ndiff --git a/mtcnn_pytorch/src/box_utils.py b/mtcnn_pytorch/src/box_utils.py\\ndeleted file mode 100644\\nindex 3557387..0000000\\n--- a/mtcnn_pytorch/src/box_utils.py\\n+++ /dev/null\\n@@ -1,238 +0,0 @@\\n-import numpy as np\\n-from PIL import Image\\n-\\n-\\n-def nms(boxes, overlap_threshold=0.5, mode=\\\'union\\\'):\\n-    """Non-maximum suppression.\\n-\\n-    Arguments:\\n-        boxes: a float numpy array of shape [n, 5],\\n-            where each row is (xmin, ymin, xmax, ymax, score).\\n-        overlap_threshold: a float number.\\n-        mode: \\\'union\\\' or \\\'min\\\'.\\n-\\n-    Returns:\\n-        list with indices of the selected boxes\\n-    """\\n-\\n-    # if there are no boxes, return the empty list\\n-    if len(boxes) == 0:\\n-        return []\\n-\\n-    # list of picked indices\\n-    pick = []\\n-\\n-    # grab the coordinates of the bounding boxes\\n-    x1, y1, x2, y2, score = [boxes[:, i] for i in range(5)]\\n-\\n-    area = (x2 - x1 + 1.0)*(y2 - y1 + 1.0)\\n-    ids = np.argsort(score)  # in increasing order\\n-\\n-    while len(ids) > 0:\\n-\\n-        # grab index of the largest value\\n-        last = len(ids) - 1\\n-        i = ids[last]\\n-        pick.append(i)\\n-\\n-        # compute intersections\\n-        # of the box with the largest score\\n-        # with the rest of boxes\\n-\\n-        # left top corner of intersection boxes\\n-        ix1 = np.maximum(x1[i], x1[ids[:last]])\\n-        iy1 = np.maximum(y1[i], y1[ids[:last]])\\n-\\n-        # right bottom corner of intersection boxes\\n-        ix2 = np.minimum(x2[i], x2[ids[:last]])\\n-        iy2 = np.minimum(y2[i], y2[ids[:last]])\\n-\\n-        # width and height of intersection boxes\\n-        w = np.maximum(0.0, ix2 - ix1 + 1.0)\\n-        h = np.maximum(0.0, iy2 - iy1 + 1.0)\\n-\\n-        # intersections\\\' areas\\n-        inter = w * h\\n-        if mode == \\\'min\\\':\\n-            overlap = inter/np.minimum(area[i], area[ids[:last]])\\n-        elif mode == \\\'union\\\':\\n-            # intersection over union (IoU)\\n-            overlap = inter/(area[i] + area[ids[:last]] - inter)\\n-\\n-        # delete all boxes where overlap is too big\\n-        ids = np.delete(\\n-            ids,\\n-            np.concatenate([[last], np.where(overlap > overlap_threshold)[0]])\\n-        )\\n-\\n-    return pick\\n-\\n-\\n-def convert_to_square(bboxes):\\n-    """Convert bounding boxes to a square form.\\n-\\n-    Arguments:\\n-        bboxes: a float numpy array of shape [n, 5].\\n-\\n-    Returns:\\n-        a float numpy array of shape [n, 5],\\n-            squared bounding boxes.\\n-    """\\n-\\n-    square_bboxes = np.zeros_like(bboxes)\\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\\n-    h = y2 - y1 + 1.0\\n-    w = x2 - x1 + 1.0\\n-    max_side = np.maximum(h, w)\\n-    square_bboxes[:, 0] = x1 + w*0.5 - max_side*0.5\\n-    square_bboxes[:, 1] = y1 + h*0.5 - max_side*0.5\\n-    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\\n-    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\\n-    return square_bboxes\\n-\\n-\\n-def calibrate_box(bboxes, offsets):\\n-    """Transform bounding boxes to be more like true bounding boxes.\\n-    \\\'offsets\\\' is one of the outputs of the nets.\\n-\\n-    Arguments:\\n-        bboxes: a float numpy array of shape [n, 5].\\n-        offsets: a float numpy array of shape [n, 4].\\n-\\n-    Returns:\\n-        a float numpy array of shape [n, 5].\\n-    """\\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\\n-    w = x2 - x1 + 1.0\\n-    h = y2 - y1 + 1.0\\n-    w = np.expand_dims(w, 1)\\n-    h = np.expand_dims(h, 1)\\n-\\n-    # this is what happening here:\\n-    # tx1, ty1, tx2, ty2 = [offsets[:, i] for i in range(4)]\\n-    # x1_true = x1 + tx1*w\\n-    # y1_true = y1 + ty1*h\\n-    # x2_true = x2 + tx2*w\\n-    # y2_true = y2 + ty2*h\\n-    # below is just more compact form of this\\n-\\n-    # are offsets always such that\\n-    # x1 < x2 and y1 < y2 ?\\n-\\n-    translation = np.hstack([w, h, w, h])*offsets\\n-    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\\n-    return bboxes\\n-\\n-\\n-def get_image_boxes(bounding_boxes, img, size=24):\\n-    """Cut out boxes from the image.\\n-\\n-    Arguments:\\n-        bounding_boxes: a float numpy array of shape [n, 5].\\n-        img: an instance of PIL.Image.\\n-        size: an integer, size of cutouts.\\n-\\n-    Returns:\\n-        a float numpy array of shape [n, 3, size, size].\\n-    """\\n-\\n-    num_boxes = len(bounding_boxes)\\n-    width, height = img.size\\n-\\n-    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\\n-    img_boxes = np.zeros((num_boxes, 3, size, size), \\\'float32\\\')\\n-\\n-    for i in range(num_boxes):\\n-        img_box = np.zeros((h[i], w[i], 3), \\\'uint8\\\')\\n-\\n-        img_array = np.asarray(img, \\\'uint8\\\')\\n-        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] =\\\\\\n-            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\\n-\\n-        # resize\\n-        img_box = Image.fromarray(img_box)\\n-        img_box = img_box.resize((size, size), Image.BILINEAR)\\n-        img_box = np.asarray(img_box, \\\'float32\\\')\\n-\\n-        img_boxes[i, :, :, :] = _preprocess(img_box)\\n-\\n-    return img_boxes\\n-\\n-\\n-def correct_bboxes(bboxes, width, height):\\n-    """Crop boxes that are too big and get coordinates\\n-    with respect to cutouts.\\n-\\n-    Arguments:\\n-        bboxes: a float numpy array of shape [n, 5],\\n-            where each row is (xmin, ymin, xmax, ymax, score).\\n-        width: a float number.\\n-        height: a float number.\\n-\\n-    Returns:\\n-        dy, dx, edy, edx: a int numpy arrays of shape [n],\\n-            coordinates of the boxes with respect to the cutouts.\\n-        y, x, ey, ex: a int numpy arrays of shape [n],\\n-            corrected ymin, xmin, ymax, xmax.\\n-        h, w: a int numpy arrays of shape [n],\\n-            just heights and widths of boxes.\\n-\\n-        in the following order:\\n-            [dy, edy, dx, edx, y, ey, x, ex, w, h].\\n-    """\\n-\\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\\n-    w, h = x2 - x1 + 1.0,  y2 - y1 + 1.0\\n-    num_boxes = bboxes.shape[0]\\n-\\n-    # \\\'e\\\' stands for end\\n-    # (x, y) -> (ex, ey)\\n-    x, y, ex, ey = x1, y1, x2, y2\\n-\\n-    # we need to cut out a box from the image.\\n-    # (x, y, ex, ey) are corrected coordinates of the box\\n-    # in the image.\\n-    # (dx, dy, edx, edy) are coordinates of the box in the cutout\\n-    # from the image.\\n-    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\\n-    edx, edy = w.copy() - 1.0, h.copy() - 1.0\\n-\\n-    # if box\\\'s bottom right corner is too far right\\n-    ind = np.where(ex > width - 1.0)[0]\\n-    edx[ind] = w[ind] + width - 2.0 - ex[ind]\\n-    ex[ind] = width - 1.0\\n-\\n-    # if box\\\'s bottom right corner is too low\\n-    ind = np.where(ey > height - 1.0)[0]\\n-    edy[ind] = h[ind] + height - 2.0 - ey[ind]\\n-    ey[ind] = height - 1.0\\n-\\n-    # if box\\\'s top left corner is too far left\\n-    ind = np.where(x < 0.0)[0]\\n-    dx[ind] = 0.0 - x[ind]\\n-    x[ind] = 0.0\\n-\\n-    # if box\\\'s top left corner is too high\\n-    ind = np.where(y < 0.0)[0]\\n-    dy[ind] = 0.0 - y[ind]\\n-    y[ind] = 0.0\\n-\\n-    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\\n-    return_list = [i.astype(\\\'int32\\\') for i in return_list]\\n-\\n-    return return_list\\n-\\n-\\n-def _preprocess(img):\\n-    """Preprocessing step before feeding the network.\\n-\\n-    Arguments:\\n-        img: a float numpy array of shape [h, w, c].\\n-\\n-    Returns:\\n-        a float numpy array of shape [1, c, h, w].\\n-    """\\n-    img = img.transpose((2, 0, 1))\\n-    img = np.expand_dims(img, 0)\\n-    img = (img - 127.5)*0.0078125\\n-    return img\\ndiff --git a/mtcnn_pytorch/src/detector.py b/mtcnn_pytorch/src/detector.py\\ndeleted file mode 100644\\nindex f66eaa5..0000000\\n--- a/mtcnn_pytorch/src/detector.py\\n+++ /dev/null\\n@@ -1,126 +0,0 @@\\n-import numpy as np\\n-import torch\\n-from torch.autograd import Variable\\n-from .get_nets import PNet, RNet, ONet\\n-from .box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\\n-from .first_stage import run_first_stage\\n-\\n-\\n-def detect_faces(image, min_face_size=20.0,\\n-                 thresholds=[0.6, 0.7, 0.8],\\n-                 nms_thresholds=[0.7, 0.7, 0.7]):\\n-    """\\n-    Arguments:\\n-        image: an instance of PIL.Image.\\n-        min_face_size: a float number.\\n-        thresholds: a list of length 3.\\n-        nms_thresholds: a list of length 3.\\n-\\n-    Returns:\\n-        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\\n-        bounding boxes and facial landmarks.\\n-    """\\n-\\n-    # LOAD MODELS\\n-    pnet = PNet()\\n-    rnet = RNet()\\n-    onet = ONet()\\n-    onet.eval()\\n-\\n-    # BUILD AN IMAGE PYRAMID\\n-    width, height = image.size\\n-    min_length = min(height, width)\\n-\\n-    min_detection_size = 12\\n-    factor = 0.707  # sqrt(0.5)\\n-\\n-    # scales for scaling the image\\n-    scales = []\\n-\\n-    # scales the image so that\\n-    # minimum size that we can detect equals to\\n-    # minimum face size that we want to detect\\n-    m = min_detection_size/min_face_size\\n-    min_length *= m\\n-\\n-    factor_count = 0\\n-    while min_length > min_detection_size:\\n-        scales.append(m*factor**factor_count)\\n-        min_length *= factor\\n-        factor_count += 1\\n-\\n-    # STAGE 1\\n-\\n-    # it will be returned\\n-    bounding_boxes = []\\n-    \\n-    with torch.no_grad():\\n-        # run P-Net on different scales\\n-        for s in scales:\\n-            boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\\n-            bounding_boxes.append(boxes)\\n-\\n-        # collect boxes (and offsets, and scores) from different scales\\n-        bounding_boxes = [i for i in bounding_boxes if i is not None]\\n-        bounding_boxes = np.vstack(bounding_boxes)\\n-\\n-        keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\\n-        bounding_boxes = bounding_boxes[keep]\\n-\\n-        # use offsets predicted by pnet to transform bounding boxes\\n-        bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\\n-        # shape [n_boxes, 5]\\n-\\n-        bounding_boxes = convert_to_square(bounding_boxes)\\n-        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\n-\\n-        # STAGE 2\\n-\\n-        img_boxes = get_image_boxes(bounding_boxes, image, size=24)\\n-        img_boxes = torch.FloatTensor(img_boxes)\\n-\\n-        output = rnet(img_boxes)\\n-        offsets = output[0].data.numpy()  # shape [n_boxes, 4]\\n-        probs = output[1].data.numpy()  # shape [n_boxes, 2]\\n-\\n-        keep = np.where(probs[:, 1] > thresholds[1])[0]\\n-        bounding_boxes = bounding_boxes[keep]\\n-        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\n-        offsets = offsets[keep]\\n-\\n-        keep = nms(bounding_boxes, nms_thresholds[1])\\n-        bounding_boxes = bounding_boxes[keep]\\n-        bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\\n-        bounding_boxes = convert_to_square(bounding_boxes)\\n-        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\n-\\n-        # STAGE 3\\n-\\n-        img_boxes = get_image_boxes(bounding_boxes, image, size=48)\\n-        if len(img_boxes) == 0: \\n-            return [], []\\n-        img_boxes = torch.FloatTensor(img_boxes)\\n-        output = onet(img_boxes)\\n-        landmarks = output[0].data.numpy()  # shape [n_boxes, 10]\\n-        offsets = output[1].data.numpy()  # shape [n_boxes, 4]\\n-        probs = output[2].data.numpy()  # shape [n_boxes, 2]\\n-\\n-        keep = np.where(probs[:, 1] > thresholds[2])[0]\\n-        bounding_boxes = bounding_boxes[keep]\\n-        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\n-        offsets = offsets[keep]\\n-        landmarks = landmarks[keep]\\n-\\n-        # compute landmark points\\n-        width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\\n-        height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\\n-        xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\\n-        landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\\n-        landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\\n-\\n-        bounding_boxes = calibrate_box(bounding_boxes, offsets)\\n-        keep = nms(bounding_boxes, nms_thresholds[2], mode=\\\'min\\\')\\n-        bounding_boxes = bounding_boxes[keep]\\n-        landmarks = landmarks[keep]\\n-\\n-    return bounding_boxes, landmarks\\ndiff --git a/mtcnn_pytorch/src/first_stage.py b/mtcnn_pytorch/src/first_stage.py\\ndeleted file mode 100644\\nindex 55ed04a..0000000\\n--- a/mtcnn_pytorch/src/first_stage.py\\n+++ /dev/null\\n@@ -1,99 +0,0 @@\\n-import torch\\n-from torch.autograd import Variable\\n-import math\\n-from PIL import Image\\n-import numpy as np\\n-from .box_utils import nms, _preprocess\\n-device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\\n-# device = \\\'cpu\\\'\\n-\\n-def run_first_stage(image, net, scale, threshold):\\n-    """Run P-Net, generate bounding boxes, and do NMS.\\n-\\n-    Arguments:\\n-        image: an instance of PIL.Image.\\n-        net: an instance of pytorch\\\'s nn.Module, P-Net.\\n-        scale: a float number,\\n-            scale width and height of the image by this number.\\n-        threshold: a float number,\\n-            threshold on the probability of a face when generating\\n-            bounding boxes from predictions of the net.\\n-\\n-    Returns:\\n-        a float numpy array of shape [n_boxes, 9],\\n-            bounding boxes with scores and offsets (4 + 1 + 4).\\n-    """\\n-\\n-    # scale the image and convert it to a float array\\n-    width, height = image.size\\n-    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\\n-    img = image.resize((sw, sh), Image.BILINEAR)\\n-    img = np.asarray(img, \\\'float32\\\')\\n-\\n-    img = torch.FloatTensor(_preprocess(img)).to(device)\\n-    with torch.no_grad():\\n-        output = net(img)\\n-        probs = output[1].cpu().data.numpy()[0, 1, :, :]\\n-        offsets = output[0].cpu().data.numpy()\\n-        # probs: probability of a face at each sliding window\\n-        # offsets: transformations to true bounding boxes\\n-\\n-        boxes = _generate_bboxes(probs, offsets, scale, threshold)\\n-        if len(boxes) == 0:\\n-            return None\\n-\\n-        keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\\n-    return boxes[keep]\\n-\\n-\\n-def _generate_bboxes(probs, offsets, scale, threshold):\\n-    """Generate bounding boxes at places\\n-    where there is probably a face.\\n-\\n-    Arguments:\\n-        probs: a float numpy array of shape [n, m].\\n-        offsets: a float numpy array of shape [1, 4, n, m].\\n-        scale: a float number,\\n-            width and height of the image were scaled by this number.\\n-        threshold: a float number.\\n-\\n-    Returns:\\n-        a float numpy array of shape [n_boxes, 9]\\n-    """\\n-\\n-    # applying P-Net is equivalent, in some sense, to\\n-    # moving 12x12 window with stride 2\\n-    stride = 2\\n-    cell_size = 12\\n-\\n-    # indices of boxes where there is probably a face\\n-    inds = np.where(probs > threshold)\\n-\\n-    if inds[0].size == 0:\\n-        return np.array([])\\n-\\n-    # transformations of bounding boxes\\n-    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\\n-    # they are defined as:\\n-    # w = x2 - x1 + 1\\n-    # h = y2 - y1 + 1\\n-    # x1_true = x1 + tx1*w\\n-    # x2_true = x2 + tx2*w\\n-    # y1_true = y1 + ty1*h\\n-    # y2_true = y2 + ty2*h\\n-\\n-    offsets = np.array([tx1, ty1, tx2, ty2])\\n-    score = probs[inds[0], inds[1]]\\n-\\n-    # P-Net is applied to scaled images\\n-    # so we need to rescale bounding boxes back\\n-    bounding_boxes = np.vstack([\\n-        np.round((stride*inds[1] + 1.0)/scale),\\n-        np.round((stride*inds[0] + 1.0)/scale),\\n-        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\\n-        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\\n-        score, offsets\\n-    ])\\n-    # why one is added?\\n-\\n-    return bounding_boxes.T\\ndiff --git a/mtcnn_pytorch/src/get_nets.py b/mtcnn_pytorch/src/get_nets.py\\ndeleted file mode 100644\\nindex 6df979a..0000000\\n--- a/mtcnn_pytorch/src/get_nets.py\\n+++ /dev/null\\n@@ -1,169 +0,0 @@\\n-import torch\\n-import torch.nn as nn\\n-import torch.nn.functional as F\\n-from collections import OrderedDict\\n-import numpy as np\\n-\\n-\\n-class Flatten(nn.Module):\\n-\\n-    def __init__(self):\\n-        super(Flatten, self).__init__()\\n-\\n-    def forward(self, x):\\n-        """\\n-        Arguments:\\n-            x: a float tensor with shape [batch_size, c, h, w].\\n-        Returns:\\n-            a float tensor with shape [batch_size, c*h*w].\\n-        """\\n-\\n-        # without this pretrained model isn\\\'t working\\n-        x = x.transpose(3, 2).contiguous()\\n-\\n-        return x.view(x.size(0), -1)\\n-\\n-\\n-class PNet(nn.Module):\\n-\\n-    def __init__(self):\\n-\\n-        super(PNet, self).__init__()\\n-\\n-        # suppose we have input with size HxW, then\\n-        # after first layer: H - 2,\\n-        # after pool: ceil((H - 2)/2),\\n-        # after second conv: ceil((H - 2)/2) - 2,\\n-        # after last conv: ceil((H - 2)/2) - 4,\\n-        # and the same for W\\n-\\n-        self.features = nn.Sequential(OrderedDict([\\n-            (\\\'conv1\\\', nn.Conv2d(3, 10, 3, 1)),\\n-            (\\\'prelu1\\\', nn.PReLU(10)),\\n-            (\\\'pool1\\\', nn.MaxPool2d(2, 2, ceil_mode=True)),\\n-\\n-            (\\\'conv2\\\', nn.Conv2d(10, 16, 3, 1)),\\n-            (\\\'prelu2\\\', nn.PReLU(16)),\\n-\\n-            (\\\'conv3\\\', nn.Conv2d(16, 32, 3, 1)),\\n-            (\\\'prelu3\\\', nn.PReLU(32))\\n-        ]))\\n-\\n-        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\\n-        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\\n-\\n-        weights = np.load(\\\'mtcnn_pytorch/src/weights/pnet.npy\\\')[()]\\n-        for n, p in self.named_parameters():\\n-            p.data = torch.FloatTensor(weights[n])\\n-\\n-    def forward(self, x):\\n-        """\\n-        Arguments:\\n-            x: a float tensor with shape [batch_size, 3, h, w].\\n-        Returns:\\n-            b: a float tensor with shape [batch_size, 4, h\\\', w\\\'].\\n-            a: a float tensor with shape [batch_size, 2, h\\\', w\\\'].\\n-        """\\n-        x = self.features(x)\\n-        a = self.conv4_1(x)\\n-        b = self.conv4_2(x)\\n-        a = F.softmax(a, dim= 1)\\n-        return b, a\\n-\\n-\\n-class RNet(nn.Module):\\n-\\n-    def __init__(self):\\n-\\n-        super(RNet, self).__init__()\\n-\\n-        self.features = nn.Sequential(OrderedDict([\\n-            (\\\'conv1\\\', nn.Conv2d(3, 28, 3, 1)),\\n-            (\\\'prelu1\\\', nn.PReLU(28)),\\n-            (\\\'pool1\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\n-\\n-            (\\\'conv2\\\', nn.Conv2d(28, 48, 3, 1)),\\n-            (\\\'prelu2\\\', nn.PReLU(48)),\\n-            (\\\'pool2\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\n-\\n-            (\\\'conv3\\\', nn.Conv2d(48, 64, 2, 1)),\\n-            (\\\'prelu3\\\', nn.PReLU(64)),\\n-\\n-            (\\\'flatten\\\', Flatten()),\\n-            (\\\'conv4\\\', nn.Linear(576, 128)),\\n-            (\\\'prelu4\\\', nn.PReLU(128))\\n-        ]))\\n-\\n-        self.conv5_1 = nn.Linear(128, 2)\\n-        self.conv5_2 = nn.Linear(128, 4)\\n-\\n-        weights = np.load(\\\'mtcnn_pytorch/src/weights/rnet.npy\\\')[()]\\n-        for n, p in self.named_parameters():\\n-            p.data = torch.FloatTensor(weights[n])\\n-\\n-    def forward(self, x):\\n-        """\\n-        Arguments:\\n-            x: a float tensor with shape [batch_size, 3, h, w].\\n-        Returns:\\n-            b: a float tensor with shape [batch_size, 4].\\n-            a: a float tensor with shape [batch_size, 2].\\n-        """\\n-        x = self.features(x)\\n-        a = self.conv5_1(x)\\n-        b = self.conv5_2(x)\\n-        a = F.softmax(a, dim=-1)\\n-        return b, a\\n-\\n-\\n-class ONet(nn.Module):\\n-\\n-    def __init__(self):\\n-\\n-        super(ONet, self).__init__()\\n-\\n-        self.features = nn.Sequential(OrderedDict([\\n-            (\\\'conv1\\\', nn.Conv2d(3, 32, 3, 1)),\\n-            (\\\'prelu1\\\', nn.PReLU(32)),\\n-            (\\\'pool1\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\n-\\n-            (\\\'conv2\\\', nn.Conv2d(32, 64, 3, 1)),\\n-            (\\\'prelu2\\\', nn.PReLU(64)),\\n-            (\\\'pool2\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\n-\\n-            (\\\'conv3\\\', nn.Conv2d(64, 64, 3, 1)),\\n-            (\\\'prelu3\\\', nn.PReLU(64)),\\n-            (\\\'pool3\\\', nn.MaxPool2d(2, 2, ceil_mode=True)),\\n-\\n-            (\\\'conv4\\\', nn.Conv2d(64, 128, 2, 1)),\\n-            (\\\'prelu4\\\', nn.PReLU(128)),\\n-\\n-            (\\\'flatten\\\', Flatten()),\\n-            (\\\'conv5\\\', nn.Linear(1152, 256)),\\n-            (\\\'drop5\\\', nn.Dropout(0.25)),\\n-            (\\\'prelu5\\\', nn.PReLU(256)),\\n-        ]))\\n-\\n-        self.conv6_1 = nn.Linear(256, 2)\\n-        self.conv6_2 = nn.Linear(256, 4)\\n-        self.conv6_3 = nn.Linear(256, 10)\\n-\\n-        weights = np.load(\\\'mtcnn_pytorch/src/weights/onet.npy\\\')[()]\\n-        for n, p in self.named_parameters():\\n-            p.data = torch.FloatTensor(weights[n])\\n-\\n-    def forward(self, x):\\n-        """\\n-        Arguments:\\n-            x: a float tensor with shape [batch_size, 3, h, w].\\n-        Returns:\\n-            c: a float tensor with shape [batch_size, 10].\\n-            b: a float tensor with shape [batch_size, 4].\\n-            a: a float tensor with shape [batch_size, 2].\\n-        """\\n-        x = self.features(x)\\n-        a = self.conv6_1(x)\\n-        b = self.conv6_2(x)\\n-        c = self.conv6_3(x)\\n-        a = F.softmax(a, dim = -1)\\n-        return c, b, a\\ndiff --git a/mtcnn_pytorch/src/matlab_cp2tform.py b/mtcnn_pytorch/src/matlab_cp2tform.py\\ndeleted file mode 100644\\nindex cdcdf96..0000000\\n--- a/mtcnn_pytorch/src/matlab_cp2tform.py\\n+++ /dev/null\\n@@ -1,350 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-"""\\n-Created on Tue Jul 11 06:54:28 2017\\n-\\n-@author: zhaoyafei\\n-"""\\n-\\n-import numpy as np\\n-from numpy.linalg import inv, norm, lstsq\\n-from numpy.linalg import matrix_rank as rank\\n-\\n-class MatlabCp2tormException(Exception):\\n-    def __str__(self):\\n-        return \\\'In File {}:{}\\\'.format(\\n-                __file__, super.__str__(self))\\n-\\n-def tformfwd(trans, uv):\\n-    """\\n-    Function:\\n-    ----------\\n-        apply affine transform \\\'trans\\\' to uv\\n-\\n-    Parameters:\\n-    ----------\\n-        @trans: 3x3 np.array\\n-            transform matrix\\n-        @uv: Kx2 np.array\\n-            each row is a pair of coordinates (x, y)\\n-\\n-    Returns:\\n-    ----------\\n-        @xy: Kx2 np.array\\n-            each row is a pair of transformed coordinates (x, y)\\n-    """\\n-    uv = np.hstack((\\n-        uv, np.ones((uv.shape[0], 1))\\n-    ))\\n-    xy = np.dot(uv, trans)\\n-    xy = xy[:, 0:-1]\\n-    return xy\\n-\\n-\\n-def tforminv(trans, uv):\\n-    """\\n-    Function:\\n-    ----------\\n-        apply the inverse of affine transform \\\'trans\\\' to uv\\n-\\n-    Parameters:\\n-    ----------\\n-        @trans: 3x3 np.array\\n-            transform matrix\\n-        @uv: Kx2 np.array\\n-            each row is a pair of coordinates (x, y)\\n-\\n-    Returns:\\n-    ----------\\n-        @xy: Kx2 np.array\\n-            each row is a pair of inverse-transformed coordinates (x, y)\\n-    """\\n-    Tinv = inv(trans)\\n-    xy = tformfwd(Tinv, uv)\\n-    return xy\\n-\\n-\\n-def findNonreflectiveSimilarity(uv, xy, options=None):\\n-\\n-    options = {\\\'K\\\': 2}\\n-\\n-    K = options[\\\'K\\\']\\n-    M = xy.shape[0]\\n-    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\\n-    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\\n-    # print(\\\'--->x, y:\\\\n\\\', x, y\\n-\\n-    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\\n-    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\\n-    X = np.vstack((tmp1, tmp2))\\n-    # print(\\\'--->X.shape: \\\', X.shape\\n-    # print(\\\'X:\\\\n\\\', X\\n-\\n-    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\\n-    v = uv[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\\n-    U = np.vstack((u, v))\\n-    # print(\\\'--->U.shape: \\\', U.shape\\n-    # print(\\\'U:\\\\n\\\', U\\n-\\n-    # We know that X * r = U\\n-    if rank(X) >= 2 * K:\\n-        r, _, _, _ = lstsq(X, U)\\n-        r = np.squeeze(r)\\n-    else:\\n-        raise Exception(\\\'cp2tform:twoUniquePointsReq\\\')\\n-\\n-    # print(\\\'--->r:\\\\n\\\', r\\n-\\n-    sc = r[0]\\n-    ss = r[1]\\n-    tx = r[2]\\n-    ty = r[3]\\n-\\n-    Tinv = np.array([\\n-        [sc, -ss, 0],\\n-        [ss,  sc, 0],\\n-        [tx,  ty, 1]\\n-    ])\\n-\\n-    # print(\\\'--->Tinv:\\\\n\\\', Tinv\\n-\\n-    T = inv(Tinv)\\n-    # print(\\\'--->T:\\\\n\\\', T\\n-\\n-    T[:, 2] = np.array([0, 0, 1])\\n-\\n-    return T, Tinv\\n-\\n-\\n-def findSimilarity(uv, xy, options=None):\\n-\\n-    options = {\\\'K\\\': 2}\\n-\\n-#    uv = np.array(uv)\\n-#    xy = np.array(xy)\\n-\\n-    # Solve for trans1\\n-    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\\n-\\n-    # Solve for trans2\\n-\\n-    # manually reflect the xy data across the Y-axis\\n-    xyR = xy\\n-    xyR[:, 0] = -1 * xyR[:, 0]\\n-\\n-    trans2r, trans2r_inv = findNonreflectiveSimilarity(uv, xyR, options)\\n-\\n-    # manually reflect the tform to undo the reflection done on xyR\\n-    TreflectY = np.array([\\n-        [-1, 0, 0],\\n-        [0, 1, 0],\\n-        [0, 0, 1]\\n-    ])\\n-\\n-    trans2 = np.dot(trans2r, TreflectY)\\n-\\n-    # Figure out if trans1 or trans2 is better\\n-    xy1 = tformfwd(trans1, uv)\\n-    norm1 = norm(xy1 - xy)\\n-\\n-    xy2 = tformfwd(trans2, uv)\\n-    norm2 = norm(xy2 - xy)\\n-\\n-    if norm1 <= norm2:\\n-        return trans1, trans1_inv\\n-    else:\\n-        trans2_inv = inv(trans2)\\n-        return trans2, trans2_inv\\n-\\n-\\n-def get_similarity_transform(src_pts, dst_pts, reflective=True):\\n-    """\\n-    Function:\\n-    ----------\\n-        Find Similarity Transform Matrix \\\'trans\\\':\\n-            u = src_pts[:, 0]\\n-            v = src_pts[:, 1]\\n-            x = dst_pts[:, 0]\\n-            y = dst_pts[:, 1]\\n-            [x, y, 1] = [u, v, 1] * trans\\n-\\n-    Parameters:\\n-    ----------\\n-        @src_pts: Kx2 np.array\\n-            source points, each row is a pair of coordinates (x, y)\\n-        @dst_pts: Kx2 np.array\\n-            destination points, each row is a pair of transformed\\n-            coordinates (x, y)\\n-        @reflective: True or False\\n-            if True:\\n-                use reflective similarity transform\\n-            else:\\n-                use non-reflective similarity transform\\n-\\n-    Returns:\\n-    ----------\\n-       @trans: 3x3 np.array\\n-            transform matrix from uv to xy\\n-        trans_inv: 3x3 np.array\\n-            inverse of trans, transform matrix from xy to uv\\n-    """\\n-\\n-    if reflective:\\n-        trans, trans_inv = findSimilarity(src_pts, dst_pts)\\n-    else:\\n-        trans, trans_inv = findNonreflectiveSimilarity(src_pts, dst_pts)\\n-\\n-    return trans, trans_inv\\n-\\n-\\n-def cvt_tform_mat_for_cv2(trans):\\n-    """\\n-    Function:\\n-    ----------\\n-        Convert Transform Matrix \\\'trans\\\' into \\\'cv2_trans\\\' which could be\\n-        directly used by cv2.warpAffine():\\n-            u = src_pts[:, 0]\\n-            v = src_pts[:, 1]\\n-            x = dst_pts[:, 0]\\n-            y = dst_pts[:, 1]\\n-            [x, y].T = cv_trans * [u, v, 1].T\\n-\\n-    Parameters:\\n-    ----------\\n-        @trans: 3x3 np.array\\n-            transform matrix from uv to xy\\n-\\n-    Returns:\\n-    ----------\\n-        @cv2_trans: 2x3 np.array\\n-            transform matrix from src_pts to dst_pts, could be directly used\\n-            for cv2.warpAffine()\\n-    """\\n-    cv2_trans = trans[:, 0:2].T\\n-\\n-    return cv2_trans\\n-\\n-\\n-def get_similarity_transform_for_cv2(src_pts, dst_pts, reflective=True):\\n-    """\\n-    Function:\\n-    ----------\\n-        Find Similarity Transform Matrix \\\'cv2_trans\\\' which could be\\n-        directly used by cv2.warpAffine():\\n-            u = src_pts[:, 0]\\n-            v = src_pts[:, 1]\\n-            x = dst_pts[:, 0]\\n-            y = dst_pts[:, 1]\\n-            [x, y].T = cv_trans * [u, v, 1].T\\n-\\n-    Parameters:\\n-    ----------\\n-        @src_pts: Kx2 np.array\\n-            source points, each row is a pair of coordinates (x, y)\\n-        @dst_pts: Kx2 np.array\\n-            destination points, each row is a pair of transformed\\n-            coordinates (x, y)\\n-        reflective: True or False\\n-            if True:\\n-                use reflective similarity transform\\n-            else:\\n-                use non-reflective similarity transform\\n-\\n-    Returns:\\n-    ----------\\n-        @cv2_trans: 2x3 np.array\\n-            transform matrix from src_pts to dst_pts, could be directly used\\n-            for cv2.warpAffine()\\n-    """\\n-    trans, trans_inv = get_similarity_transform(src_pts, dst_pts, reflective)\\n-    cv2_trans = cvt_tform_mat_for_cv2(trans)\\n-\\n-    return cv2_trans\\n-\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    """\\n-    u = [0, 6, -2]\\n-    v = [0, 3, 5]\\n-    x = [-1, 0, 4]\\n-    y = [-1, -10, 4]\\n-\\n-    # In Matlab, run:\\n-    #\\n-    #   uv = [u\\\'; v\\\'];\\n-    #   xy = [x\\\'; y\\\'];\\n-    #   tform_sim=cp2tform(uv,xy,\\\'similarity\\\');\\n-    #\\n-    #   trans = tform_sim.tdata.T\\n-    #   ans =\\n-    #       -0.0764   -1.6190         0\\n-    #        1.6190   -0.0764         0\\n-    #       -3.2156    0.0290    1.0000\\n-    #   trans_inv = tform_sim.tdata.Tinv\\n-    #    ans =\\n-    #\\n-    #       -0.0291    0.6163         0\\n-    #       -0.6163   -0.0291         0\\n-    #       -0.0756    1.9826    1.0000\\n-    #    xy_m=tformfwd(tform_sim, u,v)\\n-    #\\n-    #    xy_m =\\n-    #\\n-    #       -3.2156    0.0290\\n-    #        1.1833   -9.9143\\n-    #        5.0323    2.8853\\n-    #    uv_m=tforminv(tform_sim, x,y)\\n-    #\\n-    #    uv_m =\\n-    #\\n-    #        0.5698    1.3953\\n-    #        6.0872    2.2733\\n-    #       -2.6570    4.3314\\n-    """\\n-    u = [0, 6, -2]\\n-    v = [0, 3, 5]\\n-    x = [-1, 0, 4]\\n-    y = [-1, -10, 4]\\n-\\n-    uv = np.array((u, v)).T\\n-    xy = np.array((x, y)).T\\n-\\n-    print(\\\'\\\\n--->uv:\\\')\\n-    print(uv)\\n-    print(\\\'\\\\n--->xy:\\\')\\n-    print(xy)\\n-\\n-    trans, trans_inv = get_similarity_transform(uv, xy)\\n-\\n-    print(\\\'\\\\n--->trans matrix:\\\')\\n-    print(trans)\\n-\\n-    print(\\\'\\\\n--->trans_inv matrix:\\\')\\n-    print(trans_inv)\\n-\\n-    print(\\\'\\\\n---> apply transform to uv\\\')\\n-    print(\\\'\\\\nxy_m = uv_augmented * trans\\\')\\n-    uv_aug = np.hstack((\\n-        uv, np.ones((uv.shape[0], 1))\\n-    ))\\n-    xy_m = np.dot(uv_aug, trans)\\n-    print(xy_m)\\n-\\n-    print(\\\'\\\\nxy_m = tformfwd(trans, uv)\\\')\\n-    xy_m = tformfwd(trans, uv)\\n-    print(xy_m)\\n-\\n-    print(\\\'\\\\n---> apply inverse transform to xy\\\')\\n-    print(\\\'\\\\nuv_m = xy_augmented * trans_inv\\\')\\n-    xy_aug = np.hstack((\\n-        xy, np.ones((xy.shape[0], 1))\\n-    ))\\n-    uv_m = np.dot(xy_aug, trans_inv)\\n-    print(uv_m)\\n-\\n-    print(\\\'\\\\nuv_m = tformfwd(trans_inv, xy)\\\')\\n-    uv_m = tformfwd(trans_inv, xy)\\n-    print(uv_m)\\n-\\n-    uv_m = tforminv(trans, xy)\\n-    print(\\\'\\\\nuv_m = tforminv(trans, xy)\\\')\\n-    print(uv_m)\\ndiff --git a/mtcnn_pytorch/src/visualization_utils.py b/mtcnn_pytorch/src/visualization_utils.py\\ndeleted file mode 100644\\nindex bab02be..0000000\\n--- a/mtcnn_pytorch/src/visualization_utils.py\\n+++ /dev/null\\n@@ -1,31 +0,0 @@\\n-from PIL import ImageDraw\\n-\\n-\\n-def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\\n-    """Draw bounding boxes and facial landmarks.\\n-\\n-    Arguments:\\n-        img: an instance of PIL.Image.\\n-        bounding_boxes: a float numpy array of shape [n, 5].\\n-        facial_landmarks: a float numpy array of shape [n, 10].\\n-\\n-    Returns:\\n-        an instance of PIL.Image.\\n-    """\\n-\\n-    img_copy = img.copy()\\n-    draw = ImageDraw.Draw(img_copy)\\n-\\n-    for b in bounding_boxes:\\n-        draw.rectangle([\\n-            (b[0], b[1]), (b[2], b[3])\\n-        ], outline=\\\'white\\\')\\n-\\n-    for p in facial_landmarks:\\n-        for i in range(5):\\n-            draw.ellipse([\\n-                (p[i] - 1.0, p[i + 5] - 1.0),\\n-                (p[i] + 1.0, p[i + 5] + 1.0)\\n-            ], outline=\\\'blue\\\')\\n-\\n-    return img_copy\\ndiff --git a/mtcnn_pytorch/src/weights/onet.npy b/mtcnn_pytorch/src/weights/onet.npy\\ndeleted file mode 100644\\nindex e8f63e5..0000000\\nBinary files a/mtcnn_pytorch/src/weights/onet.npy and /dev/null differ\\ndiff --git a/mtcnn_pytorch/src/weights/pnet.npy b/mtcnn_pytorch/src/weights/pnet.npy\\ndeleted file mode 100644\\nindex 91f8f9c..0000000\\nBinary files a/mtcnn_pytorch/src/weights/pnet.npy and /dev/null differ\\ndiff --git a/mtcnn_pytorch/src/weights/rnet.npy b/mtcnn_pytorch/src/weights/rnet.npy\\ndeleted file mode 100644\\nindex 5e9bbab..0000000\\nBinary files a/mtcnn_pytorch/src/weights/rnet.npy and /dev/null differ\\ndiff --git a/prepare_data.py b/prepare_data.py\\ndeleted file mode 100644\\nindex 91ab673..0000000\\n--- a/prepare_data.py\\n+++ /dev/null\\n@@ -1,17 +0,0 @@\\n-from pathlib import Path\\n-from config import get_config\\n-from data.data_pipe import load_bin, load_mx_rec\\n-import argparse\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    parser = argparse.ArgumentParser(description=\\\'for face verification\\\')\\n-    parser.add_argument("-r", "--rec_path", help="mxnet record file path",default=\\\'faces_emore\\\', type=str)\\n-    args = parser.parse_args()\\n-    conf = get_config()\\n-    rec_path = conf.data_path/args.rec_path\\n-    load_mx_rec(rec_path)\\n-    \\n-    bin_files = [\\\'agedb_30\\\', \\\'cfp_fp\\\', \\\'lfw\\\', \\\'calfw\\\', \\\'cfp_ff\\\', \\\'cplfw\\\', \\\'vgg2_fp\\\']\\n-    \\n-    for i in range(len(bin_files)):\\n-        load_bin(rec_path/(bin_files[i]+\\\'.bin\\\'), rec_path/bin_files[i], conf.test_transform)\\n\\\\ No newline at end of file\\ndiff --git a/requirements.txt b/requirements.txt\\ndeleted file mode 100644\\nindex cb619de..0000000\\nBinary files a/requirements.txt and /dev/null differ\\ndiff --git a/templates/index.html b/templates/index.html\\ndeleted file mode 100644\\nindex b54f90b..0000000\\n--- a/templates/index.html\\n+++ /dev/null\\n@@ -1,23 +0,0 @@\\n-<html>\\n-    <head>\\n-    <style>\\n-    body {\\n-        display:block;\\n-        background: url("E:\\\\Github\\\\Flask\\\\images.png"); \\n-    }\\n-    h1{\\n-        text-align: center;\\n-        text-decoration: underline;\\n-        text-transform: uppercase;\\n-    }\\n-    </style>\\n-    <title>Face Recognition</title>\\n-    </head>\\n-    <body>\\n-    \\n-    <h1 align = "center">Face Recognition</h1>\\n-    <p align ="center">\\n-    <img src="{{ url_for(\\\'video_feed\\\') }}">\\n-    </p>\\n-    </body>\\n-</html>\\n\\\\ No newline at end of file\\ndiff --git a/utils.py b/utils.py\\ndeleted file mode 100644\\nindex b50cbc8..0000000\\n--- a/utils.py\\n+++ /dev/null\\n@@ -1,156 +0,0 @@\\n-from datetime import datetime\\n-from PIL import Image\\n-import numpy as np\\n-import matplotlib.pyplot as plt\\n-plt.switch_backend(\\\'agg\\\')\\n-import io\\n-from torchvision import transforms as trans\\n-from data.data_pipe import de_preprocess\\n-import torch\\n-from model import l2_norm\\n-import pdb\\n-import cv2\\n-\\n-def separate_bn_paras(modules):\\n-    if not isinstance(modules, list):\\n-        modules = [*modules.modules()]\\n-    paras_only_bn = []\\n-    paras_wo_bn = []\\n-    for layer in modules:\\n-        if \\\'model\\\' in str(layer.__class__):\\n-            continue\\n-        if \\\'container\\\' in str(layer.__class__):\\n-            continue\\n-        else:\\n-            if \\\'batchnorm\\\' in str(layer.__class__):\\n-                paras_only_bn.extend([*layer.parameters()])\\n-            else:\\n-                paras_wo_bn.extend([*layer.parameters()])\\n-    return paras_only_bn, paras_wo_bn\\n-\\n-def prepare_facebank(conf, model, mtcnn, tta = True):\\n-    model.eval()\\n-    embeddings =  []\\n-    names = [\\\'Unknown\\\']\\n-    for path in conf.facebank_path.iterdir():\\n-        if path.is_file():\\n-            continue\\n-        else:\\n-            embs = []\\n-            for file in path.iterdir():\\n-                if not file.is_file():\\n-                    continue\\n-                else:\\n-                    try:\\n-                        img = Image.open(file)\\n-                    except:\\n-                        continue\\n-                    if img.size != (112, 112):\\n-                        img = mtcnn.align(img)\\n-                    with torch.no_grad():\\n-                        if tta:\\n-                            mirror = trans.functional.hflip(img)\\n-                            emb = model(conf.test_transform(img).to(conf.device).unsqueeze(0))\\n-                            emb_mirror = model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\\n-                            embs.append(l2_norm(emb + emb_mirror))\\n-                        else:                        \\n-                            embs.append(model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\\n-        if len(embs) == 0:\\n-            continue\\n-        embedding = torch.cat(embs).mean(0,keepdim=True)\\n-        embeddings.append(embedding)\\n-        names.append(path.name)\\n-    embeddings = torch.cat(embeddings)\\n-    names = np.array(names)\\n-    torch.save(embeddings, conf.facebank_path/\\\'facebank.pth\\\')\\n-    np.save(conf.facebank_path/\\\'names\\\', names)\\n-    return embeddings, names\\n-\\n-def load_facebank(conf):\\n-    embeddings = torch.load(conf.facebank_path/\\\'facebank.pth\\\', map_location=torch.device(\\\'cpu\\\'))\\n-    names = np.load(conf.facebank_path/\\\'names.npy\\\')\\n-    return embeddings, names\\n-\\n-def face_reader(conf, conn, flag, boxes_arr, result_arr, learner, mtcnn, targets, tta):\\n-    while True:\\n-        try:\\n-            image = conn.recv()\\n-        except:\\n-            continue\\n-        try:            \\n-            bboxes, faces = mtcnn.align_multi(image, limit=conf.face_limit)\\n-        except:\\n-            bboxes = []\\n-            \\n-        results = learner.infer(conf, faces, targets, tta)\\n-        \\n-        if len(bboxes) > 0:\\n-            print(\\\'bboxes in reader : {}\\\'.format(bboxes))\\n-            bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\\n-            bboxes = bboxes.astype(int)\\n-            bboxes = bboxes + [-1,-1,1,1] # personal choice            \\n-            assert bboxes.shape[0] == results.shape[0],\\\'bbox and faces number not same\\\'\\n-            bboxes = bboxes.reshape([-1])\\n-            for i in range(len(boxes_arr)):\\n-                if i < len(bboxes):\\n-                    boxes_arr[i] = bboxes[i]\\n-                else:\\n-                    boxes_arr[i] = 0 \\n-            for i in range(len(result_arr)):\\n-                if i < len(results):\\n-                    result_arr[i] = results[i]\\n-                else:\\n-                    result_arr[i] = -1 \\n-        else:\\n-            for i in range(len(boxes_arr)):\\n-                boxes_arr[i] = 0 # by default,it\\\'s all 0\\n-            for i in range(len(result_arr)):\\n-                result_arr[i] = -1 # by default,it\\\'s all -1\\n-        print(\\\'boxes_arr \\xef\\xbc\\x9a {}\\\'.format(boxes_arr[:4]))\\n-        print(\\\'result_arr \\xef\\xbc\\x9a {}\\\'.format(result_arr[:4]))\\n-        flag.value = 0\\n-\\n-hflip = trans.Compose([\\n-            de_preprocess,\\n-            trans.ToPILImage(),\\n-            trans.functional.hflip,\\n-            trans.ToTensor(),\\n-            trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\n-        ])\\n-\\n-def hflip_batch(imgs_tensor):\\n-    hfliped_imgs = torch.empty_like(imgs_tensor)\\n-    for i, img_ten in enumerate(imgs_tensor):\\n-        hfliped_imgs[i] = hflip(img_ten)\\n-    return hfliped_imgs\\n-\\n-def get_time():\\n-    return (str(datetime.now())[:-10]).replace(\\\' \\\',\\\'-\\\').replace(\\\':\\\',\\\'-\\\')\\n-\\n-def gen_plot(fpr, tpr):\\n-    """Create a pyplot plot and save to buffer."""\\n-    plt.figure()\\n-    plt.xlabel("FPR", fontsize=14)\\n-    plt.ylabel("TPR", fontsize=14)\\n-    plt.title("ROC Curve", fontsize=14)\\n-    plot = plt.plot(fpr, tpr, linewidth=2)\\n-    buf = io.BytesIO()\\n-    plt.savefig(buf, format=\\\'jpeg\\\')\\n-    buf.seek(0)\\n-    plt.close()\\n-    return buf\\n-\\n-def draw_box_name(bbox,name,frame):\\n-    if name == "Unknown":\\n-        frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,000),6)\\n-    else:\\n-        frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,000),6)\\n-        frame = cv2.putText(frame,\\n-                        name,\\n-                        (bbox[0],bbox[1]), \\n-                        cv2.FONT_HERSHEY_SIMPLEX, \\n-                        2,\\n-                        (0,255,0),\\n-                        3,\\n-                        cv2.LINE_AA)\\n-    return frame\\ndiff --git a/verifacation.py b/verifacation.py\\ndeleted file mode 100644\\nindex ff7d5ed..0000000\\n--- a/verifacation.py\\n+++ /dev/null\\n@@ -1,170 +0,0 @@\\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset\\n-"""\\n-\\n-# MIT License\\n-#\\n-# Copyright (c) 2016 David Sandberg\\n-#\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-#\\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-#\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-import numpy as np\\n-from sklearn.model_selection import KFold\\n-from sklearn.decomposition import PCA\\n-import sklearn\\n-from scipy import interpolate\\n-import datetime\\n-import mxnet as mx\\n-\\n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, pca=0):\\n-    assert (embeddings1.shape[0] == embeddings2.shape[0])\\n-    assert (embeddings1.shape[1] == embeddings2.shape[1])\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\n-    nrof_thresholds = len(thresholds)\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\n-\\n-    tprs = np.zeros((nrof_folds, nrof_thresholds))\\n-    fprs = np.zeros((nrof_folds, nrof_thresholds))\\n-    accuracy = np.zeros((nrof_folds))\\n-    best_thresholds = np.zeros((nrof_folds))\\n-    indices = np.arange(nrof_pairs)\\n-    # print(\\\'pca\\\', pca)\\n-\\n-    if pca == 0:\\n-        diff = np.subtract(embeddings1, embeddings2)\\n-        dist = np.sum(np.square(diff), 1)\\n-\\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\n-        # print(\\\'train_set\\\', train_set)\\n-        # print(\\\'test_set\\\', test_set)\\n-        if pca > 0:\\n-            print(\\\'doing pca on\\\', fold_idx)\\n-            embed1_train = embeddings1[train_set]\\n-            embed2_train = embeddings2[train_set]\\n-            _embed_train = np.concatenate((embed1_train, embed2_train), axis=0)\\n-            # print(_embed_train.shape)\\n-            pca_model = PCA(n_components=pca)\\n-            pca_model.fit(_embed_train)\\n-            embed1 = pca_model.transform(embeddings1)\\n-            embed2 = pca_model.transform(embeddings2)\\n-            embed1 = sklearn.preprocessing.normalize(embed1)\\n-            embed2 = sklearn.preprocessing.normalize(embed2)\\n-            # print(embed1.shape, embed2.shape)\\n-            diff = np.subtract(embed1, embed2)\\n-            dist = np.sum(np.square(diff), 1)\\n-\\n-        # Find the best threshold for the fold\\n-        acc_train = np.zeros((nrof_thresholds))\\n-        for threshold_idx, threshold in enumerate(thresholds):\\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\\n-        best_threshold_index = np.argmax(acc_train)\\n-#         print(\\\'best_threshold_index\\\', best_threshold_index, acc_train[best_threshold_index])\\n-        best_thresholds[fold_idx] = thresholds[best_threshold_index]\\n-        for threshold_idx, threshold in enumerate(thresholds):\\n-            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\\n-                                                                                                 dist[test_set],\\n-                                                                                                 actual_issame[\\n-                                                                                                     test_set])\\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set],\\n-                                                      actual_issame[test_set])\\n-\\n-    tpr = np.mean(tprs, 0)\\n-    fpr = np.mean(fprs, 0)\\n-    return tpr, fpr, accuracy, best_thresholds\\n-\\n-\\n-def calculate_accuracy(threshold, dist, actual_issame):\\n-    predict_issame = np.less(dist, threshold)\\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\\n-\\n-    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\\n-    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\\n-    acc = float(tp + tn) / dist.size\\n-    return tpr, fpr, acc\\n-\\n-\\n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):\\n-    \\\'\\\'\\\'\\n-    Copy from [insightface](https://github.com/deepinsight/insightface)\\n-    :param thresholds:\\n-    :param embeddings1:\\n-    :param embeddings2:\\n-    :param actual_issame:\\n-    :param far_target:\\n-    :param nrof_folds:\\n-    :return:\\n-    \\\'\\\'\\\'\\n-    assert (embeddings1.shape[0] == embeddings2.shape[0])\\n-    assert (embeddings1.shape[1] == embeddings2.shape[1])\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\n-    nrof_thresholds = len(thresholds)\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\n-\\n-    val = np.zeros(nrof_folds)\\n-    far = np.zeros(nrof_folds)\\n-\\n-    diff = np.subtract(embeddings1, embeddings2)\\n-    dist = np.sum(np.square(diff), 1)\\n-    indices = np.arange(nrof_pairs)\\n-\\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\n-\\n-        # Find the threshold that gives FAR = far_target\\n-        far_train = np.zeros(nrof_thresholds)\\n-        for threshold_idx, threshold in enumerate(thresholds):\\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\\n-        if np.max(far_train) >= far_target:\\n-            f = interpolate.interp1d(far_train, thresholds, kind=\\\'slinear\\\')\\n-            threshold = f(far_target)\\n-        else:\\n-            threshold = 0.0\\n-\\n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\\n-\\n-    val_mean = np.mean(val)\\n-    far_mean = np.mean(far)\\n-    val_std = np.std(val)\\n-    return val_mean, val_std, far_mean\\n-\\n-\\n-def calculate_val_far(threshold, dist, actual_issame):\\n-    predict_issame = np.less(dist, threshold)\\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\n-    n_same = np.sum(actual_issame)\\n-    n_diff = np.sum(np.logical_not(actual_issame))\\n-    val = float(true_accept) / float(n_same)\\n-    far = float(false_accept) / float(n_diff)\\n-    return val, far\\n-\\n-\\n-def evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\\n-    # Calculate evaluation metrics\\n-    thresholds = np.arange(0, 4, 0.01)\\n-    embeddings1 = embeddings[0::2]\\n-    embeddings2 = embeddings[1::2]\\n-    tpr, fpr, accuracy, best_thresholds = calculate_roc(thresholds, embeddings1, embeddings2,\\n-                                       np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)\\n-#     thresholds = np.arange(0, 4, 0.001)\\n-#     val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\\n-#                                       np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\\n-#     return tpr, fpr, accuracy, best_thresholds, val, val_std, far\\n-    return tpr, fpr, accuracy, best_thresholds\\n\\\\ No newline at end of file\\ndiff --git a/work_space/history b/work_space/history\\ndeleted file mode 100644\\nindex 30c2973..0000000\\n--- a/work_space/history\\n+++ /dev/null\\n@@ -1 +0,0 @@\\n-/home/f/learning/face_studio/history\\n\\\\ No newline at end of file\\ndiff --git a/work_space/log b/work_space/log\\ndeleted file mode 100644\\nindex 2b970ae..0000000\\n--- a/work_space/log\\n+++ /dev/null\\n@@ -1 +0,0 @@\\n-/home/f/learning/face_studio/log\\n\\\\ No newline at end of file\\ndiff --git a/work_space/model/model_ir_se50.pth b/work_space/model/model_ir_se50.pth\\ndeleted file mode 100644\\nindex d3a030d..0000000\\n--- a/work_space/model/model_ir_se50.pth\\n+++ /dev/null\\n@@ -1,3 +0,0 @@\\n-version https://git-lfs.github.com/spec/v1\\n-oid sha256:a035c768259b98ab1ce0e646312f48b9e1e218197a0f80ac6765e88f8b6ddf28\\n-size 175367323\\ndiff --git a/work_space/save/model_cpu_final.pth b/work_space/save/model_cpu_final.pth\\ndeleted file mode 100644\\nindex d3a030d..0000000\\n--- a/work_space/save/model_cpu_final.pth\\n+++ /dev/null\\n@@ -1,3 +0,0 @@\\n-version https://git-lfs.github.com/spec/v1\\n-oid sha256:a035c768259b98ab1ce0e646312f48b9e1e218197a0f80ac6765e88f8b6ddf28\\n-size 175367323\'\n\\ No newline at end of file\n+b\'diff --git a/Learner.py b/Learner.py\\nindex 5f70bc0..1e8dd8b 100644\\n--- a/Learner.py\\n+++ b/Learner.py\\n@@ -235,7 +235,7 @@ class face_learner(object):\\n         names : recorded names of faces in facebank\\n         tta : test time augmentation (hfilp, that\\\'s all)\\n         \\\'\\\'\\\'\\n-        embs = []\\n+        embs = [torch.zeros(1, 512)]\\n         for img in faces:\\n             if tta:\\n                 mirror = trans.functional.hflip(img)\\n@@ -243,10 +243,20 @@ class face_learner(object):\\n                 emb_mirror = self.model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\\n                 embs.append(l2_norm(emb + emb_mirror))\\n             else:                        \\n-                embs.append(self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\\n-        source_embs = torch.cat(embs)\\n+                emb = (self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\\n+                embs.append(emb)\\n+\\n+        source_embs = torch.cat(embs[-1:])\\n         \\n-        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1,0).unsqueeze(0)\\n+        # Move source_embs to CUDA\\n+        source_embs = source_embs.to(\\\'cuda:0\\\')\\n+\\n+        # Move target_embs to CUDA\\n+        target_embs = target_embs.to(\\\'cuda:0\\\')\\n+\\n+        # Perform the operation\\n+        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1, 0).unsqueeze(0)\\n+\\n         dist = torch.sum(torch.pow(diff, 2), dim=1)\\n         minimum, min_idx = torch.min(dist, dim=1)\\n         if float(minimum[0]) > 0.9:\\ndiff --git a/data/facebank/Joey/Joey.01.png b/data/facebank/Joey/Joey.01.png\\ndeleted file mode 100644\\nindex 3c685d0..0000000\\nBinary files a/data/facebank/Joey/Joey.01.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.02.png b/data/facebank/Joey/Joey.02.png\\ndeleted file mode 100644\\nindex 9ca5128..0000000\\nBinary files a/data/facebank/Joey/Joey.02.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.03.png b/data/facebank/Joey/Joey.03.png\\ndeleted file mode 100644\\nindex 6d2caca..0000000\\nBinary files a/data/facebank/Joey/Joey.03.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.04.png b/data/facebank/Joey/Joey.04.png\\ndeleted file mode 100644\\nindex f698017..0000000\\nBinary files a/data/facebank/Joey/Joey.04.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.05.png b/data/facebank/Joey/Joey.05.png\\ndeleted file mode 100644\\nindex 11c4831..0000000\\nBinary files a/data/facebank/Joey/Joey.05.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.06.png b/data/facebank/Joey/Joey.06.png\\ndeleted file mode 100644\\nindex 96f7ed3..0000000\\nBinary files a/data/facebank/Joey/Joey.06.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.07.png b/data/facebank/Joey/Joey.07.png\\ndeleted file mode 100644\\nindex 1e165fa..0000000\\nBinary files a/data/facebank/Joey/Joey.07.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.08.png b/data/facebank/Joey/Joey.08.png\\ndeleted file mode 100644\\nindex 09144d8..0000000\\nBinary files a/data/facebank/Joey/Joey.08.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.09.png b/data/facebank/Joey/Joey.09.png\\ndeleted file mode 100644\\nindex 7ed417e..0000000\\nBinary files a/data/facebank/Joey/Joey.09.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.11.png b/data/facebank/Joey/Joey.11.png\\ndeleted file mode 100644\\nindex 580bc53..0000000\\nBinary files a/data/facebank/Joey/Joey.11.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.12.png b/data/facebank/Joey/Joey.12.png\\ndeleted file mode 100644\\nindex f298964..0000000\\nBinary files a/data/facebank/Joey/Joey.12.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.13.png b/data/facebank/Joey/Joey.13.png\\ndeleted file mode 100644\\nindex 0498bc2..0000000\\nBinary files a/data/facebank/Joey/Joey.13.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.14.png b/data/facebank/Joey/Joey.14.png\\ndeleted file mode 100644\\nindex 54b8ddc..0000000\\nBinary files a/data/facebank/Joey/Joey.14.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.15.png b/data/facebank/Joey/Joey.15.png\\ndeleted file mode 100644\\nindex c2a4757..0000000\\nBinary files a/data/facebank/Joey/Joey.15.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.16.png b/data/facebank/Joey/Joey.16.png\\ndeleted file mode 100644\\nindex e3bc9b8..0000000\\nBinary files a/data/facebank/Joey/Joey.16.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.17.png b/data/facebank/Joey/Joey.17.png\\ndeleted file mode 100644\\nindex 4e9312e..0000000\\nBinary files a/data/facebank/Joey/Joey.17.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.18.png b/data/facebank/Joey/Joey.18.png\\ndeleted file mode 100644\\nindex 936d8a5..0000000\\nBinary files a/data/facebank/Joey/Joey.18.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.19.png b/data/facebank/Joey/Joey.19.png\\ndeleted file mode 100644\\nindex bdfb07d..0000000\\nBinary files a/data/facebank/Joey/Joey.19.png and /dev/null differ\\ndiff --git a/data/facebank/Joey/Joey.20.png b/data/facebank/Joey/Joey.20.png\\ndeleted file mode 100644\\nindex b78b6bf..0000000\\nBinary files a/data/facebank/Joey/Joey.20.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.02.png b/data/facebank/Monica/Monica.02.png\\ndeleted file mode 100644\\nindex 1a05511..0000000\\nBinary files a/data/facebank/Monica/Monica.02.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.03.png b/data/facebank/Monica/Monica.03.png\\ndeleted file mode 100644\\nindex 1f58157..0000000\\nBinary files a/data/facebank/Monica/Monica.03.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.04.png b/data/facebank/Monica/Monica.04.png\\ndeleted file mode 100644\\nindex 05a254a..0000000\\nBinary files a/data/facebank/Monica/Monica.04.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.05.png b/data/facebank/Monica/Monica.05.png\\ndeleted file mode 100644\\nindex f97be0d..0000000\\nBinary files a/data/facebank/Monica/Monica.05.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.08.png b/data/facebank/Monica/Monica.08.png\\ndeleted file mode 100644\\nindex 4ccbc47..0000000\\nBinary files a/data/facebank/Monica/Monica.08.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica.png b/data/facebank/Monica/Monica.png\\ndeleted file mode 100644\\nindex 758c3c6..0000000\\nBinary files a/data/facebank/Monica/Monica.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica10.png b/data/facebank/Monica/Monica10.png\\ndeleted file mode 100644\\nindex 8994753..0000000\\nBinary files a/data/facebank/Monica/Monica10.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica11.png b/data/facebank/Monica/Monica11.png\\ndeleted file mode 100644\\nindex 357b8bc..0000000\\nBinary files a/data/facebank/Monica/Monica11.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica12.png b/data/facebank/Monica/Monica12.png\\ndeleted file mode 100644\\nindex dc9ac29..0000000\\nBinary files a/data/facebank/Monica/Monica12.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica13.png b/data/facebank/Monica/Monica13.png\\ndeleted file mode 100644\\nindex d7c2cf6..0000000\\nBinary files a/data/facebank/Monica/Monica13.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica14.png b/data/facebank/Monica/Monica14.png\\ndeleted file mode 100644\\nindex a528414..0000000\\nBinary files a/data/facebank/Monica/Monica14.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica15.png b/data/facebank/Monica/Monica15.png\\ndeleted file mode 100644\\nindex 5bfe1a9..0000000\\nBinary files a/data/facebank/Monica/Monica15.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica16.png b/data/facebank/Monica/Monica16.png\\ndeleted file mode 100644\\nindex f94e645..0000000\\nBinary files a/data/facebank/Monica/Monica16.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica17.png b/data/facebank/Monica/Monica17.png\\ndeleted file mode 100644\\nindex bf24311..0000000\\nBinary files a/data/facebank/Monica/Monica17.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica18.png b/data/facebank/Monica/Monica18.png\\ndeleted file mode 100644\\nindex b04f261..0000000\\nBinary files a/data/facebank/Monica/Monica18.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica19.png b/data/facebank/Monica/Monica19.png\\ndeleted file mode 100644\\nindex 0eeb0d6..0000000\\nBinary files a/data/facebank/Monica/Monica19.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica20.png b/data/facebank/Monica/Monica20.png\\ndeleted file mode 100644\\nindex e5d4e24..0000000\\nBinary files a/data/facebank/Monica/Monica20.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica6.png b/data/facebank/Monica/Monica6.png\\ndeleted file mode 100644\\nindex 7112741..0000000\\nBinary files a/data/facebank/Monica/Monica6.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica7.png b/data/facebank/Monica/Monica7.png\\ndeleted file mode 100644\\nindex 3567770..0000000\\nBinary files a/data/facebank/Monica/Monica7.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica8.png b/data/facebank/Monica/Monica8.png\\ndeleted file mode 100644\\nindex de723c2..0000000\\nBinary files a/data/facebank/Monica/Monica8.png and /dev/null differ\\ndiff --git a/data/facebank/Monica/Monica9.png b/data/facebank/Monica/Monica9.png\\ndeleted file mode 100644\\nindex c7ae768..0000000\\nBinary files a/data/facebank/Monica/Monica9.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe06.png b/data/facebank/Phoebe/Pheobe06.png\\ndeleted file mode 100644\\nindex 3270a78..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe06.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe07.png b/data/facebank/Phoebe/Pheobe07.png\\ndeleted file mode 100644\\nindex 25cb4fd..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe07.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe08.png b/data/facebank/Phoebe/Pheobe08.png\\ndeleted file mode 100644\\nindex cd5d7e0..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe08.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe09.png b/data/facebank/Phoebe/Pheobe09.png\\ndeleted file mode 100644\\nindex 4df4c47..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe09.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe10.png b/data/facebank/Phoebe/Pheobe10.png\\ndeleted file mode 100644\\nindex 8637633..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe10.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe11.png b/data/facebank/Phoebe/Pheobe11.png\\ndeleted file mode 100644\\nindex 7ad6ed9..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe11.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe12.png b/data/facebank/Phoebe/Pheobe12.png\\ndeleted file mode 100644\\nindex 7ce09dc..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe12.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe13.png b/data/facebank/Phoebe/Pheobe13.png\\ndeleted file mode 100644\\nindex 17b6c11..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe13.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe15.png b/data/facebank/Phoebe/Pheobe15.png\\ndeleted file mode 100644\\nindex 6c47f10..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe15.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe16.png b/data/facebank/Phoebe/Pheobe16.png\\ndeleted file mode 100644\\nindex 56775ea..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe16.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe17.png b/data/facebank/Phoebe/Pheobe17.png\\ndeleted file mode 100644\\nindex 0897eb9..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe17.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe18.png b/data/facebank/Phoebe/Pheobe18.png\\ndeleted file mode 100644\\nindex 2579394..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe18.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe19.png b/data/facebank/Phoebe/Pheobe19.png\\ndeleted file mode 100644\\nindex 111693b..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe19.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Pheobe20.png b/data/facebank/Phoebe/Pheobe20.png\\ndeleted file mode 100644\\nindex 82ecd08..0000000\\nBinary files a/data/facebank/Phoebe/Pheobe20.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.02.png b/data/facebank/Phoebe/Phoebe.02.png\\ndeleted file mode 100644\\nindex 5a67bbc..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.02.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.03.png b/data/facebank/Phoebe/Phoebe.03.png\\ndeleted file mode 100644\\nindex 43a1ae2..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.03.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.04.png b/data/facebank/Phoebe/Phoebe.04.png\\ndeleted file mode 100644\\nindex e221b0d..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.04.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.05.png b/data/facebank/Phoebe/Phoebe.05.png\\ndeleted file mode 100644\\nindex 96a6629..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.05.png and /dev/null differ\\ndiff --git a/data/facebank/Phoebe/Phoebe.png b/data/facebank/Phoebe/Phoebe.png\\ndeleted file mode 100644\\nindex befbda6..0000000\\nBinary files a/data/facebank/Phoebe/Phoebe.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.02.png b/data/facebank/Rachel/Rachel.02.png\\ndeleted file mode 100644\\nindex e62a99a..0000000\\nBinary files a/data/facebank/Rachel/Rachel.02.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.03.png b/data/facebank/Rachel/Rachel.03.png\\ndeleted file mode 100644\\nindex f4ac8fa..0000000\\nBinary files a/data/facebank/Rachel/Rachel.03.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.04.png b/data/facebank/Rachel/Rachel.04.png\\ndeleted file mode 100644\\nindex 1ff0d0c..0000000\\nBinary files a/data/facebank/Rachel/Rachel.04.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel.05.png b/data/facebank/Rachel/Rachel.05.png\\ndeleted file mode 100644\\nindex 2d69283..0000000\\nBinary files a/data/facebank/Rachel/Rachel.05.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel01.png b/data/facebank/Rachel/Rachel01.png\\ndeleted file mode 100644\\nindex 91e0fd9..0000000\\nBinary files a/data/facebank/Rachel/Rachel01.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel06.png b/data/facebank/Rachel/Rachel06.png\\ndeleted file mode 100644\\nindex be80ce0..0000000\\nBinary files a/data/facebank/Rachel/Rachel06.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel07.png b/data/facebank/Rachel/Rachel07.png\\ndeleted file mode 100644\\nindex 7f1c0d8..0000000\\nBinary files a/data/facebank/Rachel/Rachel07.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel08.png b/data/facebank/Rachel/Rachel08.png\\ndeleted file mode 100644\\nindex 2295162..0000000\\nBinary files a/data/facebank/Rachel/Rachel08.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel09.png b/data/facebank/Rachel/Rachel09.png\\ndeleted file mode 100644\\nindex 49dfe08..0000000\\nBinary files a/data/facebank/Rachel/Rachel09.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel10.png b/data/facebank/Rachel/Rachel10.png\\ndeleted file mode 100644\\nindex bae25a1..0000000\\nBinary files a/data/facebank/Rachel/Rachel10.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel11.png b/data/facebank/Rachel/Rachel11.png\\ndeleted file mode 100644\\nindex 0e39cef..0000000\\nBinary files a/data/facebank/Rachel/Rachel11.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel12.png b/data/facebank/Rachel/Rachel12.png\\ndeleted file mode 100644\\nindex d0945d5..0000000\\nBinary files a/data/facebank/Rachel/Rachel12.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel13.png b/data/facebank/Rachel/Rachel13.png\\ndeleted file mode 100644\\nindex 9a942f2..0000000\\nBinary files a/data/facebank/Rachel/Rachel13.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel14.png b/data/facebank/Rachel/Rachel14.png\\ndeleted file mode 100644\\nindex 3786268..0000000\\nBinary files a/data/facebank/Rachel/Rachel14.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel15.png b/data/facebank/Rachel/Rachel15.png\\ndeleted file mode 100644\\nindex c8b5b67..0000000\\nBinary files a/data/facebank/Rachel/Rachel15.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel16.png b/data/facebank/Rachel/Rachel16.png\\ndeleted file mode 100644\\nindex 8fa13b5..0000000\\nBinary files a/data/facebank/Rachel/Rachel16.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel17.png b/data/facebank/Rachel/Rachel17.png\\ndeleted file mode 100644\\nindex dcd563d..0000000\\nBinary files a/data/facebank/Rachel/Rachel17.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel18.png b/data/facebank/Rachel/Rachel18.png\\ndeleted file mode 100644\\nindex 5d58bc1..0000000\\nBinary files a/data/facebank/Rachel/Rachel18.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel19.png b/data/facebank/Rachel/Rachel19.png\\ndeleted file mode 100644\\nindex 5f9778a..0000000\\nBinary files a/data/facebank/Rachel/Rachel19.png and /dev/null differ\\ndiff --git a/data/facebank/Rachel/Rachel20.png b/data/facebank/Rachel/Rachel20.png\\ndeleted file mode 100644\\nindex 7d62102..0000000\\nBinary files a/data/facebank/Rachel/Rachel20.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/Ross03.png b/data/facebank/Ross/Ross03.png\\ndeleted file mode 100644\\nindex d8c2c7d..0000000\\nBinary files a/data/facebank/Ross/Ross03.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/Ross04.png b/data/facebank/Ross/Ross04.png\\ndeleted file mode 100644\\nindex b3ccc1b..0000000\\nBinary files a/data/facebank/Ross/Ross04.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/Ross05.png b/data/facebank/Ross/Ross05.png\\ndeleted file mode 100644\\nindex 95fda3f..0000000\\nBinary files a/data/facebank/Ross/Ross05.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross01.png b/data/facebank/Ross/ross01.png\\ndeleted file mode 100644\\nindex 3459b69..0000000\\nBinary files a/data/facebank/Ross/ross01.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross02.png b/data/facebank/Ross/ross02.png\\ndeleted file mode 100644\\nindex 332c896..0000000\\nBinary files a/data/facebank/Ross/ross02.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross06.png b/data/facebank/Ross/ross06.png\\ndeleted file mode 100644\\nindex 8cc4528..0000000\\nBinary files a/data/facebank/Ross/ross06.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross07.png b/data/facebank/Ross/ross07.png\\ndeleted file mode 100644\\nindex f26cf1d..0000000\\nBinary files a/data/facebank/Ross/ross07.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross08.png b/data/facebank/Ross/ross08.png\\ndeleted file mode 100644\\nindex 76b0a88..0000000\\nBinary files a/data/facebank/Ross/ross08.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross09.png b/data/facebank/Ross/ross09.png\\ndeleted file mode 100644\\nindex 50d07c8..0000000\\nBinary files a/data/facebank/Ross/ross09.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross10.png b/data/facebank/Ross/ross10.png\\ndeleted file mode 100644\\nindex 64651c2..0000000\\nBinary files a/data/facebank/Ross/ross10.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross11.png b/data/facebank/Ross/ross11.png\\ndeleted file mode 100644\\nindex bfc016b..0000000\\nBinary files a/data/facebank/Ross/ross11.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross12.png b/data/facebank/Ross/ross12.png\\ndeleted file mode 100644\\nindex d65cc34..0000000\\nBinary files a/data/facebank/Ross/ross12.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross13.png b/data/facebank/Ross/ross13.png\\ndeleted file mode 100644\\nindex 2671298..0000000\\nBinary files a/data/facebank/Ross/ross13.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross14.png b/data/facebank/Ross/ross14.png\\ndeleted file mode 100644\\nindex c239eea..0000000\\nBinary files a/data/facebank/Ross/ross14.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross15.png b/data/facebank/Ross/ross15.png\\ndeleted file mode 100644\\nindex 7326319..0000000\\nBinary files a/data/facebank/Ross/ross15.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross16.png b/data/facebank/Ross/ross16.png\\ndeleted file mode 100644\\nindex 7786cdd..0000000\\nBinary files a/data/facebank/Ross/ross16.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross17.png b/data/facebank/Ross/ross17.png\\ndeleted file mode 100644\\nindex 0c70f90..0000000\\nBinary files a/data/facebank/Ross/ross17.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross18.png b/data/facebank/Ross/ross18.png\\ndeleted file mode 100644\\nindex 06d916d..0000000\\nBinary files a/data/facebank/Ross/ross18.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross19.png b/data/facebank/Ross/ross19.png\\ndeleted file mode 100644\\nindex 8f79393..0000000\\nBinary files a/data/facebank/Ross/ross19.png and /dev/null differ\\ndiff --git a/data/facebank/Ross/ross20.png b/data/facebank/Ross/ross20.png\\ndeleted file mode 100644\\nindex 6621203..0000000\\nBinary files a/data/facebank/Ross/ross20.png and /dev/null differ\\ndiff --git a/data/facebank/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png b/data/facebank/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png\\ndeleted file mode 100644\\nindex 12d01cb..0000000\\nBinary files a/data/facebank/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png and /dev/null differ\\ndiff --git a/data/facebank/facebank.pth b/data/facebank/facebank.pth\\nindex 5fdbbbc..847e0e9 100644\\n--- a/data/facebank/facebank.pth\\n+++ b/data/facebank/facebank.pth\\n@@ -1,3 +1,3 @@\\n version https://git-lfs.github.com/spec/v1\\n-oid sha256:01da0a10ca9fae085c25f535aed1241340a3bad79a49c1f2ec28b07aaa3307d0\\n-size 15083\\n+oid sha256:90508e947263ababfecb4ce38fcecdf02d25732da1b799abb2d503d38a55f6ba\\n+size 22867\\ndiff --git a/data/facebank/names.npy b/data/facebank/names.npy\\nindex 108dbb2..83a122b 100644\\nBinary files a/data/facebank/names.npy and b/data/facebank/names.npy differ\\ndiff --git a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png\\ndeleted file mode 100644\\nindex 14720ab..0000000\\nBinary files a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png\\ndeleted file mode 100644\\nindex 52d5e06..0000000\\nBinary files a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png and /dev/null differ\\ndiff --git a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.14 PM.png b/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.14 PM.png\\ndeleted file mode 100644\\nindex e62c53b..0000000\\nBinary files a/data/processed/Akshay/WhatsApp Image 2023-02-23 at 5.12.14 PM.png and /dev/null differ\\ndiff --git a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png\\ndeleted file mode 100644\\nindex 3c00e1d..0000000\\nBinary files a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM.png b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM.png\\ndeleted file mode 100644\\nindex 7ad0e0a..0000000\\nBinary files a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.10 PM.png and /dev/null differ\\ndiff --git a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.11 PM.png b/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.11 PM.png\\ndeleted file mode 100644\\nindex 2a06492..0000000\\nBinary files a/data/processed/Ananya/WhatsApp Image 2023-02-23 at 5.12.11 PM.png and /dev/null differ\\ndiff --git a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png b/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png\\ndeleted file mode 100644\\nindex d10cd02..0000000\\nBinary files a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png b/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png\\ndeleted file mode 100644\\nindex 597fb32..0000000\\nBinary files a/data/processed/Bhavya/WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png and /dev/null differ\\ndiff --git a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.06 PM.png b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.06 PM.png\\ndeleted file mode 100644\\nindex c17575a..0000000\\nBinary files a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.06 PM.png and /dev/null differ\\ndiff --git a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png\\ndeleted file mode 100644\\nindex d6503c1..0000000\\nBinary files a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM.png b/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM.png\\ndeleted file mode 100644\\nindex 81e4f6c..0000000\\nBinary files a/data/processed/Golchi/WhatsApp Image 2023-02-14 at 5.05.07 PM.png and /dev/null differ\\ndiff --git a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png\\ndeleted file mode 100644\\nindex bb1b1d2..0000000\\nBinary files a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png\\ndeleted file mode 100644\\nindex 4bf224f..0000000\\nBinary files a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png and /dev/null differ\\ndiff --git a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM.png b/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM.png\\ndeleted file mode 100644\\nindex 3361266..0000000\\nBinary files a/data/processed/Jain/WhatsApp Image 2023-02-14 at 5.05.08 PM.png and /dev/null differ\\ndiff --git a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png\\ndeleted file mode 100644\\nindex d48b5cb..0000000\\nBinary files a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png\\ndeleted file mode 100644\\nindex 58b7ccc..0000000\\nBinary files a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png and /dev/null differ\\ndiff --git a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM.png b/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM.png\\ndeleted file mode 100644\\nindex 6066aaa..0000000\\nBinary files a/data/processed/Saksham/WhatsApp Image 2023-02-23 at 5.12.09 PM.png and /dev/null differ\\ndiff --git a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png\\ndeleted file mode 100644\\nindex ef2e857..0000000\\nBinary files a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM.png b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM.png\\ndeleted file mode 100644\\nindex 45c8571..0000000\\nBinary files a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.12 PM.png and /dev/null differ\\ndiff --git a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.13 PM.png b/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.13 PM.png\\ndeleted file mode 100644\\nindex f7f3f63..0000000\\nBinary files a/data/processed/Sandeep/WhatsApp Image 2023-02-23 at 5.12.13 PM.png and /dev/null differ\\ndiff --git a/data/processed/Suhail/WhatsApp Image 2023-02-23 at 5.12.15 PM.png b/data/processed/Suhail/WhatsApp Image 2023-02-23 at 5.12.15 PM.png\\ndeleted file mode 100644\\nindex d9af0a2..0000000\\nBinary files a/data/processed/Suhail/WhatsApp Image 2023-02-23 at 5.12.15 PM.png and /dev/null differ\\ndiff --git a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png b/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png\\ndeleted file mode 100644\\nindex 12d01cb..0000000\\nBinary files a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png and /dev/null differ\\ndiff --git a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM.png b/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM.png\\ndeleted file mode 100644\\nindex a418fd1..0000000\\nBinary files a/data/processed/Utkarsh/WhatsApp Image 2023-02-23 at 5.08.13 PM.png and /dev/null differ\\ndiff --git a/data/processed/bounding_boxes_50704.txt b/data/processed/bounding_boxes_50704.txt\\ndeleted file mode 100644\\nindex 65b6e95..0000000\\n--- a/data/processed/bounding_boxes_50704.txt\\n+++ /dev/null\\n@@ -1,23 +0,0 @@\\n-data/processed\\\\Akshay\\\\WhatsApp Image 2023-02-23 at 5.12.13 PM (1).png 208 309 533 747\\n-data/processed\\\\Akshay\\\\WhatsApp Image 2023-02-23 at 5.12.13 PM (2).png 326 328 541 621\\n-data/processed\\\\Akshay\\\\WhatsApp Image 2023-02-23 at 5.12.14 PM.png 195 438 434 733\\n-data/processed\\\\Ananya\\\\WhatsApp Image 2023-02-23 at 5.12.10 PM (1).png 401 430 682 835\\n-data/processed\\\\Ananya\\\\WhatsApp Image 2023-02-23 at 5.12.10 PM.png 441 590 726 963\\n-data/processed\\\\Ananya\\\\WhatsApp Image 2023-02-23 at 5.12.11 PM.png 477 511 795 920\\n-data/processed\\\\Bhavya\\\\WhatsApp Image 2023-02-23 at 5.12.11 PM (1).png 431 479 664 764\\n-data/processed\\\\Bhavya\\\\WhatsApp Image 2023-02-23 at 5.12.11 PM (2).png 434 409 756 801\\n-data/processed\\\\Golchi\\\\WhatsApp Image 2023-02-14 at 5.05.06 PM.png 359 250 706 730\\n-data/processed\\\\Golchi\\\\WhatsApp Image 2023-02-14 at 5.05.07 PM (1).png 305 196 634 616\\n-data/processed\\\\Golchi\\\\WhatsApp Image 2023-02-14 at 5.05.07 PM.png 296 261 647 735\\n-data/processed\\\\Jain\\\\WhatsApp Image 2023-02-14 at 5.05.08 PM (1).png 308 269 604 653\\n-data/processed\\\\Jain\\\\WhatsApp Image 2023-02-14 at 5.05.08 PM (2).png 314 305 627 692\\n-data/processed\\\\Jain\\\\WhatsApp Image 2023-02-14 at 5.05.08 PM.png 252 248 632 738\\n-data/processed\\\\Saksham\\\\WhatsApp Image 2023-02-23 at 5.12.09 PM (1).png 8 355 643 1106\\n-data/processed\\\\Saksham\\\\WhatsApp Image 2023-02-23 at 5.12.09 PM (2).png 239 280 860 1040\\n-data/processed\\\\Saksham\\\\WhatsApp Image 2023-02-23 at 5.12.09 PM.png 561 392 1105 1085\\n-data/processed\\\\Sandeep\\\\WhatsApp Image 2023-02-23 at 5.12.12 PM (1).png 479 480 730 813\\n-data/processed\\\\Sandeep\\\\WhatsApp Image 2023-02-23 at 5.12.12 PM.png 420 404 715 755\\n-data/processed\\\\Sandeep\\\\WhatsApp Image 2023-02-23 at 5.12.13 PM.png 422 409 727 766\\n-data/processed\\\\Suhail\\\\WhatsApp Image 2023-02-23 at 5.12.15 PM.png 248 108 484 411\\n-data/processed\\\\Utkarsh\\\\WhatsApp Image 2023-02-23 at 5.08.13 PM (1).png 581 514 658 596\\n-data/processed\\\\Utkarsh\\\\WhatsApp Image 2023-02-23 at 5.08.13 PM.png 391 116 673 477\\ndiff --git a/data/processed/revision_info.txt b/data/processed/revision_info.txt\\ndeleted file mode 100644\\nindex ed99a36..0000000\\n--- a/data/processed/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: .\\\\create-dataset\\\\align_dataset_mtcnn.py data/raw/ data/processed --image_size 112\\n---------------------\\n-tensorflow version: 1.15.0\\n---------------------\\n-git hash: b\\\'1769caaa4169f2bdae17e269e1348cb594357fb3\\\'\\n---------------------\\n-b\\\'diff --git a/.gitattributes b/.gitattributes\\\\ndeleted file mode 100644\\\\nindex ec4a626..0000000\\\\n--- a/.gitattributes\\\\n+++ /dev/null\\\\n@@ -1 +0,0 @@\\\\n-*.pth filter=lfs diff=lfs merge=lfs -text\\\\ndiff --git a/.gitignore b/.gitignore\\\\ndeleted file mode 100644\\\\nindex 894a44c..0000000\\\\n--- a/.gitignore\\\\n+++ /dev/null\\\\n@@ -1,104 +0,0 @@\\\\n-# Byte-compiled / optimized / DLL files\\\\n-__pycache__/\\\\n-*.py[cod]\\\\n-*$py.class\\\\n-\\\\n-# C extensions\\\\n-*.so\\\\n-\\\\n-# Distribution / packaging\\\\n-.Python\\\\n-build/\\\\n-develop-eggs/\\\\n-dist/\\\\n-downloads/\\\\n-eggs/\\\\n-.eggs/\\\\n-lib/\\\\n-lib64/\\\\n-parts/\\\\n-sdist/\\\\n-var/\\\\n-wheels/\\\\n-*.egg-info/\\\\n-.installed.cfg\\\\n-*.egg\\\\n-MANIFEST\\\\n-\\\\n-# PyInstaller\\\\n-#  Usually these files are written by a python script from a template\\\\n-#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\\\n-*.manifest\\\\n-*.spec\\\\n-\\\\n-# Installer logs\\\\n-pip-log.txt\\\\n-pip-delete-this-directory.txt\\\\n-\\\\n-# Unit test / coverage reports\\\\n-htmlcov/\\\\n-.tox/\\\\n-.coverage\\\\n-.coverage.*\\\\n-.cache\\\\n-nosetests.xml\\\\n-coverage.xml\\\\n-*.cover\\\\n-.hypothesis/\\\\n-.pytest_cache/\\\\n-\\\\n-# Translations\\\\n-*.mo\\\\n-*.pot\\\\n-\\\\n-# Django stuff:\\\\n-*.log\\\\n-local_settings.py\\\\n-db.sqlite3\\\\n-\\\\n-# Flask stuff:\\\\n-instance/\\\\n-.webassets-cache\\\\n-\\\\n-# Scrapy stuff:\\\\n-.scrapy\\\\n-\\\\n-# Sphinx documentation\\\\n-docs/_build/\\\\n-\\\\n-# PyBuilder\\\\n-target/\\\\n-\\\\n-# Jupyter Notebook\\\\n-.ipynb_checkpoints\\\\n-\\\\n-# pyenv\\\\n-.python-version\\\\n-\\\\n-# celery beat schedule file\\\\n-celerybeat-schedule\\\\n-\\\\n-# SageMath parsed files\\\\n-*.sage.py\\\\n-\\\\n-# Environments\\\\n-.env\\\\n-.venv\\\\n-env/\\\\n-venv/\\\\n-ENV/\\\\n-env.bak/\\\\n-venv.bak/\\\\n-\\\\n-# Spyder project settings\\\\n-.spyderproject\\\\n-.spyproject\\\\n-\\\\n-# Rope project settings\\\\n-.ropeproject\\\\n-\\\\n-# mkdocs documentation\\\\n-/site\\\\n-\\\\n-# mypy\\\\n-.mypy_cache/\\\\ndiff --git a/Learner.py b/Learner.py\\\\ndeleted file mode 100644\\\\nindex 5f70bc0..0000000\\\\n--- a/Learner.py\\\\n+++ /dev/null\\\\n@@ -1,254 +0,0 @@\\\\n-from data.data_pipe import de_preprocess, get_train_loader, get_val_data\\\\n-from model import Backbone, Arcface, MobileFaceNet, Am_softmax, l2_norm\\\\n-from verifacation import evaluate\\\\n-import torch\\\\n-from torch import optim\\\\n-import numpy as np\\\\n-from tqdm import tqdm\\\\n-from tensorboardX import SummaryWriter\\\\n-from matplotlib import pyplot as plt\\\\n-plt.switch_backend(\\\\\\\'agg\\\\\\\')\\\\n-from utils import get_time, gen_plot, hflip_batch, separate_bn_paras\\\\n-from PIL import Image\\\\n-from torchvision import transforms as trans\\\\n-import math\\\\n-import config\\\\n-\\\\n-class face_learner(object):\\\\n-    def __init__(self, conf, inference=False):\\\\n-        print(conf)\\\\n-        if conf.use_mobilfacenet:\\\\n-            self.model = MobileFaceNet(conf.embedding_size).to(conf.device)\\\\n-            print(\\\\\\\'MobileFaceNet model generated\\\\\\\')\\\\n-        else:\\\\n-            self.model = Backbone(conf.net_depth, conf.drop_ratio, conf.net_mode).to(conf.device)\\\\n-            print(\\\\\\\'{}_{} model opened\\\\\\\'.format(conf.net_mode, conf.net_depth))\\\\n-        \\\\n-        if not inference:\\\\n-            self.milestones = conf.milestones\\\\n-            self.loader, self.class_num = get_train_loader(conf)        \\\\n-\\\\n-            self.writer = SummaryWriter(conf.log_path)\\\\n-            self.step = 0\\\\n-            self.head = Arcface(embedding_size=conf.embedding_size, classnum=self.class_num).to(conf.device)\\\\n-\\\\n-            print(\\\\\\\'two model heads generated\\\\\\\')\\\\n-\\\\n-            paras_only_bn, paras_wo_bn = separate_bn_paras(self.model)\\\\n-            \\\\n-            if conf.use_mobilfacenet:\\\\n-                self.optimizer = optim.SGD([\\\\n-                                    {\\\\\\\'params\\\\\\\': paras_wo_bn[:-1], \\\\\\\'weight_decay\\\\\\\': 4e-5},\\\\n-                                    {\\\\\\\'params\\\\\\\': [paras_wo_bn[-1]] + [self.head.kernel], \\\\\\\'weight_decay\\\\\\\': 4e-4},\\\\n-                                    {\\\\\\\'params\\\\\\\': paras_only_bn}\\\\n-                                ], lr = conf.lr, momentum = conf.momentum)\\\\n-            else:\\\\n-                self.optimizer = optim.SGD([\\\\n-                                    {\\\\\\\'params\\\\\\\': paras_wo_bn + [self.head.kernel], \\\\\\\'weight_decay\\\\\\\': 5e-4},\\\\n-                                    {\\\\\\\'params\\\\\\\': paras_only_bn}\\\\n-                                ], lr = conf.lr, momentum = conf.momentum)\\\\n-            print(self.optimizer)\\\\n-#             self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=40, verbose=True)\\\\n-\\\\n-            print(\\\\\\\'optimizers generated\\\\\\\')    \\\\n-            self.board_loss_every = len(self.loader)//100\\\\n-            self.evaluate_every = len(self.loader)//10\\\\n-            self.save_every = len(self.loader)//5\\\\n-            self.agedb_30, self.cfp_fp, self.lfw, self.agedb_30_issame, self.cfp_fp_issame, self.lfw_issame = get_val_data(self.loader.dataset.root.parent)\\\\n-        else:\\\\n-            self.threshold = conf.threshold\\\\n-    \\\\n-    def save_state(self, conf, accuracy, to_save_folder=False, extra=None, model_only=False):\\\\n-        if to_save_folder:\\\\n-            save_path = conf.save_path\\\\n-        else:\\\\n-            save_path = conf.model_path\\\\n-        torch.save(\\\\n-            self.model.state_dict(), save_path /\\\\n-            (\\\\\\\'model_{}_accuracy:{}_step:{}_{}.pth\\\\\\\'.format(get_time(), accuracy, self.step, extra)))\\\\n-        if not model_only:\\\\n-            torch.save(\\\\n-                self.head.state_dict(), save_path /\\\\n-                (\\\\\\\'head_{}_accuracy:{}_step:{}_{}.pth\\\\\\\'.format(get_time(), accuracy, self.step, extra)))\\\\n-            torch.save(\\\\n-                self.optimizer.state_dict(), save_path /\\\\n-                (\\\\\\\'optimizer_{}_accuracy:{}_step:{}_{}.pth\\\\\\\'.format(get_time(), accuracy, self.step, extra)))\\\\n-    \\\\n-    def load_state(self, conf, fixed_str, from_save_folder=False, model_only=False):\\\\n-        if from_save_folder:\\\\n-            save_path = conf.save_path\\\\n-        else:\\\\n-            save_path = conf.model_path            \\\\n-        self.model.load_state_dict(torch.load(save_path/\\\\\\\'model_{}\\\\\\\'.format(fixed_str), map_location=torch.device(\\\\\\\'cpu\\\\\\\')))\\\\n-        if not model_only:\\\\n-            self.head.load_state_dict(torch.load(save_path/\\\\\\\'head_{}\\\\\\\'.format(fixed_str), map_location=torch.device(\\\\\\\'cpu\\\\\\\')))\\\\n-            self.optimizer.load_state_dict(torch.load(save_path/\\\\\\\'optimizer_{}\\\\\\\'.format(fixed_str), map_location=torch.device(\\\\\\\'cpu\\\\\\\')))\\\\n-        \\\\n-    def board_val(self, db_name, accuracy, best_threshold, roc_curve_tensor):\\\\n-        self.writer.add_scalar(\\\\\\\'{}_accuracy\\\\\\\'.format(db_name), accuracy, self.step)\\\\n-        self.writer.add_scalar(\\\\\\\'{}_best_threshold\\\\\\\'.format(db_name), best_threshold, self.step)\\\\n-        self.writer.add_image(\\\\\\\'{}_roc_curve\\\\\\\'.format(db_name), roc_curve_tensor, self.step)\\\\n-#         self.writer.add_scalar(\\\\\\\'{}_val:true accept ratio\\\\\\\'.format(db_name), val, self.step)\\\\n-#         self.writer.add_scalar(\\\\\\\'{}_val_std\\\\\\\'.format(db_name), val_std, self.step)\\\\n-#         self.writer.add_scalar(\\\\\\\'{}_far:False Acceptance Ratio\\\\\\\'.format(db_name), far, self.step)\\\\n-        \\\\n-    def evaluate(self, conf, carray, issame, nrof_folds = 5, tta = False):\\\\n-        self.model.eval()\\\\n-        idx = 0\\\\n-        embeddings = np.zeros([len(carray), conf.embedding_size])\\\\n-        with torch.no_grad():\\\\n-            while idx + conf.batch_size <= len(carray):\\\\n-                batch = torch.tensor(carray[idx:idx + conf.batch_size])\\\\n-                if tta:\\\\n-                    fliped = hflip_batch(batch)\\\\n-                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\\\\n-                    embeddings[idx:idx + conf.batch_size] = l2_norm(emb_batch)\\\\n-                else:\\\\n-                    embeddings[idx:idx + conf.batch_size] = self.model(batch.to(conf.device)).cpu()\\\\n-                idx += conf.batch_size\\\\n-            if idx < len(carray):\\\\n-                batch = torch.tensor(carray[idx:])            \\\\n-                if tta:\\\\n-                    fliped = hflip_batch(batch)\\\\n-                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\\\\n-                    embeddings[idx:] = l2_norm(emb_batch)\\\\n-                else:\\\\n-                    embeddings[idx:] = self.model(batch.to(conf.device)).cpu()\\\\n-        tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\\\\n-        buf = gen_plot(fpr, tpr)\\\\n-        roc_curve = Image.open(buf)\\\\n-        roc_curve_tensor = trans.ToTensor()(roc_curve)\\\\n-        return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\\\\n-    \\\\n-    def find_lr(self,\\\\n-                conf,\\\\n-                init_value=1e-8,\\\\n-                final_value=10.,\\\\n-                beta=0.98,\\\\n-                bloding_scale=3.,\\\\n-                num=None):\\\\n-        if not num:\\\\n-            num = len(self.loader)\\\\n-        mult = (final_value / init_value)**(1 / num)\\\\n-        lr = init_value\\\\n-        for params in self.optimizer.param_groups:\\\\n-            params[\\\\\\\'lr\\\\\\\'] = lr\\\\n-        self.model.train()\\\\n-        avg_loss = 0.\\\\n-        best_loss = 0.\\\\n-        batch_num = 0\\\\n-        losses = []\\\\n-        log_lrs = []\\\\n-        for i, (imgs, labels) in tqdm(enumerate(self.loader), total=num):\\\\n-\\\\n-            imgs = imgs.to(conf.device)\\\\n-            labels = labels.to(conf.device)\\\\n-            batch_num += 1          \\\\n-\\\\n-            self.optimizer.zero_grad()\\\\n-\\\\n-            embeddings = self.model(imgs)\\\\n-            thetas = self.head(embeddings, labels)\\\\n-            loss = conf.ce_loss(thetas, labels)          \\\\n-    \\\\n-            #Compute the smoothed loss\\\\n-            avg_loss = beta * avg_loss + (1 - beta) * loss.item()\\\\n-            self.writer.add_scalar(\\\\\\\'avg_loss\\\\\\\', avg_loss, batch_num)\\\\n-            smoothed_loss = avg_loss / (1 - beta**batch_num)\\\\n-            self.writer.add_scalar(\\\\\\\'smoothed_loss\\\\\\\', smoothed_loss,batch_num)\\\\n-            #Stop if the loss is exploding\\\\n-            if batch_num > 1 and smoothed_loss > bloding_scale * best_loss:\\\\n-                print(\\\\\\\'exited with best_loss at {}\\\\\\\'.format(best_loss))\\\\n-                plt.plot(log_lrs[10:-5], losses[10:-5])\\\\n-                return log_lrs, losses\\\\n-            #Record the best loss\\\\n-            if smoothed_loss < best_loss or batch_num == 1:\\\\n-                best_loss = smoothed_loss\\\\n-            #Store the values\\\\n-            losses.append(smoothed_loss)\\\\n-            log_lrs.append(math.log10(lr))\\\\n-            self.writer.add_scalar(\\\\\\\'log_lr\\\\\\\', math.log10(lr), batch_num)\\\\n-            #Do the SGD step\\\\n-            #Update the lr for the next step\\\\n-\\\\n-            loss.backward()\\\\n-            self.optimizer.step()\\\\n-\\\\n-            lr *= mult\\\\n-            for params in self.optimizer.param_groups:\\\\n-                params[\\\\\\\'lr\\\\\\\'] = lr\\\\n-            if batch_num > num:\\\\n-                plt.plot(log_lrs[10:-5], losses[10:-5])\\\\n-                return log_lrs, losses    \\\\n-\\\\n-    def train(self, conf, epochs):\\\\n-        self.model.train()\\\\n-        running_loss = 0.            \\\\n-        for e in range(epochs):\\\\n-            print(\\\\\\\'epoch {} started\\\\\\\'.format(e))\\\\n-            if e == self.milestones[0]:\\\\n-                self.schedule_lr()\\\\n-            if e == self.milestones[1]:\\\\n-                self.schedule_lr()      \\\\n-            if e == self.milestones[2]:\\\\n-                self.schedule_lr()                                 \\\\n-            for imgs, labels in tqdm(iter(self.loader)):\\\\n-                imgs = imgs.to(conf.device)\\\\n-                labels = labels.to(conf.device)\\\\n-                self.optimizer.zero_grad()\\\\n-                embeddings = self.model(imgs)\\\\n-                thetas = self.head(embeddings, labels)\\\\n-                loss = conf.ce_loss(thetas, labels)\\\\n-                loss.backward()\\\\n-                running_loss += loss.item()\\\\n-                self.optimizer.step()\\\\n-                \\\\n-                if self.step % self.board_loss_every == 0 and self.step != 0:\\\\n-                    loss_board = running_loss / self.board_loss_every\\\\n-                    self.writer.add_scalar(\\\\\\\'train_loss\\\\\\\', loss_board, self.step)\\\\n-                    running_loss = 0.\\\\n-                \\\\n-                if self.step % self.evaluate_every == 0 and self.step != 0:\\\\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.agedb_30, self.agedb_30_issame)\\\\n-                    self.board_val(\\\\\\\'agedb_30\\\\\\\', accuracy, best_threshold, roc_curve_tensor)\\\\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.lfw, self.lfw_issame)\\\\n-                    self.board_val(\\\\\\\'lfw\\\\\\\', accuracy, best_threshold, roc_curve_tensor)\\\\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.cfp_fp, self.cfp_fp_issame)\\\\n-                    self.board_val(\\\\\\\'cfp_fp\\\\\\\', accuracy, best_threshold, roc_curve_tensor)\\\\n-                    self.model.train()\\\\n-                if self.step % self.save_every == 0 and self.step != 0:\\\\n-                    self.save_state(conf, accuracy)\\\\n-                    \\\\n-                self.step += 1\\\\n-                \\\\n-        self.save_state(conf, accuracy, to_save_folder=True, extra=\\\\\\\'final\\\\\\\')\\\\n-\\\\n-    def schedule_lr(self):\\\\n-        for params in self.optimizer.param_groups:                 \\\\n-            params[\\\\\\\'lr\\\\\\\'] /= 10\\\\n-        print(self.optimizer)\\\\n-    \\\\n-    def infer(self, conf, faces, target_embs, tta=False):\\\\n-        \\\\\\\'\\\\\\\'\\\\\\\'\\\\n-        faces : list of PIL Image\\\\n-        target_embs : [n, 512] computed embeddings of faces in facebank\\\\n-        names : recorded names of faces in facebank\\\\n-        tta : test time augmentation (hfilp, that\\\\\\\'s all)\\\\n-        \\\\\\\'\\\\\\\'\\\\\\\'\\\\n-        embs = []\\\\n-        for img in faces:\\\\n-            if tta:\\\\n-                mirror = trans.functional.hflip(img)\\\\n-                emb = self.model(conf.test_transform(img).to(conf.device).unsqueeze(0))\\\\n-                emb_mirror = self.model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\\\\n-                embs.append(l2_norm(emb + emb_mirror))\\\\n-            else:                        \\\\n-                embs.append(self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\\\\n-        source_embs = torch.cat(embs)\\\\n-        \\\\n-        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1,0).unsqueeze(0)\\\\n-        dist = torch.sum(torch.pow(diff, 2), dim=1)\\\\n-        minimum, min_idx = torch.min(dist, dim=1)\\\\n-        if float(minimum[0]) > 0.9:\\\\n-            min_idx = -1\\\\n-        return min_idx, minimum               \\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/Pipfile b/Pipfile\\\\ndeleted file mode 100644\\\\nindex 9edb193..0000000\\\\n--- a/Pipfile\\\\n+++ /dev/null\\\\n@@ -1,32 +0,0 @@\\\\n-[[source]]\\\\n-url = "https://pypi.org/simple"\\\\n-verify_ssl = true\\\\n-name = "pypi"\\\\n-\\\\n-[packages]\\\\n-flask = "==1.1.1"\\\\n-scipy = "==1.1.0"\\\\n-matplotlib = "*"\\\\n-werkzeug = "==0.15.5"\\\\n-easydict = "==1.9"\\\\n-opencv-python = "*"\\\\n-gevent = "==21.8.0"\\\\n-mxnet-mkl = "==1.5.0"\\\\n-tqdm = "==4.35.0"\\\\n-bcolz = "==1.2.1"\\\\n-Pillow = "*"\\\\n-mtcnn-pytorch = "==1.0.2"\\\\n-mxnet = "==1.5.0"\\\\n-scikit-learn = "==0.21.3"\\\\n-tensorboardx = "==1.8"\\\\n-torch = "==1.10.2"\\\\n-numpy = "==1.16.1"\\\\n-greenlet = "==1.1.3"\\\\n-torchvision = "==0.4.1"\\\\n-imageio = "*"\\\\n-tensorflow = "==1.15"\\\\n-\\\\n-[dev-packages]\\\\n-\\\\n-[requires]\\\\n-python_version = "3.6"\\\\ndiff --git a/Pipfile.lock b/Pipfile.lock\\\\ndeleted file mode 100644\\\\nindex 9bb2547..0000000\\\\n--- a/Pipfile.lock\\\\n+++ /dev/null\\\\n@@ -1,1102 +0,0 @@\\\\n-{\\\\n-    "_meta": {\\\\n-        "hash": {\\\\n-            "sha256": "223161e10a8ee7514d3b29019bf148cf04324958428babf22504a82ff208e70a"\\\\n-        },\\\\n-        "pipfile-spec": 6,\\\\n-        "requires": {\\\\n-            "python_version": "3.6"\\\\n-        },\\\\n-        "sources": [\\\\n-            {\\\\n-                "name": "pypi",\\\\n-                "url": "https://pypi.org/simple",\\\\n-                "verify_ssl": true\\\\n-            }\\\\n-        ]\\\\n-    },\\\\n-    "default": {\\\\n-        "absl-py": {\\\\n-            "hashes": [\\\\n-                "sha256:0d3fe606adfa4f7db64792dd4c7aee4ee0c38ab75dfd353b7a83ed3e957fcb47",\\\\n-                "sha256:d2c244d01048ba476e7c080bd2c6df5e141d211de80223460d5b3b8a2a58433d"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==1.4.0"\\\\n-        },\\\\n-        "astor": {\\\\n-            "hashes": [\\\\n-                "sha256:070a54e890cefb5b3739d19f30f5a5ec840ffc9c50ffa7d23cc9fc1a38ebbfc5",\\\\n-                "sha256:6a6effda93f4e1ce9f618779b2dd1d9d84f1e32812c23a29b3fff6fd7f63fa5e"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3\\\\\\\'",\\\\n-            "version": "==0.8.1"\\\\n-        },\\\\n-        "bcolz": {\\\\n-            "hashes": [\\\\n-                "sha256:c017d09bb0cb5bbb07f2ae223a3f3638285be3b574cb328e91525b2880300bd1"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.2.1"\\\\n-        },\\\\n-        "cached-property": {\\\\n-            "hashes": [\\\\n-                "sha256:9fa5755838eecbb2d234c3aa390bd80fbd3ac6b6869109bfc1b499f7bd89a130",\\\\n-                "sha256:df4f613cf7ad9a588cc381aaf4a512d26265ecebd5eb9e1ba12f1319eb85a6a0"\\\\n-            ],\\\\n-            "markers": "python_version < \\\\\\\'3.8\\\\\\\'",\\\\n-            "version": "==1.5.2"\\\\n-        },\\\\n-        "certifi": {\\\\n-            "hashes": [\\\\n-                "sha256:35824b4c3a97115964b408844d64aa14db1cc518f6562e8d7261699d1350a9e3",\\\\n-                "sha256:4ad3232f5e926d6718ec31cfc1fcadfde020920e278684144551c91769c7bc18"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==2022.12.7"\\\\n-        },\\\\n-        "cffi": {\\\\n-            "hashes": [\\\\n-                "sha256:00a9ed42e88df81ffae7a8ab6d9356b371399b91dbdf0c3cb1e84c03a13aceb5",\\\\n-                "sha256:03425bdae262c76aad70202debd780501fabeaca237cdfddc008987c0e0f59ef",\\\\n-                "sha256:04ed324bda3cda42b9b695d51bb7d54b680b9719cfab04227cdd1e04e5de3104",\\\\n-                "sha256:0e2642fe3142e4cc4af0799748233ad6da94c62a8bec3a6648bf8ee68b1c7426",\\\\n-                "sha256:173379135477dc8cac4bc58f45db08ab45d228b3363adb7af79436135d028405",\\\\n-                "sha256:198caafb44239b60e252492445da556afafc7d1e3ab7a1fb3f0584ef6d742375",\\\\n-                "sha256:1e74c6b51a9ed6589199c787bf5f9875612ca4a8a0785fb2d4a84429badaf22a",\\\\n-                "sha256:2012c72d854c2d03e45d06ae57f40d78e5770d252f195b93f581acf3ba44496e",\\\\n-                "sha256:21157295583fe8943475029ed5abdcf71eb3911894724e360acff1d61c1d54bc",\\\\n-                "sha256:2470043b93ff09bf8fb1d46d1cb756ce6132c54826661a32d4e4d132e1977adf",\\\\n-                "sha256:285d29981935eb726a4399badae8f0ffdff4f5050eaa6d0cfc3f64b857b77185",\\\\n-                "sha256:30d78fbc8ebf9c92c9b7823ee18eb92f2e6ef79b45ac84db507f52fbe3ec4497",\\\\n-                "sha256:320dab6e7cb2eacdf0e658569d2575c4dad258c0fcc794f46215e1e39f90f2c3",\\\\n-                "sha256:33ab79603146aace82c2427da5ca6e58f2b3f2fb5da893ceac0c42218a40be35",\\\\n-                "sha256:3548db281cd7d2561c9ad9984681c95f7b0e38881201e157833a2342c30d5e8c",\\\\n-                "sha256:3799aecf2e17cf585d977b780ce79ff0dc9b78d799fc694221ce814c2c19db83",\\\\n-                "sha256:39d39875251ca8f612b6f33e6b1195af86d1b3e60086068be9cc053aa4376e21",\\\\n-                "sha256:3b926aa83d1edb5aa5b427b4053dc420ec295a08e40911296b9eb1b6170f6cca",\\\\n-                "sha256:3bcde07039e586f91b45c88f8583ea7cf7a0770df3a1649627bf598332cb6984",\\\\n-                "sha256:3d08afd128ddaa624a48cf2b859afef385b720bb4b43df214f85616922e6a5ac",\\\\n-                "sha256:3eb6971dcff08619f8d91607cfc726518b6fa2a9eba42856be181c6d0d9515fd",\\\\n-                "sha256:40f4774f5a9d4f5e344f31a32b5096977b5d48560c5592e2f3d2c4374bd543ee",\\\\n-                "sha256:4289fc34b2f5316fbb762d75362931e351941fa95fa18789191b33fc4cf9504a",\\\\n-                "sha256:470c103ae716238bbe698d67ad020e1db9d9dba34fa5a899b5e21577e6d52ed2",\\\\n-                "sha256:4f2c9f67e9821cad2e5f480bc8d83b8742896f1242dba247911072d4fa94c192",\\\\n-                "sha256:50a74364d85fd319352182ef59c5c790484a336f6db772c1a9231f1c3ed0cbd7",\\\\n-                "sha256:54a2db7b78338edd780e7ef7f9f6c442500fb0d41a5a4ea24fff1c929d5af585",\\\\n-                "sha256:5635bd9cb9731e6d4a1132a498dd34f764034a8ce60cef4f5319c0541159392f",\\\\n-                "sha256:59c0b02d0a6c384d453fece7566d1c7e6b7bae4fc5874ef2ef46d56776d61c9e",\\\\n-                "sha256:5d598b938678ebf3c67377cdd45e09d431369c3b1a5b331058c338e201f12b27",\\\\n-                "sha256:5df2768244d19ab7f60546d0c7c63ce1581f7af8b5de3eb3004b9b6fc8a9f84b",\\\\n-                "sha256:5ef34d190326c3b1f822a5b7a45f6c4535e2f47ed06fec77d3d799c450b2651e",\\\\n-                "sha256:6975a3fac6bc83c4a65c9f9fcab9e47019a11d3d2cf7f3c0d03431bf145a941e",\\\\n-                "sha256:6c9a799e985904922a4d207a94eae35c78ebae90e128f0c4e521ce339396be9d",\\\\n-                "sha256:70df4e3b545a17496c9b3f41f5115e69a4f2e77e94e1d2a8e1070bc0c38c8a3c",\\\\n-                "sha256:7473e861101c9e72452f9bf8acb984947aa1661a7704553a9f6e4baa5ba64415",\\\\n-                "sha256:8102eaf27e1e448db915d08afa8b41d6c7ca7a04b7d73af6514df10a3e74bd82",\\\\n-                "sha256:87c450779d0914f2861b8526e035c5e6da0a3199d8f1add1a665e1cbc6fc6d02",\\\\n-                "sha256:8b7ee99e510d7b66cdb6c593f21c043c248537a32e0bedf02e01e9553a172314",\\\\n-                "sha256:91fc98adde3d7881af9b59ed0294046f3806221863722ba7d8d120c575314325",\\\\n-                "sha256:94411f22c3985acaec6f83c6df553f2dbe17b698cc7f8ae751ff2237d96b9e3c",\\\\n-                "sha256:98d85c6a2bef81588d9227dde12db8a7f47f639f4a17c9ae08e773aa9c697bf3",\\\\n-                "sha256:9ad5db27f9cabae298d151c85cf2bad1d359a1b9c686a275df03385758e2f914",\\\\n-                "sha256:a0b71b1b8fbf2b96e41c4d990244165e2c9be83d54962a9a1d118fd8657d2045",\\\\n-                "sha256:a0f100c8912c114ff53e1202d0078b425bee3649ae34d7b070e9697f93c5d52d",\\\\n-                "sha256:a591fe9e525846e4d154205572a029f653ada1a78b93697f3b5a8f1f2bc055b9",\\\\n-                "sha256:a5c84c68147988265e60416b57fc83425a78058853509c1b0629c180094904a5",\\\\n-                "sha256:a66d3508133af6e8548451b25058d5812812ec3798c886bf38ed24a98216fab2",\\\\n-                "sha256:a8c4917bd7ad33e8eb21e9a5bbba979b49d9a97acb3a803092cbc1133e20343c",\\\\n-                "sha256:b3bbeb01c2b273cca1e1e0c5df57f12dce9a4dd331b4fa1635b8bec26350bde3",\\\\n-                "sha256:cba9d6b9a7d64d4bd46167096fc9d2f835e25d7e4c121fb2ddfc6528fb0413b2",\\\\n-                "sha256:cc4d65aeeaa04136a12677d3dd0b1c0c94dc43abac5860ab33cceb42b801c1e8",\\\\n-                "sha256:ce4bcc037df4fc5e3d184794f27bdaab018943698f4ca31630bc7f84a7b69c6d",\\\\n-                "sha256:cec7d9412a9102bdc577382c3929b337320c4c4c4849f2c5cdd14d7368c5562d",\\\\n-                "sha256:d400bfb9a37b1351253cb402671cea7e89bdecc294e8016a707f6d1d8ac934f9",\\\\n-                "sha256:d61f4695e6c866a23a21acab0509af1cdfd2c013cf256bbf5b6b5e2695827162",\\\\n-                "sha256:db0fbb9c62743ce59a9ff687eb5f4afbe77e5e8403d6697f7446e5f609976f76",\\\\n-                "sha256:dd86c085fae2efd48ac91dd7ccffcfc0571387fe1193d33b6394db7ef31fe2a4",\\\\n-                "sha256:e00b098126fd45523dd056d2efba6c5a63b71ffe9f2bbe1a4fe1716e1d0c331e",\\\\n-                "sha256:e229a521186c75c8ad9490854fd8bbdd9a0c9aa3a524326b55be83b54d4e0ad9",\\\\n-                "sha256:e263d77ee3dd201c3a142934a086a4450861778baaeeb45db4591ef65550b0a6",\\\\n-                "sha256:ed9cb427ba5504c1dc15ede7d516b84757c3e3d7868ccc85121d9310d27eed0b",\\\\n-                "sha256:fa6693661a4c91757f4412306191b6dc88c1703f780c8234035eac011922bc01",\\\\n-                "sha256:fcd131dd944808b5bdb38e6f5b53013c5aa4f334c5cad0c72742f6eba4b73db0"\\\\n-            ],\\\\n-            "markers": "platform_python_implementation == \\\\\\\'CPython\\\\\\\' and sys_platform == \\\\\\\'win32\\\\\\\'",\\\\n-            "version": "==1.15.1"\\\\n-        },\\\\n-        "chardet": {\\\\n-            "hashes": [\\\\n-                "sha256:84ab92ed1c4d4f16916e05906b6b75a6c0fb5db821cc65e70cbd64a3e2a5eaae",\\\\n-                "sha256:fc323ffcaeaed0e0a02bf4d117757b98aed530d9ed4531e3e15460124c106691"\\\\n-            ],\\\\n-            "version": "==3.0.4"\\\\n-        },\\\\n-        "click": {\\\\n-            "hashes": [\\\\n-                "sha256:6a7a62563bbfabfda3a38f3023a1db4a35978c0abd76f6c9605ecd6554d6d9b1",\\\\n-                "sha256:8458d7b1287c5fb128c90e23381cf99dcde74beaf6c7ff6384ce84d6fe090adb"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==8.0.4"\\\\n-        },\\\\n-        "colorama": {\\\\n-            "hashes": [\\\\n-                "sha256:854bf444933e37f5824ae7bfc1e98d5bce2ebe4160d46b5edf346a89358e99da",\\\\n-                "sha256:e6c6b4334fc50988a639d9b98aa429a0b57da6e17b9a44f0451f930b6967b7a4"\\\\n-            ],\\\\n-            "markers": "platform_system == \\\\\\\'Windows\\\\\\\'",\\\\n-            "version": "==0.4.5"\\\\n-        },\\\\n-        "cycler": {\\\\n-            "hashes": [\\\\n-                "sha256:3a27e95f763a428a739d2add979fa7494c912a32c17c4c38c4d5f082cad165a3",\\\\n-                "sha256:9c87405839a19696e837b3b818fed3f5f69f16f1eec1a1ad77e043dcea9c772f"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==0.11.0"\\\\n-        },\\\\n-        "dataclasses": {\\\\n-            "hashes": [\\\\n-                "sha256:0201d89fa866f68c8ebd9d08ee6ff50c0b255f8ec63a71c16fda7af82bb887bf",\\\\n-                "sha256:8479067f342acf957dc82ec415d355ab5edb7e7646b90dc6e2fd1d96ad084c97"\\\\n-            ],\\\\n-            "markers": "python_version < \\\\\\\'3.7\\\\\\\'",\\\\n-            "version": "==0.8"\\\\n-        },\\\\n-        "easydict": {\\\\n-            "hashes": [\\\\n-                "sha256:3f3f0dab07c299f0f4df032db1f388d985bb57fa4c5be30acd25c5f9a516883b"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.9"\\\\n-        },\\\\n-        "flask": {\\\\n-            "hashes": [\\\\n-                "sha256:13f9f196f330c7c2c5d7a5cf91af894110ca0215ac051b5844701f2bfd934d52",\\\\n-                "sha256:45eb5a6fd193d6cf7e0cf5d8a5b31f83d5faae0293695626f539a823e93b13f6"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.1.1"\\\\n-        },\\\\n-        "gast": {\\\\n-            "hashes": [\\\\n-                "sha256:fe939df4583692f0512161ec1c880e0a10e71e6a232da045ab8edd3756fbadf0"\\\\n-            ],\\\\n-            "version": "==0.2.2"\\\\n-        },\\\\n-        "gevent": {\\\\n-            "hashes": [\\\\n-                "sha256:02d1e8ca227d0ab0b7917fd7e411f9a534475e0a41fb6f434e9264b20155201a",\\\\n-                "sha256:0c7b4763514fec74c9fe6ad10c3de62d8fe7b926d520b1e35eb6887181b954ff",\\\\n-                "sha256:1c9c87b15f792af80edc950a83ab8ef4f3ba3889712211c2c42740ddb57b5492",\\\\n-                "sha256:23077d87d1589ac141c22923fd76853d2cc5b7e3c5e1f1f9cdf6ff23bc9790fc",\\\\n-                "sha256:37a469a99e6000b42dd0b9bbd9d716dbd66cdc6e5738f136f6a266c29b90ee99",\\\\n-                "sha256:3b600145dc0c5b39c6f89c2e91ec6c55eb0dd52dc8148228479ca42cded358e4",\\\\n-                "sha256:3f5ba654bdd3c774079b553fef535ede5b52c7abd224cb235a15da90ae36251b",\\\\n-                "sha256:43e93e1a4738c922a2416baf33f0afb0a20b22d3dba886720bc037cd02a98575",\\\\n-                "sha256:473f918bdf7d2096e391f66bd8ce1e969639aa235e710aaf750a37774bb585bd",\\\\n-                "sha256:4c94d27be9f0439b28eb8bd0f879e6142918c62092fda7fb96b6d06f01886b94",\\\\n-                "sha256:55ede95f41b74e7506fab293ad04cc7fc2b6f662b42281e9f2d668ad3817b574",\\\\n-                "sha256:6cad37a55e904879beef2a7e7c57c57d62fde2331fef1bec7f2b2a7ef14da6a2",\\\\n-                "sha256:72d4c2a8e65bbc702db76456841c7ddd6de2d9ab544a24aa74ad9c2b6411a269",\\\\n-                "sha256:75c29ed5148c916021d39d2fac90ccc0e19adf854626a34eaee012aa6b1fcb67",\\\\n-                "sha256:84e1af2dfb4ea9495cb914b00b6303ca0d54bf0a92e688a17e60f6b033873df2",\\\\n-                "sha256:8d8655ce581368b7e1ab42c8a3a166c0b43ea04e59970efbade9448864585e99",\\\\n-                "sha256:90131877d3ce1a05da1b718631860815b89ff44e93c42d168c9c9e8893b26318",\\\\n-                "sha256:9d46bea8644048ceac5737950c08fc89c37a66c34a56a6c9e3648726e60cb767",\\\\n-                "sha256:a8656d6e02bf47d7fa47728cf7a7cbf408f77ef1fad12afd9e0e3246c5de1707",\\\\n-                "sha256:aaf1451cd0d9c32f65a50e461084a0540be52b8ea05c18669c95b42e1f71592a",\\\\n-                "sha256:afc877ff4f277d0e51a1206d748fdab8c1e0256f7a05e1b1067abbed71c64da9",\\\\n-                "sha256:b10c3326edb76ec3049646dc5131608d6d3733b5adfc75d34852028ecc67c52c",\\\\n-                "sha256:ceec7c5f15fb2f9b767b194daa55246830db6c7c3c2f0b1c7e9e90cb4d01f3f9",\\\\n-                "sha256:e00dc0450f79253b7a3a7f2a28e6ca959c8d0d47c0f9fa2c57894c7974d5965f",\\\\n-                "sha256:e91632fdcf1c9a33e97e35f96edcbdf0b10e36cf53b58caa946dca4836bb688c",\\\\n-                "sha256:f39d5defda9443b5fb99a185050e94782fe7ac38f34f751b491142216ad23bc7"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==21.8.0"\\\\n-        },\\\\n-        "google-pasta": {\\\\n-            "hashes": [\\\\n-                "sha256:4612951da876b1a10fe3960d7226f0c7682cf901e16ac06e473b267a5afa8954",\\\\n-                "sha256:b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed",\\\\n-                "sha256:c9f2c8dfc8f96d0d5808299920721be30c9eec37f2389f28904f454565c8a16e"\\\\n-            ],\\\\n-            "version": "==0.2.0"\\\\n-        },\\\\n-        "graphviz": {\\\\n-            "hashes": [\\\\n-                "sha256:4958a19cbd8461757a08db308a4a15c3d586660417e1e364f0107d2fe481689f",\\\\n-                "sha256:7caa53f0b0be42c5f2eaa3f3d71dcc863b15bacceb5d531c2ad7519e1980ff82"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3\\\\\\\'",\\\\n-            "version": "==0.8.4"\\\\n-        },\\\\n-        "greenlet": {\\\\n-            "hashes": [\\\\n-                "sha256:0118817c9341ef2b0f75f5af79ac377e4da6ff637e5ee4ac91802c0e379dadb4",\\\\n-                "sha256:048d2bed76c2aa6de7af500ae0ea51dd2267aec0e0f2a436981159053d0bc7cc",\\\\n-                "sha256:07c58e169bbe1e87b8bbf15a5c1b779a7616df9fd3e61cadc9d691740015b4f8",\\\\n-                "sha256:095a980288fe05adf3d002fbb180c99bdcf0f930e220aa66fcd56e7914a38202",\\\\n-                "sha256:0b181e9aa6cb2f5ec0cacc8cee6e5a3093416c841ba32c185c30c160487f0380",\\\\n-                "sha256:1626185d938d7381631e48e6f7713e8d4b964be246073e1a1d15c2f061ac9f08",\\\\n-                "sha256:184416e481295832350a4bf731ba619a92f5689bf5d0fa4341e98b98b1265bd7",\\\\n-                "sha256:1dd51d2650e70c6c4af37f454737bf4a11e568945b27f74b471e8e2a9fd21268",\\\\n-                "sha256:1ec2779774d8e42ed0440cf8bc55540175187e8e934f2be25199bf4ed948cd9e",\\\\n-                "sha256:2cf45e339cabea16c07586306a31cfcc5a3b5e1626d365714d283732afed6809",\\\\n-                "sha256:2fb0aa7f6996879551fd67461d5d3ab0c3c0245da98be90c89fcb7a18d437403",\\\\n-                "sha256:44b4817c34c9272c65550b788913620f1fdc80362b209bc9d7dd2f40d8793080",\\\\n-                "sha256:466ce0928e33421ee84ae04c4ac6f253a3a3e6b8d600a79bd43fd4403e0a7a76",\\\\n-                "sha256:4f166b4aca8d7d489e82d74627a7069ab34211ef5ebb57c300ec4b9337b60fc0",\\\\n-                "sha256:510c3b15587afce9800198b4b142202b323bf4b4b5f9d6c79cb9a35e5e3c30d2",\\\\n-                "sha256:5b756e6730ea59b2745072e28ad27f4c837084688e6a6b3633c8b1e509e6ae0e",\\\\n-                "sha256:5fbe1ab72b998ca77ceabbae63a9b2e2dc2d963f4299b9b278252ddba142d3f1",\\\\n-                "sha256:6200a11f003ec26815f7e3d2ded01b43a3810be3528dd760d2f1fa777490c3cd",\\\\n-                "sha256:65ad1a7a463a2a6f863661329a944a5802c7129f7ad33583dcc11069c17e622c",\\\\n-                "sha256:694ffa7144fa5cc526c8f4512665003a39fa09ef00d19bbca5c8d3406db72fbe",\\\\n-                "sha256:6f5d4b2280ceea76c55c893827961ed0a6eadd5a584a7c4e6e6dd7bc10dfdd96",\\\\n-                "sha256:7532a46505470be30cbf1dbadb20379fb481244f1ca54207d7df3bf0bbab6a20",\\\\n-                "sha256:76a53bfa10b367ee734b95988bd82a9a5f0038a25030f9f23bbbc005010ca600",\\\\n-                "sha256:77e41db75f9958f2083e03e9dd39da12247b3430c92267df3af77c83d8ff9eed",\\\\n-                "sha256:7a43bbfa9b6cfdfaeefbd91038dde65ea2c421dc387ed171613df340650874f2",\\\\n-                "sha256:7b41d19c0cfe5c259fe6c539fd75051cd39a5d33d05482f885faf43f7f5e7d26",\\\\n-                "sha256:7c5227963409551ae4a6938beb70d56bf1918c554a287d3da6853526212fbe0a",\\\\n-                "sha256:870a48007872d12e95a996fca3c03a64290d3ea2e61076aa35d3b253cf34cd32",\\\\n-                "sha256:88b04e12c9b041a1e0bcb886fec709c488192638a9a7a3677513ac6ba81d8e79",\\\\n-                "sha256:8c287ae7ac921dfde88b1c125bd9590b7ec3c900c2d3db5197f1286e144e712b",\\\\n-                "sha256:903fa5716b8fbb21019268b44f73f3748c41d1a30d71b4a49c84b642c2fed5fa",\\\\n-                "sha256:9537e4baf0db67f382eb29255a03154fcd4984638303ff9baaa738b10371fa57",\\\\n-                "sha256:9951dcbd37850da32b2cb6e391f621c1ee456191c6ae5528af4a34afe357c30e",\\\\n-                "sha256:9b2f7d0408ddeb8ea1fd43d3db79a8cefaccadd2a812f021333b338ed6b10aba",\\\\n-                "sha256:9c88e134d51d5e82315a7c32b914a58751b7353eb5268dbd02eabf020b4c4700",\\\\n-                "sha256:9fae214f6c43cd47f7bef98c56919b9222481e833be2915f6857a1e9e8a15318",\\\\n-                "sha256:a3a669f11289a8995d24fbfc0e63f8289dd03c9aaa0cc8f1eab31d18ca61a382",\\\\n-                "sha256:aa741c1a8a8cc25eb3a3a01a62bdb5095a773d8c6a86470bde7f607a447e7905",\\\\n-                "sha256:b0877a9a2129a2c56a2eae2da016743db7d9d6a05d5e1c198f1b7808c602a30e",\\\\n-                "sha256:bcb6c6dd1d6be6d38d6db283747d07fda089ff8c559a835236560a4410340455",\\\\n-                "sha256:caff52cb5cd7626872d9696aee5b794abe172804beb7db52eed1fd5824b63910",\\\\n-                "sha256:cbc1eb55342cbac8f7ec159088d54e2cfdd5ddf61c87b8bbe682d113789331b2",\\\\n-                "sha256:cd16a89efe3a003029c87ff19e9fba635864e064da646bc749fc1908a4af18f3",\\\\n-                "sha256:ce5b64dfe8d0cca407d88b0ee619d80d4215a2612c1af8c98a92180e7109f4b5",\\\\n-                "sha256:d58a5a71c4c37354f9e0c24c9c8321f0185f6945ef027460b809f4bb474bfe41",\\\\n-                "sha256:db41f3845eb579b544c962864cce2c2a0257fe30f0f1e18e51b1e8cbb4e0ac6d",\\\\n-                "sha256:db5b25265010a1b3dca6a174a443a0ed4c4ab12d5e2883a11c97d6e6d59b12f9",\\\\n-                "sha256:dd0404d154084a371e6d2bafc787201612a1359c2dee688ae334f9118aa0bf47",\\\\n-                "sha256:de431765bd5fe62119e0bc6bc6e7b17ac53017ae1782acf88fcf6b7eae475a49",\\\\n-                "sha256:df02fdec0c533301497acb0bc0f27f479a3a63dcdc3a099ae33a902857f07477",\\\\n-                "sha256:e8533f5111704d75de3139bf0b8136d3a6c1642c55c067866fa0a51c2155ee33",\\\\n-                "sha256:f2f908239b7098799b8845e5936c2ccb91d8c2323be02e82f8dcb4a80dcf4a25",\\\\n-                "sha256:f8bfd36f368efe0ab2a6aa3db7f14598aac454b06849fb633b762ddbede1db90",\\\\n-                "sha256:ffe73f9e7aea404722058405ff24041e59d31ca23d1da0895af48050a07b6932"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.1.3"\\\\n-        },\\\\n-        "grpcio": {\\\\n-            "hashes": [\\\\n-                "sha256:0802b080b6b8603a065e505ce83190b6a06229b9a74d0a1681175271ac84fe12",\\\\n-                "sha256:0bfb637344442b273b698ff425d735a5d806ca8715f988875ad669277fb9b1e6",\\\\n-                "sha256:104b555e1cb2e0614f05c1def24eb8bb06f1277460058aa0f9c9e6a1018716da",\\\\n-                "sha256:110028e0b9c346230ae69b8a6d8b25d4d43bfd37bda61a8ec46486da1e781dcb",\\\\n-                "sha256:13c3b69f8efb214a54f48e8dd1e235a4d8d22fa985f32a9b2844373993c5a605",\\\\n-                "sha256:199526758f6f8d35a596c610f33ea76faae65ec175dc109e8481ea3404d8527c",\\\\n-                "sha256:1e2dc213fe71566efbf9a5d704c665ff4b1760a88d37f8533b19ca92776070c9",\\\\n-                "sha256:1fea4cb4368dd0467eb2d208e2d5e3c4f0be28fe33965d45ac9e1d562be67a8c",\\\\n-                "sha256:26ed6d07f91ce8aeb4697b7e71d930355282ec80acb7a488f4030a3a75c2f7a8",\\\\n-                "sha256:2bb1df2920a4968f0c09041b49e591df96f2e6f801f15eed3821c1f16a12f1a8",\\\\n-                "sha256:2d99fb56c7e836f165828719c3695d3d27ac70b103ab52226f7a7c237e4a3928",\\\\n-                "sha256:2f185b8c5130663c455f6542906ce99f046608e94950c8b354aa22462c202c2d",\\\\n-                "sha256:3463256399158e9abf115620994e968db8f003224c36bda0d14570eab8a44cfc",\\\\n-                "sha256:3d3225d477663c27b9051546a32551babd1ccb80192905e08340264deccc975b",\\\\n-                "sha256:3f52ef5ba7a8bc334daa87675838d6dafda7d8a116a72b567b8351e561ace498",\\\\n-                "sha256:47ace91631176efa575c7a34d5004286288f1af1e9de2ff380d1433f241aeed4",\\\\n-                "sha256:514392a30a275f4f719c2e05ea969c239e5f03eec4a25965852c7582073d8b94",\\\\n-                "sha256:550b08dfa938e30ffbc1652193cf2877906aa6242d6ba9f61318dc87fcecee63",\\\\n-                "sha256:5571cb828d694b34a7c75484722803e13a2f5e4760e47ae32fb077c83d0c9b2c",\\\\n-                "sha256:5792943481d4270b3e9a4700af0eea86e7183f4d3c250a46e0b357949cc09411",\\\\n-                "sha256:665141b3a97b7d22978c8d2ba0c0af7f67bd6d7a56889c5c0aa715d04009b518",\\\\n-                "sha256:6affa7e685edbb7421f942296eb618359362e89e641bcf46779c6ec7b944d275",\\\\n-                "sha256:6f693da8ffd2486c354c90ba5a8ca0f4c50bfb8853495501884cadc152551360",\\\\n-                "sha256:855c125e8cd1c3ab09a239689c940d26c30680edf2edf87c3c1543bd8633cc8f",\\\\n-                "sha256:894c5f02c25c83c2320310521a82978b4e252ee18b99a5c4c564d73daeb5c1de",\\\\n-                "sha256:8d130f666463e4d09a63ff033a6c5cd032867fd51a0db4660c18106aa352be3a",\\\\n-                "sha256:90e5da224c6b9b23658adf6f36de6f435ef7dbcc9c5c12330314d70d6f8de1f7",\\\\n-                "sha256:9dc08baa1b28749e90428aaa16e038e8c389d8ccb843ddc0dc8b95231640b432",\\\\n-                "sha256:a760ef87fde9a8f2761c7ad8ccf617fc590547ed743a9207fe7e367496164c60",\\\\n-                "sha256:abee7dd82443b2cd128004e053b263a6d7256d570df80956a974634b8c5bc121",\\\\n-                "sha256:b860e13c112bb9cb44007ef02853a19397d915b31c42dfa18570448bdc0a6245",\\\\n-                "sha256:b8768daa636e0fa48fec75517bea65ce8fdaca0066dc411fc0a2290d92032f91",\\\\n-                "sha256:b8ec07dcc1cbd77b8c09dfc0ce6274920cb7b09cc04013110971d95d8bcc0bbf",\\\\n-                "sha256:c19d6f337860f382ceaa35c5acab439d84c5ffaa8baba36df1f83ab6b9ac4bd3",\\\\n-                "sha256:c92b5ef64cd5a0c6aea82dd6862fdb8a1562510d537ea3c356a7fe60db7021af",\\\\n-                "sha256:cebeed160466a1e254eb75e7e1bbeeb1359c50b33a1b8f3b2241a8b8dc9bd216",\\\\n-                "sha256:d7b1a3a75c34ab39c9df73aa9fecc519dc1035e588a41af19f39b1298a283a57",\\\\n-                "sha256:d7ec6b04875a5065d04ad86cd2678ca6431dec868c01d731b8233f3de155bfdf",\\\\n-                "sha256:dc00681d546cae66e9d54451f650fe140f9e1aca2dc4f8c9686cfaa4dd5d680b",\\\\n-                "sha256:e48595440dc86e13245aec7c096238db12b659e5ae6078aecf99d66befb77678",\\\\n-                "sha256:e69a5907a2a4cf0011ff46205b6bff8f56b8391436acc3c66b70ce8519578d7e",\\\\n-                "sha256:ec2dd9f7ab0c809af6b2c65ed31c3cbef2ca9695f7f4d49866ec4707e7836890",\\\\n-                "sha256:f1d2cd5b1adecbcffee4ad6613f100e0b583ae2e253d2f8f685e7770ec72d622",\\\\n-                "sha256:f3c0d0995a0cd8c7198cb49b8ce98b4936c5a70109f9246c58e69c898e4f7329",\\\\n-                "sha256:f5697e4ab90a41a6a1202c1a3ec268a0d69f1cd127a4940d2b2521a0fbc1277b",\\\\n-                "sha256:f6afd1f4b5e0ec320fb2b027a646944fee8b58ba00fb43d081968f77d1a6e925"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==1.48.2"\\\\n-        },\\\\n-        "h5py": {\\\\n-            "hashes": [\\\\n-                "sha256:02c391fdb980762a1cc03a4bcaecd03dc463994a9a63a02264830114a96e111f",\\\\n-                "sha256:1cd367f89a5441236bdbb795e9fb9a9e3424929c00b4a54254ca760437f83d69",\\\\n-                "sha256:1cdfd1c5449ca1329d152f0b66830e93226ebce4f5e07dd8dc16bfc2b1a49d7b",\\\\n-                "sha256:1e2516f190652beedcb8c7acfa1c6fa92d99b42331cbef5e5c7ec2d65b0fc3c2",\\\\n-                "sha256:236ac8d943be30b617ab615c3d4a4bf4a438add2be87e54af3687ab721a18fac",\\\\n-                "sha256:2e37352ddfcf9d77a2a47f7c8f7e125c6d20cc06c2995edeb7be222d4e152636",\\\\n-                "sha256:80c623be10479e81b64fa713b7ed4c0bbe9f02e8e7d2a2e5382336087b615ce4",\\\\n-                "sha256:ba71f6229d2013fbb606476ecc29c6223fc16b244d35fcd8566ad9dbaf910857",\\\\n-                "sha256:cb74df83709d6d03d11e60b9480812f58da34f194beafa8c8314dbbeeedfe0a6",\\\\n-                "sha256:dccb89358bc84abcd711363c3e138f9f4eccfdf866f2139a8e72308328765b2c",\\\\n-                "sha256:e33f61d3eb862614c0f273a1f993a64dc2f093e1a3094932c50ada9d2db2170f",\\\\n-                "sha256:f89a3dae38843ffa49d17a31a3509a8129e9b46ece602a0138e1ed79e685c361",\\\\n-                "sha256:fea05349f63625a8fb808e57e42bb4c76930cf5d50ac58b678c52f913a48a89b"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==3.1.0"\\\\n-        },\\\\n-        "idna": {\\\\n-            "hashes": [\\\\n-                "sha256:2c6a5de3089009e3da7c5dde64a141dbc8551d5b7f6cf4ed7c2568d0cc520a8f",\\\\n-                "sha256:8c7309c718f94b3a625cb648ace320157ad16ff131ae0af362c9f21b80ef6ec4"\\\\n-            ],\\\\n-            "version": "==2.6"\\\\n-        },\\\\n-        "imageio": {\\\\n-            "hashes": [\\\\n-                "sha256:d0d7abb4e5c4044c06fc573233489c4a25582698f93ca94f7bd70b6f4ab172ec",\\\\n-                "sha256:d5e553c8e8fd01ac27094df32bf020b553f640a71b042c960c8341bf7d313a8e"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==2.15.0"\\\\n-        },\\\\n-        "importlib-metadata": {\\\\n-            "hashes": [\\\\n-                "sha256:65a9576a5b2d58ca44d133c42a241905cc45e34d2c06fd5ba2bafa221e5d7b5e",\\\\n-                "sha256:766abffff765960fcc18003801f7044eb6755ffae4521c8e8ce8e83b9c9b0668"\\\\n-            ],\\\\n-            "markers": "python_version < \\\\\\\'3.8\\\\\\\'",\\\\n-            "version": "==4.8.3"\\\\n-        },\\\\n-        "itsdangerous": {\\\\n-            "hashes": [\\\\n-                "sha256:5174094b9637652bdb841a3029700391451bd092ba3db90600dea710ba28e97c",\\\\n-                "sha256:9e724d68fc22902a1435351f84c3fb8623f303fffcc566a4cb952df8c572cff0"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==2.0.1"\\\\n-        },\\\\n-        "jinja2": {\\\\n-            "hashes": [\\\\n-                "sha256:077ce6014f7b40d03b47d1f1ca4b0fc8328a692bd284016f806ed0eaca390ad8",\\\\n-                "sha256:611bb273cd68f3b993fabdc4064fc858c5b47a973cb5aa7999ec1ba405c87cd7"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==3.0.3"\\\\n-        },\\\\n-        "joblib": {\\\\n-            "hashes": [\\\\n-                "sha256:301f0375f49586a7effee3f6348c419d5765fca1c750186b20690a0d90b82900",\\\\n-                "sha256:f9d6c3cdf2a7778e9058e10e9dba028e47771a1a355e5768f46704bf05342eba"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==1.1.1"\\\\n-        },\\\\n-        "keras-applications": {\\\\n-            "hashes": [\\\\n-                "sha256:5579f9a12bcde9748f4a12233925a59b93b73ae6947409ff34aa2ba258189fe5",\\\\n-                "sha256:df4323692b8c1174af821bf906f1e442e63fa7589bf0f1230a0b6bdc5a810c95"\\\\n-            ],\\\\n-            "version": "==1.0.8"\\\\n-        },\\\\n-        "keras-preprocessing": {\\\\n-            "hashes": [\\\\n-                "sha256:7b82029b130ff61cc99b55f3bd27427df4838576838c5b2f65940e4fcec99a7b",\\\\n-                "sha256:add82567c50c8bc648c14195bf544a5ce7c1f76761536956c3d2978970179ef3"\\\\n-            ],\\\\n-            "version": "==1.1.2"\\\\n-        },\\\\n-        "kiwisolver": {\\\\n-            "hashes": [\\\\n-                "sha256:0cd53f403202159b44528498de18f9285b04482bab2a6fc3f5dd8dbb9352e30d",\\\\n-                "sha256:1e1bc12fb773a7b2ffdeb8380609f4f8064777877b2225dec3da711b421fda31",\\\\n-                "sha256:225e2e18f271e0ed8157d7f4518ffbf99b9450fca398d561eb5c4a87d0986dd9",\\\\n-                "sha256:232c9e11fd7ac3a470d65cd67e4359eee155ec57e822e5220322d7b2ac84fbf0",\\\\n-                "sha256:24cc411232d14c8abafbd0dddb83e1a4f54d77770b53db72edcfe1d611b3bf11",\\\\n-                "sha256:31dfd2ac56edc0ff9ac295193eeaea1c0c923c0355bf948fbd99ed6018010b72",\\\\n-                "sha256:33449715e0101e4d34f64990352bce4095c8bf13bed1b390773fc0a7295967b3",\\\\n-                "sha256:401a2e9afa8588589775fe34fc22d918ae839aaaf0c0e96441c0fdbce6d8ebe6",\\\\n-                "sha256:44a62e24d9b01ba94ae7a4a6c3fb215dc4af1dde817e7498d901e229aaf50e4e",\\\\n-                "sha256:50af681a36b2a1dee1d3c169ade9fdc59207d3c31e522519181e12f1b3ba7000",\\\\n-                "sha256:563c649cfdef27d081c84e72a03b48ea9408c16657500c312575ae9d9f7bc1c3",\\\\n-                "sha256:5989db3b3b34b76c09253deeaf7fbc2707616f130e166996606c284395da3f18",\\\\n-                "sha256:5a7a7dbff17e66fac9142ae2ecafb719393aaee6a3768c9de2fd425c63b53e21",\\\\n-                "sha256:5c3e6455341008a054cccee8c5d24481bcfe1acdbc9add30aa95798e95c65621",\\\\n-                "sha256:5f6ccd3dd0b9739edcf407514016108e2280769c73a85b9e59aa390046dbf08b",\\\\n-                "sha256:6d9d8d9b31aa8c2d80a690693aebd8b5e2b7a45ab065bb78f1609995d2c79240",\\\\n-                "sha256:72c99e39d005b793fb7d3d4e660aed6b6281b502e8c1eaf8ee8346023c8e03bc",\\\\n-                "sha256:78751b33595f7f9511952e7e60ce858c6d64db2e062afb325985ddbd34b5c131",\\\\n-                "sha256:792e69140828babe9649de583e1a03a0f2ff39918a71782c76b3c683a67c6dfd",\\\\n-                "sha256:834ee27348c4aefc20b479335fd422a2c69db55f7d9ab61721ac8cd83eb78882",\\\\n-                "sha256:8be8d84b7d4f2ba4ffff3665bcd0211318aa632395a1a41553250484a871d454",\\\\n-                "sha256:950a199911a8d94683a6b10321f9345d5a3a8433ec58b217ace979e18f16e248",\\\\n-                "sha256:a357fd4f15ee49b4a98b44ec23a34a95f1e00292a139d6015c11f55774ef10de",\\\\n-                "sha256:a53d27d0c2a0ebd07e395e56a1fbdf75ffedc4a05943daf472af163413ce9598",\\\\n-                "sha256:acef3d59d47dd85ecf909c359d0fd2c81ed33bdff70216d3956b463e12c38a54",\\\\n-                "sha256:b38694dcdac990a743aa654037ff1188c7a9801ac3ccc548d3341014bc5ca278",\\\\n-                "sha256:b9edd0110a77fc321ab090aaa1cfcaba1d8499850a12848b81be2222eab648f6",\\\\n-                "sha256:c08e95114951dc2090c4a630c2385bef681cacf12636fb0241accdc6b303fd81",\\\\n-                "sha256:c5518d51a0735b1e6cee1fdce66359f8d2b59c3ca85dc2b0813a8aa86818a030",\\\\n-                "sha256:c8fd0f1ae9d92b42854b2979024d7597685ce4ada367172ed7c09edf2cef9cb8",\\\\n-                "sha256:ca3820eb7f7faf7f0aa88de0e54681bddcb46e485beb844fcecbcd1c8bd01689",\\\\n-                "sha256:cf8b574c7b9aa060c62116d4181f3a1a4e821b2ec5cbfe3775809474113748d4",\\\\n-                "sha256:d3155d828dec1d43283bd24d3d3e0d9c7c350cdfcc0bd06c0ad1209c1bbc36d0",\\\\n-                "sha256:d6563ccd46b645e966b400bb8a95d3457ca6cf3bba1e908f9e0927901dfebeb1",\\\\n-                "sha256:ef6eefcf3944e75508cdfa513c06cf80bafd7d179e14c1334ebdca9ebb8c2c66",\\\\n-                "sha256:f8d6f8db88049a699817fd9178782867bf22283e3813064302ac59f61d95be05",\\\\n-                "sha256:fd34fbbfbc40628200730bc1febe30631347103fc8d3d4fa012c21ab9c11eca9"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==1.3.1"\\\\n-        },\\\\n-        "markdown": {\\\\n-            "hashes": [\\\\n-                "sha256:cbb516f16218e643d8e0a95b309f77eb118cb138d39a4f27851e6a63581db874",\\\\n-                "sha256:f5da449a6e1c989a4cea2631aa8ee67caa5a2ef855d551c88f9e309f4634c621"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==3.3.7"\\\\n-        },\\\\n-        "markupsafe": {\\\\n-            "hashes": [\\\\n-                "sha256:01a9b8ea66f1658938f65b93a85ebe8bc016e6769611be228d797c9d998dd298",\\\\n-                "sha256:023cb26ec21ece8dc3907c0e8320058b2e0cb3c55cf9564da612bc325bed5e64",\\\\n-                "sha256:0446679737af14f45767963a1a9ef7620189912317d095f2d9ffa183a4d25d2b",\\\\n-                "sha256:04635854b943835a6ea959e948d19dcd311762c5c0c6e1f0e16ee57022669194",\\\\n-                "sha256:0717a7390a68be14b8c793ba258e075c6f4ca819f15edfc2a3a027c823718567",\\\\n-                "sha256:0955295dd5eec6cb6cc2fe1698f4c6d84af2e92de33fbcac4111913cd100a6ff",\\\\n-                "sha256:0d4b31cc67ab36e3392bbf3862cfbadac3db12bdd8b02a2731f509ed5b829724",\\\\n-                "sha256:10f82115e21dc0dfec9ab5c0223652f7197feb168c940f3ef61563fc2d6beb74",\\\\n-                "sha256:168cd0a3642de83558a5153c8bd34f175a9a6e7f6dc6384b9655d2697312a646",\\\\n-                "sha256:1d609f577dc6e1aa17d746f8bd3c31aa4d258f4070d61b2aa5c4166c1539de35",\\\\n-                "sha256:1f2ade76b9903f39aa442b4aadd2177decb66525062db244b35d71d0ee8599b6",\\\\n-                "sha256:20dca64a3ef2d6e4d5d615a3fd418ad3bde77a47ec8a23d984a12b5b4c74491a",\\\\n-                "sha256:2a7d351cbd8cfeb19ca00de495e224dea7e7d919659c2841bbb7f420ad03e2d6",\\\\n-                "sha256:2d7d807855b419fc2ed3e631034685db6079889a1f01d5d9dac950f764da3dad",\\\\n-                "sha256:2ef54abee730b502252bcdf31b10dacb0a416229b72c18b19e24a4509f273d26",\\\\n-                "sha256:36bc903cbb393720fad60fc28c10de6acf10dc6cc883f3e24ee4012371399a38",\\\\n-                "sha256:37205cac2a79194e3750b0af2a5720d95f786a55ce7df90c3af697bfa100eaac",\\\\n-                "sha256:3c112550557578c26af18a1ccc9e090bfe03832ae994343cfdacd287db6a6ae7",\\\\n-                "sha256:3dd007d54ee88b46be476e293f48c85048603f5f516008bee124ddd891398ed6",\\\\n-                "sha256:4296f2b1ce8c86a6aea78613c34bb1a672ea0e3de9c6ba08a960efe0b0a09047",\\\\n-                "sha256:47ab1e7b91c098ab893b828deafa1203de86d0bc6ab587b160f78fe6c4011f75",\\\\n-                "sha256:49e3ceeabbfb9d66c3aef5af3a60cc43b85c33df25ce03d0031a608b0a8b2e3f",\\\\n-                "sha256:4dc8f9fb58f7364b63fd9f85013b780ef83c11857ae79f2feda41e270468dd9b",\\\\n-                "sha256:4efca8f86c54b22348a5467704e3fec767b2db12fc39c6d963168ab1d3fc9135",\\\\n-                "sha256:53edb4da6925ad13c07b6d26c2a852bd81e364f95301c66e930ab2aef5b5ddd8",\\\\n-                "sha256:5855f8438a7d1d458206a2466bf82b0f104a3724bf96a1c781ab731e4201731a",\\\\n-                "sha256:594c67807fb16238b30c44bdf74f36c02cdf22d1c8cda91ef8a0ed8dabf5620a",\\\\n-                "sha256:5b6d930f030f8ed98e3e6c98ffa0652bdb82601e7a016ec2ab5d7ff23baa78d1",\\\\n-                "sha256:5bb28c636d87e840583ee3adeb78172efc47c8b26127267f54a9c0ec251d41a9",\\\\n-                "sha256:60bf42e36abfaf9aff1f50f52644b336d4f0a3fd6d8a60ca0d054ac9f713a864",\\\\n-                "sha256:611d1ad9a4288cf3e3c16014564df047fe08410e628f89805e475368bd304914",\\\\n-                "sha256:6300b8454aa6930a24b9618fbb54b5a68135092bc666f7b06901f897fa5c2fee",\\\\n-                "sha256:63f3268ba69ace99cab4e3e3b5840b03340efed0948ab8f78d2fd87ee5442a4f",\\\\n-                "sha256:6557b31b5e2c9ddf0de32a691f2312a32f77cd7681d8af66c2692efdbef84c18",\\\\n-                "sha256:693ce3f9e70a6cf7d2fb9e6c9d8b204b6b39897a2c4a1aa65728d5ac97dcc1d8",\\\\n-                "sha256:6a7fae0dd14cf60ad5ff42baa2e95727c3d81ded453457771d02b7d2b3f9c0c2",\\\\n-                "sha256:6c4ca60fa24e85fe25b912b01e62cb969d69a23a5d5867682dd3e80b5b02581d",\\\\n-                "sha256:6fcf051089389abe060c9cd7caa212c707e58153afa2c649f00346ce6d260f1b",\\\\n-                "sha256:7d91275b0245b1da4d4cfa07e0faedd5b0812efc15b702576d103293e252af1b",\\\\n-                "sha256:89c687013cb1cd489a0f0ac24febe8c7a666e6e221b783e53ac50ebf68e45d86",\\\\n-                "sha256:8d206346619592c6200148b01a2142798c989edcb9c896f9ac9722a99d4e77e6",\\\\n-                "sha256:905fec760bd2fa1388bb5b489ee8ee5f7291d692638ea5f67982d968366bef9f",\\\\n-                "sha256:97383d78eb34da7e1fa37dd273c20ad4320929af65d156e35a5e2d89566d9dfb",\\\\n-                "sha256:984d76483eb32f1bcb536dc27e4ad56bba4baa70be32fa87152832cdd9db0833",\\\\n-                "sha256:99df47edb6bda1249d3e80fdabb1dab8c08ef3975f69aed437cb69d0a5de1e28",\\\\n-                "sha256:9f02365d4e99430a12647f09b6cc8bab61a6564363f313126f775eb4f6ef798e",\\\\n-                "sha256:a30e67a65b53ea0a5e62fe23682cfe22712e01f453b95233b25502f7c61cb415",\\\\n-                "sha256:ab3ef638ace319fa26553db0624c4699e31a28bb2a835c5faca8f8acf6a5a902",\\\\n-                "sha256:aca6377c0cb8a8253e493c6b451565ac77e98c2951c45f913e0b52facdcff83f",\\\\n-                "sha256:add36cb2dbb8b736611303cd3bfcee00afd96471b09cda130da3581cbdc56a6d",\\\\n-                "sha256:b2f4bf27480f5e5e8ce285a8c8fd176c0b03e93dcc6646477d4630e83440c6a9",\\\\n-                "sha256:b7f2d075102dc8c794cbde1947378051c4e5180d52d276987b8d28a3bd58c17d",\\\\n-                "sha256:baa1a4e8f868845af802979fcdbf0bb11f94f1cb7ced4c4b8a351bb60d108145",\\\\n-                "sha256:be98f628055368795d818ebf93da628541e10b75b41c559fdf36d104c5787066",\\\\n-                "sha256:bf5d821ffabf0ef3533c39c518f3357b171a1651c1ff6827325e4489b0e46c3c",\\\\n-                "sha256:c47adbc92fc1bb2b3274c4b3a43ae0e4573d9fbff4f54cd484555edbf030baf1",\\\\n-                "sha256:cdfba22ea2f0029c9261a4bd07e830a8da012291fbe44dc794e488b6c9bb353a",\\\\n-                "sha256:d6c7ebd4e944c85e2c3421e612a7057a2f48d478d79e61800d81468a8d842207",\\\\n-                "sha256:d7f9850398e85aba693bb640262d3611788b1f29a79f0c93c565694658f4071f",\\\\n-                "sha256:d8446c54dc28c01e5a2dbac5a25f071f6653e6e40f3a8818e8b45d790fe6ef53",\\\\n-                "sha256:deb993cacb280823246a026e3b2d81c493c53de6acfd5e6bfe31ab3402bb37dd",\\\\n-                "sha256:e0f138900af21926a02425cf736db95be9f4af72ba1bb21453432a07f6082134",\\\\n-                "sha256:e9936f0b261d4df76ad22f8fee3ae83b60d7c3e871292cd42f40b81b70afae85",\\\\n-                "sha256:f0567c4dc99f264f49fe27da5f735f414c4e7e7dd850cfd8e69f0862d7c74ea9",\\\\n-                "sha256:f5653a225f31e113b152e56f154ccbe59eeb1c7487b39b9d9f9cdb58e6c79dc5",\\\\n-                "sha256:f826e31d18b516f653fe296d967d700fddad5901ae07c622bb3705955e1faa94",\\\\n-                "sha256:f8ba0e8349a38d3001fae7eadded3f6606f0da5d748ee53cc1dab1d6527b9509",\\\\n-                "sha256:f9081981fe268bd86831e5c75f7de206ef275defcb82bc70740ae6dc507aee51",\\\\n-                "sha256:fa130dd50c57d53368c9d59395cb5526eda596d3ffe36666cd81a44d56e48872"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==2.0.1"\\\\n-        },\\\\n-        "matplotlib": {\\\\n-            "hashes": [\\\\n-                "sha256:1de0bb6cbfe460725f0e97b88daa8643bcf9571c18ba90bb8e41432aaeca91d6",\\\\n-                "sha256:1e850163579a8936eede29fad41e202b25923a0a8d5ffd08ce50fc0a97dcdc93",\\\\n-                "sha256:215e2a30a2090221a9481db58b770ce56b8ef46f13224ae33afe221b14b24dc1",\\\\n-                "sha256:348e6032f666ffd151b323342f9278b16b95d4a75dfacae84a11d2829a7816ae",\\\\n-                "sha256:3d2eb9c1cc254d0ffa90bc96fde4b6005d09c2228f99dfd493a4219c1af99644",\\\\n-                "sha256:3e477db76c22929e4c6876c44f88d790aacdf3c3f8f3a90cb1975c0bf37825b0",\\\\n-                "sha256:451cc89cb33d6652c509fc6b588dc51c41d7246afdcc29b8624e256b7663ed1f",\\\\n-                "sha256:46b1a60a04e6d884f0250d5cc8dc7bd21a9a96c584a7acdaab44698a44710bab",\\\\n-                "sha256:5f571b92a536206f7958f7cb2d367ff6c9a1fa8229dc35020006e4cdd1ca0acd",\\\\n-                "sha256:672960dd114e342b7c610bf32fb99d14227f29919894388b41553217457ba7ef",\\\\n-                "sha256:7310e353a4a35477c7f032409966920197d7df3e757c7624fd842f3eeb307d3d",\\\\n-                "sha256:746a1df55749629e26af7f977ea426817ca9370ad1569436608dc48d1069b87c",\\\\n-                "sha256:7c155437ae4fd366e2700e2716564d1787700687443de46bcb895fe0f84b761d",\\\\n-                "sha256:9265ae0fb35e29f9b8cc86c2ab0a2e3dcddc4dd9de4b85bf26c0f63fe5c1c2ca",\\\\n-                "sha256:94bdd1d55c20e764d8aea9d471d2ae7a7b2c84445e0fa463f02e20f9730783e1",\\\\n-                "sha256:9a79e5dd7bb797aa611048f5b70588b23c5be05b63eefd8a0d152ac77c4243db",\\\\n-                "sha256:a17f0a10604fac7627ec82820439e7db611722e80c408a726cd00d8c974c2fb3",\\\\n-                "sha256:a1acb72f095f1d58ecc2538ed1b8bca0b57df313b13db36ed34b8cdf1868e674",\\\\n-                "sha256:aa49571d8030ad0b9ac39708ee77bd2a22f87815e12bdee52ecaffece9313ed8",\\\\n-                "sha256:c24c05f645aef776e8b8931cb81e0f1632d229b42b6d216e30836e2e145a2b40",\\\\n-                "sha256:cf3a7e54eff792f0815dbbe9b85df2f13d739289c93d346925554f71d484be78",\\\\n-                "sha256:d738acfdfb65da34c91acbdb56abed46803db39af259b7f194dc96920360dbe4",\\\\n-                "sha256:e15fa23d844d54e7b3b7243afd53b7567ee71c721f592deb0727ee85e668f96a",\\\\n-                "sha256:ed4a9e6dcacba56b17a0a9ac22ae2c72a35b7f0ef0693aa68574f0b2df607a89",\\\\n-                "sha256:f44149a0ef5b4991aaef12a93b8e8d66d6412e762745fea1faa61d98524e0ba9"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==3.3.4"\\\\n-        },\\\\n-        "mtcnn-pytorch": {\\\\n-            "hashes": [\\\\n-                "sha256:ab2c2f5721afe11d961892f1b34aa508424f114c21aad031cc0c7ea3ceb15ad5"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.0.2"\\\\n-        },\\\\n-        "mxnet": {\\\\n-            "hashes": [\\\\n-                "sha256:1a1439cf9c6d4001755fc87816e67b0840dba15e711de01e818dd200714316b0",\\\\n-                "sha256:272435960a316c66c24f8774179344294618357be67d3ced8b3c87adbf41e121",\\\\n-                "sha256:4a34868f8f73eb90db4404b449fac8ccda53a3bc938276a43ab196a0ed35e7a1",\\\\n-                "sha256:a6f1d2c381977dc2594b880eb43470447438f6a31321e76142831168821c30d0",\\\\n-                "sha256:c223c63916a2bd843df32a3f1cc1aecd668c5c85dd2de7cbe166d99a4334cff9",\\\\n-                "sha256:e6049408be3ac915674ef52e9490ffce0c5e04fd82e550eb618a0f0a75a1b96d"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.5.0"\\\\n-        },\\\\n-        "mxnet-mkl": {\\\\n-            "hashes": [\\\\n-                "sha256:0ce9bf4a35312a6ef2f8276370969dea2317423487a170a90cd742cb5d9f5f4c",\\\\n-                "sha256:0dfd7eb60332858fe3f0d79542d6c38fb8c18c28bc5265de2b84ee00f74c8c80",\\\\n-                "sha256:723318d28d07afaedc685e059b045d37bf6d35134b157f36dd16c6830b94dae2",\\\\n-                "sha256:7d9a5bdff9cefb6435460da5e90dbf7072ac0d5fb877c8fb7d65c7618cec7ad7",\\\\n-                "sha256:91c02710ca76921ef1a364a8b309dd2b4bd5a0909332bad9e42f8089b95451c8",\\\\n-                "sha256:cab8fb56c6bd952b30866575b3632dc9c7a8d22da273017401745b2d01524d49"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.5.0"\\\\n-        },\\\\n-        "numpy": {\\\\n-            "hashes": [\\\\n-                "sha256:0cdbbaa30ae69281b18dd995d3079c4e552ad6d5426977f66b9a2a95f11f552a",\\\\n-                "sha256:2b0cca1049bd39d1879fa4d598624cafe82d35529c72de1b3d528d68031cdd95",\\\\n-                "sha256:31d3fe5b673e99d33d70cfee2ea8fe8dccd60f265c3ed990873a88647e3dd288",\\\\n-                "sha256:34dd4922aab246c39bf5df03ca653d6265e65971deca6784c956bf356bca6197",\\\\n-                "sha256:384e2dfa03da7c8d54f8f934f61b6a5e4e1ebb56a65b287567629d6c14578003",\\\\n-                "sha256:392e2ea22b41a22c0289a88053204b616181288162ba78e6823e1760309d5277",\\\\n-                "sha256:4341a39fc085f31a583be505eabf00e17c619b469fef78dc7e8241385bfddaa4",\\\\n-                "sha256:45080f065dcaa573ebecbfe13cdd86e8c0a68c4e999aa06bd365374ea7137706",\\\\n-                "sha256:485cb1eb4c9962f4cd042fed9424482ec1d83fee5dc2ef3f2552ac47852cb259",\\\\n-                "sha256:575cefd28d3e0da85b0864506ae26b06483ee4a906e308be5a7ad11083f9d757",\\\\n-                "sha256:62784b35df7de7ca4d0d81c5b6af5983f48c5cdef32fc3635b445674e56e3266",\\\\n-                "sha256:69c152f7c11bf3b4fc11bc4cc62eb0334371c0db6844ebace43b7c815b602805",\\\\n-                "sha256:6ccfdcefd287f252cf1ea7a3f1656070da330c4a5658e43ad223269165cdf977",\\\\n-                "sha256:7298fbd73c0b3eff1d53dc9b9bdb7add8797bb55eeee38c8ccd7906755ba28af",\\\\n-                "sha256:79463d918d1bf3aeb9186e3df17ddb0baca443f41371df422f99ee94f4f2bbfe",\\\\n-                "sha256:8bbee788d82c0ac656536de70e817af09b7694f5326b0ef08e5c1014fcb96bb3",\\\\n-                "sha256:a863957192855c4c57f60a75a1ac06ce5362ad18506d362dd807e194b4baf3ce",\\\\n-                "sha256:ae602ba425fb2b074e16d125cdce4f0194903da935b2e7fe284ebecca6d92e76",\\\\n-                "sha256:b13faa258b20fa66d29011f99fdf498641ca74a0a6d9266bc27d83c70fea4a6a",\\\\n-                "sha256:c2c39d69266621dd7464e2bb740d6eb5abc64ddc339cc97aa669f3bb4d75c103",\\\\n-                "sha256:e9c88f173d31909d881a60f08a8494e63f1aff2a4052476b24d4f50e82c47e24",\\\\n-                "sha256:f1a29267ac29fff0913de0f11f3a9edfcd3f39595f467026c29376fad243ebe3",\\\\n-                "sha256:f69dde0c5a137d887676a8129373e44366055cf19d1b434e853310c7a1e68f93"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.16.1"\\\\n-        },\\\\n-        "opencv-python": {\\\\n-            "hashes": [\\\\n-                "sha256:0dc82a3d8630c099d2f3ac1b1aabee164e8188db54a786abb7a4e27eba309440",\\\\n-                "sha256:5af8ba35a4fcb8913ffb86e92403e9a656a4bff4a645d196987468f0f8947875",\\\\n-                "sha256:6e32af22e3202748bd233ed8f538741876191863882eba44e332d1a34993165b",\\\\n-                "sha256:c5bfae41ad4031e66bb10ec4a0a2ffd3e514d092652781e8b1ac98d1b59f1158",\\\\n-                "sha256:dbdc84a9b4ea2cbae33861652d25093944b9959279200b7ae0badd32439f74de",\\\\n-                "sha256:e6e448b62afc95c5b58f97e87ef84699e6607fe5c58730a03301c52496005cae",\\\\n-                "sha256:f482e78de6e7b0b060ff994ffd859bddc3f7f382bb2019ef157b0ea8ca8712f5"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==4.6.0.66"\\\\n-        },\\\\n-        "opt-einsum": {\\\\n-            "hashes": [\\\\n-                "sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147",\\\\n-                "sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.5\\\\\\\'",\\\\n-            "version": "==3.3.0"\\\\n-        },\\\\n-        "pillow": {\\\\n-            "hashes": [\\\\n-                "sha256:066f3999cb3b070a95c3652712cffa1a748cd02d60ad7b4e485c3748a04d9d76",\\\\n-                "sha256:0a0956fdc5defc34462bb1c765ee88d933239f9a94bc37d132004775241a7585",\\\\n-                "sha256:0b052a619a8bfcf26bd8b3f48f45283f9e977890263e4571f2393ed8898d331b",\\\\n-                "sha256:1394a6ad5abc838c5cd8a92c5a07535648cdf6d09e8e2d6df916dfa9ea86ead8",\\\\n-                "sha256:1bc723b434fbc4ab50bb68e11e93ce5fb69866ad621e3c2c9bdb0cd70e345f55",\\\\n-                "sha256:244cf3b97802c34c41905d22810846802a3329ddcb93ccc432870243211c79fc",\\\\n-                "sha256:25a49dc2e2f74e65efaa32b153527fc5ac98508d502fa46e74fa4fd678ed6645",\\\\n-                "sha256:2e4440b8f00f504ee4b53fe30f4e381aae30b0568193be305256b1462216feff",\\\\n-                "sha256:3862b7256046fcd950618ed22d1d60b842e3a40a48236a5498746f21189afbbc",\\\\n-                "sha256:3eb1ce5f65908556c2d8685a8f0a6e989d887ec4057326f6c22b24e8a172c66b",\\\\n-                "sha256:3f97cfb1e5a392d75dd8b9fd274d205404729923840ca94ca45a0af57e13dbe6",\\\\n-                "sha256:493cb4e415f44cd601fcec11c99836f707bb714ab03f5ed46ac25713baf0ff20",\\\\n-                "sha256:4acc0985ddf39d1bc969a9220b51d94ed51695d455c228d8ac29fcdb25810e6e",\\\\n-                "sha256:5503c86916d27c2e101b7f71c2ae2cddba01a2cf55b8395b0255fd33fa4d1f1a",\\\\n-                "sha256:5b7bb9de00197fb4261825c15551adf7605cf14a80badf1761d61e59da347779",\\\\n-                "sha256:5e9ac5f66616b87d4da618a20ab0a38324dbe88d8a39b55be8964eb520021e02",\\\\n-                "sha256:620582db2a85b2df5f8a82ddeb52116560d7e5e6b055095f04ad828d1b0baa39",\\\\n-                "sha256:62cc1afda735a8d109007164714e73771b499768b9bb5afcbbee9d0ff374b43f",\\\\n-                "sha256:70ad9e5c6cb9b8487280a02c0ad8a51581dcbbe8484ce058477692a27c151c0a",\\\\n-                "sha256:72b9e656e340447f827885b8d7a15fc8c4e68d410dc2297ef6787eec0f0ea409",\\\\n-                "sha256:72cbcfd54df6caf85cc35264c77ede902452d6df41166010262374155947460c",\\\\n-                "sha256:792e5c12376594bfcb986ebf3855aa4b7c225754e9a9521298e460e92fb4a488",\\\\n-                "sha256:7b7017b61bbcdd7f6363aeceb881e23c46583739cb69a3ab39cb384f6ec82e5b",\\\\n-                "sha256:81f8d5c81e483a9442d72d182e1fb6dcb9723f289a57e8030811bac9ea3fef8d",\\\\n-                "sha256:82aafa8d5eb68c8463b6e9baeb4f19043bb31fefc03eb7b216b51e6a9981ae09",\\\\n-                "sha256:84c471a734240653a0ec91dec0996696eea227eafe72a33bd06c92697728046b",\\\\n-                "sha256:8c803ac3c28bbc53763e6825746f05cc407b20e4a69d0122e526a582e3b5e153",\\\\n-                "sha256:93ce9e955cc95959df98505e4608ad98281fff037350d8c2671c9aa86bcf10a9",\\\\n-                "sha256:9a3e5ddc44c14042f0844b8cf7d2cd455f6cc80fd7f5eefbe657292cf601d9ad",\\\\n-                "sha256:a4901622493f88b1a29bd30ec1a2f683782e57c3c16a2dbc7f2595ba01f639df",\\\\n-                "sha256:a5a4532a12314149d8b4e4ad8ff09dde7427731fcfa5917ff16d0291f13609df",\\\\n-                "sha256:b8831cb7332eda5dc89b21a7bce7ef6ad305548820595033a4b03cf3091235ed",\\\\n-                "sha256:b8e2f83c56e141920c39464b852de3719dfbfb6e3c99a2d8da0edf4fb33176ed",\\\\n-                "sha256:c70e94281588ef053ae8998039610dbd71bc509e4acbc77ab59d7d2937b10698",\\\\n-                "sha256:c8a17b5d948f4ceeceb66384727dde11b240736fddeda54ca740b9b8b1556b29",\\\\n-                "sha256:d82cdb63100ef5eedb8391732375e6d05993b765f72cb34311fab92103314649",\\\\n-                "sha256:d89363f02658e253dbd171f7c3716a5d340a24ee82d38aab9183f7fdf0cdca49",\\\\n-                "sha256:d99ec152570e4196772e7a8e4ba5320d2d27bf22fdf11743dd882936ed64305b",\\\\n-                "sha256:ddc4d832a0f0b4c52fff973a0d44b6c99839a9d016fe4e6a1cb8f3eea96479c2",\\\\n-                "sha256:e3dacecfbeec9a33e932f00c6cd7996e62f53ad46fbe677577394aaa90ee419a",\\\\n-                "sha256:eb9fc393f3c61f9054e1ed26e6fe912c7321af2f41ff49d3f83d05bacf22cc78"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==8.4.0"\\\\n-        },\\\\n-        "protobuf": {\\\\n-            "hashes": [\\\\n-                "sha256:010be24d5a44be7b0613750ab40bc8b8cedc796db468eae6c779b395f50d1fa1",\\\\n-                "sha256:0469bc66160180165e4e29de7f445e57a34ab68f49357392c5b2f54c656ab25e",\\\\n-                "sha256:0c0714b025ec057b5a7600cb66ce7c693815f897cfda6d6efb58201c472e3437",\\\\n-                "sha256:11478547958c2dfea921920617eb457bc26867b0d1aa065ab05f35080c5d9eb6",\\\\n-                "sha256:14082457dc02be946f60b15aad35e9f5c69e738f80ebbc0900a19bc83734a5a4",\\\\n-                "sha256:2b2d2913bcda0e0ec9a784d194bc490f5dc3d9d71d322d070b11a0ade32ff6ba",\\\\n-                "sha256:30a15015d86b9c3b8d6bf78d5b8c7749f2512c29f168ca259c9d7727604d0e39",\\\\n-                "sha256:30f5370d50295b246eaa0296533403961f7e64b03ea12265d6dfce3a391d8992",\\\\n-                "sha256:347b393d4dd06fb93a77620781e11c058b3b0a5289262f094379ada2920a3730",\\\\n-                "sha256:4bc98de3cdccfb5cd769620d5785b92c662b6bfad03a202b83799b6ed3fa1fa7",\\\\n-                "sha256:5057c64052a1f1dd7d4450e9aac25af6bf36cfbfb3a1cd89d16393a036c49157",\\\\n-                "sha256:559670e006e3173308c9254d63facb2c03865818f22204037ab76f7a0ff70b5f",\\\\n-                "sha256:5a0d7539a1b1fb7e76bf5faa0b44b30f812758e989e59c40f77a7dab320e79b9",\\\\n-                "sha256:5f5540d57a43042389e87661c6eaa50f47c19c6176e8cf1c4f287aeefeccb5c4",\\\\n-                "sha256:7a552af4dc34793803f4e735aabe97ffc45962dfd3a237bdde242bff5a3de684",\\\\n-                "sha256:84a04134866861b11556a82dd91ea6daf1f4925746b992f277b84013a7cc1229",\\\\n-                "sha256:878b4cd080a21ddda6ac6d1e163403ec6eea2e206cf225982ae04567d39be7b0",\\\\n-                "sha256:90b0d02163c4e67279ddb6dc25e063db0130fc299aefabb5d481053509fae5c8",\\\\n-                "sha256:91d5f1e139ff92c37e0ff07f391101df77e55ebb97f46bbc1535298d72019462",\\\\n-                "sha256:a8ce5ae0de28b51dff886fb922012dad885e66176663950cb2344c0439ecb473",\\\\n-                "sha256:aa3b82ca1f24ab5326dcf4ea00fcbda703e986b22f3d27541654f749564d778b",\\\\n-                "sha256:bb6776bd18f01ffe9920e78e03a8676530a5d6c5911934c6a1ac6eb78973ecb6",\\\\n-                "sha256:bbf5cea5048272e1c60d235c7bd12ce1b14b8a16e76917f371c718bd3005f045",\\\\n-                "sha256:c0ccd3f940fe7f3b35a261b1dd1b4fc850c8fde9f74207015431f174be5976b3",\\\\n-                "sha256:d0b635cefebd7a8a0f92020562dead912f81f401af7e71f16bf9506ff3bdbb38"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.5\\\\\\\'",\\\\n-            "version": "==3.19.6"\\\\n-        },\\\\n-        "pycparser": {\\\\n-            "hashes": [\\\\n-                "sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9",\\\\n-                "sha256:e644fdec12f7872f86c58ff790da456218b10f863970249516d60a5eaca77206"\\\\n-            ],\\\\n-            "version": "==2.21"\\\\n-        },\\\\n-        "pyparsing": {\\\\n-            "hashes": [\\\\n-                "sha256:2b020ecf7d21b687f219b71ecad3631f644a47f01403fa1d1036b0c6416d70fb",\\\\n-                "sha256:5026bae9a10eeaefb61dab2f09052b9f4307d44aee4eda64b309723d8d206bbc"\\\\n-            ],\\\\n-            "markers": "python_full_version >= \\\\\\\'3.6.8\\\\\\\'",\\\\n-            "version": "==3.0.9"\\\\n-        },\\\\n-        "python-dateutil": {\\\\n-            "hashes": [\\\\n-                "sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86",\\\\n-                "sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3\\\\\\\'",\\\\n-            "version": "==2.8.2"\\\\n-        },\\\\n-        "requests": {\\\\n-            "hashes": [\\\\n-                "sha256:6a1b267aa90cac58ac3a765d067950e7dbbf75b1da07e895d1f594193a40a38b",\\\\n-                "sha256:9c443e7324ba5b85070c4a818ade28bfabedf16ea10206da1132edaa6dda237e"\\\\n-            ],\\\\n-            "version": "==2.18.4"\\\\n-        },\\\\n-        "scikit-learn": {\\\\n-            "hashes": [\\\\n-                "sha256:1ac81293d261747c25ea5a0ee8cd2bb1f3b5ba9ec05421a7f9f0feb4eb7c4116",\\\\n-                "sha256:289361cf003d90b007f5066b27fcddc2d71324c82f1c88e316fedacb0dfdd516",\\\\n-                "sha256:3a14d0abd4281fc3fd2149c486c3ec7cedad848b8d5f7b6f61522029d65a29f8",\\\\n-                "sha256:5083a5e50d9d54548e4ada829598ae63a05651dd2bb319f821ffd9e8388384a6",\\\\n-                "sha256:777cdd5c077b7ca9cb381396c81990cf41d2fa8350760d3cad3b4c460a7db644",\\\\n-                "sha256:8bf2ff63da820d09b96b18e88f9625228457bff8df4618f6b087e12442ef9e15",\\\\n-                "sha256:8d319b71c449627d178f21c57614e21747e54bb3fc9602b6f42906c3931aa320",\\\\n-                "sha256:928050b65781fea9542dfe9bfe02d8c4f5530baa8472ec60782ea77347d2c836",\\\\n-                "sha256:92c903613ff50e22aa95d589f9fff5deb6f34e79f7f21f609680087f137bb524",\\\\n-                "sha256:ae322235def5ce8fae645b439e332e6f25d34bb90d6a6c8e261f17eb476457b7",\\\\n-                "sha256:c1cd6b29eb1fd1cc672ac5e4a8be5f6ea936d094a3dc659ada0746d6fac750b1",\\\\n-                "sha256:c41a6e2685d06bcdb0d26533af2540f54884d40db7e48baed6a5bcbf1a7cc642",\\\\n-                "sha256:d07fcb0c0acbc043faa0e7cf4d2037f71193de3fb04fb8ed5c259b089af1cf5c",\\\\n-                "sha256:d146d5443cda0a41f74276e42faf8c7f283fef49e8a853b832885239ef544e05",\\\\n-                "sha256:eb2b7bed0a26ba5ce3700e15938b28a4f4513578d3e54a2156c29df19ac5fd01",\\\\n-                "sha256:eb9b8ebf59eddd8b96366428238ab27d05a19e89c5516ce294abc35cea75d003"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==0.21.3"\\\\n-        },\\\\n-        "scipy": {\\\\n-            "hashes": [\\\\n-                "sha256:0611ee97296265af4a21164a5323f8c1b4e8e15c582d3dfa7610825900136bb7",\\\\n-                "sha256:08237eda23fd8e4e54838258b124f1cd141379a5f281b0a234ca99b38918c07a",\\\\n-                "sha256:0e645dbfc03f279e1946cf07c9c754c2a1859cb4a41c5f70b25f6b3a586b6dbd",\\\\n-                "sha256:0e9bb7efe5f051ea7212555b290e784b82f21ffd0f655405ac4f87e288b730b3",\\\\n-                "sha256:108c16640849e5827e7d51023efb3bd79244098c3f21e4897a1007720cb7ce37",\\\\n-                "sha256:340ef70f5b0f4e2b4b43c8c8061165911bc6b2ad16f8de85d9774545e2c47463",\\\\n-                "sha256:3ad73dfc6f82e494195144bd3a129c7241e761179b7cb5c07b9a0ede99c686f3",\\\\n-                "sha256:3b243c77a822cd034dad53058d7c2abf80062aa6f4a32e9799c95d6391558631",\\\\n-                "sha256:404a00314e85eca9d46b80929571b938e97a143b4f2ddc2b2b3c91a4c4ead9c5",\\\\n-                "sha256:423b3ff76957d29d1cce1bc0d62ebaf9a3fdfaf62344e3fdec14619bb7b5ad3a",\\\\n-                "sha256:42d9149a2fff7affdd352d157fa5717033767857c11bd55aa4a519a44343dfef",\\\\n-                "sha256:625f25a6b7d795e8830cb70439453c9f163e6870e710ec99eba5722775b318f3",\\\\n-                "sha256:698c6409da58686f2df3d6f815491fd5b4c2de6817a45379517c92366eea208f",\\\\n-                "sha256:729f8f8363d32cebcb946de278324ab43d28096f36593be6281ca1ee86ce6559",\\\\n-                "sha256:8190770146a4c8ed5d330d5b5ad1c76251c63349d25c96b3094875b930c44692",\\\\n-                "sha256:878352408424dffaa695ffedf2f9f92844e116686923ed9aa8626fc30d32cfd1",\\\\n-                "sha256:8b984f0821577d889f3c7ca8445564175fb4ac7c7f9659b7c60bef95b2b70e76",\\\\n-                "sha256:8f841bbc21d3dad2111a94c490fb0a591b8612ffea86b8e5571746ae76a3deac",\\\\n-                "sha256:c22b27371b3866c92796e5d7907e914f0e58a36d3222c5d436ddd3f0e354227a",\\\\n-                "sha256:d0cdd5658b49a722783b8b4f61a6f1f9c75042d0e29a30ccb6cacc9b25f6d9e2",\\\\n-                "sha256:d40dc7f494b06dcee0d303e51a00451b2da6119acbeaccf8369f2d29e28917ac",\\\\n-                "sha256:d8491d4784aceb1f100ddb8e31239c54e4afab8d607928a9f7ef2469ec35ae01",\\\\n-                "sha256:dfc5080c38dde3f43d8fbb9c0539a7839683475226cf83e4b24363b227dfe552",\\\\n-                "sha256:e24e22c8d98d3c704bb3410bce9b69e122a8de487ad3dbfe9985d154e5c03a40",\\\\n-                "sha256:e7a01e53163818d56eabddcafdc2090e9daba178aad05516b20c6591c4811020",\\\\n-                "sha256:ee677635393414930541a096fc8e61634304bb0153e4e02b75685b11eba14cae",\\\\n-                "sha256:f0521af1b722265d824d6ad055acfe9bd3341765735c44b5a4d0069e189a0f40",\\\\n-                "sha256:f25c281f12c0da726c6ed00535ca5d1622ec755c30a3f8eafef26cf43fede694"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.1.0"\\\\n-        },\\\\n-        "setuptools": {\\\\n-            "hashes": [\\\\n-                "sha256:22c7348c6d2976a52632c67f7ab0cdf40147db7789f9aed18734643fe9cf3373",\\\\n-                "sha256:4ce92f1e1f8f01233ee9952c04f6b81d1e02939d6e1b488428154974a4d0783e"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==59.6.0"\\\\n-        },\\\\n-        "six": {\\\\n-            "hashes": [\\\\n-                "sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926",\\\\n-                "sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3\\\\\\\'",\\\\n-            "version": "==1.16.0"\\\\n-        },\\\\n-        "tensorboard": {\\\\n-            "hashes": [\\\\n-                "sha256:4cad2c65f6011e51609b463014c014fd7c6ddd9c1263af1d4f18dd97ed88c2bc",\\\\n-                "sha256:612b789386aa1b2c4804e1961273b37f8e4dd97613f98bc90ff0402d24627f50"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3\\\\\\\'",\\\\n-            "version": "==1.15.0"\\\\n-        },\\\\n-        "tensorboardx": {\\\\n-            "hashes": [\\\\n-                "sha256:13fe0abba27f407778a7321937190eedaf12bc8c544d9a4e294fcf0ba177fd76",\\\\n-                "sha256:f52e59b38b4cdf83384f3fce067bcaf2d2847619f9f533394df0de3b5a71ab8e"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.8"\\\\n-        },\\\\n-        "tensorflow": {\\\\n-            "hashes": [\\\\n-                "sha256:0a01def34c28298970dc83776dd43877fd59e43fddd8e960d01b6eb849ba9938",\\\\n-                "sha256:1afb2e573fe666eb0dd6a45dbb7679de622a318fcc1fc401fb7819068f2d858b",\\\\n-                "sha256:20bbda24b022c7cedd569bb36a6d620b68e0ea37afc91645c918971c72881388",\\\\n-                "sha256:26b86d32f20dba79da30cb87d67a708678cc6c47f6e596c8783433e282e91f00",\\\\n-                "sha256:5910cdde20a9143760d3b49f6d03a6a09ab5221e2f871eb51dbd422b80da07fc",\\\\n-                "sha256:6265f7d86a66754b875e252effa01f7d54aa4464df3b71182d4be8b269b8a148",\\\\n-                "sha256:79fbb4a38c6e6417d5eef8ccae00b16053750c5d61092ec7caabe4cea6870dc2",\\\\n-                "sha256:8e4bb1c361a9b350497741c1d0a556d9592b5ce52c9b073d88dd053c182f3d15",\\\\n-                "sha256:a214d7bbdae4093bab9a4c582bbf9c4464274cc9db99f94bb42b88c5c961a452",\\\\n-                "sha256:af57e0e16adb4d6ccd387954c1d70e34cc4925b74da9135d2b83ca7d3dd9d102",\\\\n-                "sha256:d1694e25887bc0d0c31d6c7d9c92c8bf9e0499085c7e5a9dbaacf675c44027a8"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.15"\\\\n-        },\\\\n-        "tensorflow-estimator": {\\\\n-            "hashes": [\\\\n-                "sha256:8853bfb7c3c96fbdc80b3d66c37a10af6ccbcd235dc87474764270c02a0f86b9"\\\\n-            ],\\\\n-            "version": "==1.15.1"\\\\n-        },\\\\n-        "termcolor": {\\\\n-            "hashes": [\\\\n-                "sha256:1d6d69ce66211143803fbc56652b41d73b4a400a2891d7bf7a1cdf4c02de613b"\\\\n-            ],\\\\n-            "version": "==1.1.0"\\\\n-        },\\\\n-        "torch": {\\\\n-            "hashes": [\\\\n-                "sha256:258a0729fb77a3457d5822d84b536057cd119b08049a8d3c41dc3dcdeb48d56e",\\\\n-                "sha256:3592d3dd62b32760c82624e7586222747fe2281240e8653970b35f1d6d4a434c",\\\\n-                "sha256:376fc18407add20daa6bbaaffc5a5e06d733abe53bcbd60ef2532bfed34bc091",\\\\n-                "sha256:3eee3cf53c1f8fb3f1fe107a22025a8501fc6440d14e09599ba7153002531f84",\\\\n-                "sha256:5b68e9108bd7ebd99eee941686046c517cfaac5331f757bcf440fe02f2e3ced1",\\\\n-                "sha256:65fd02ed889c63fd82bf1a440c5a94c1310c29f3e6f9f62add416d34da355d97",\\\\n-                "sha256:6a81f886823bbd15edc2dc0908fa214070df61c9f7ab8831f0a03630275cca5a",\\\\n-                "sha256:6da1b877880435440a5aa9678ef0f01986d4886416844db1d97ebfb7fd1778d0",\\\\n-                "sha256:8f3fd2e3ffc3bb867133fdf7fbcc8a0bb2e62a5c0696396f51856f5abf9045a8",\\\\n-                "sha256:901b52787baeb2e9e1357ca7037da0028bc6ad743f530e0040ae96ef8e27156c",\\\\n-                "sha256:935e5ac804c5093c79f23a7e6ca5b912c166071aa9d8b4a0a3d6a85126d6a47b",\\\\n-                "sha256:97b7b0c667e8b0dd1fc70137a36e0a4841ec10ef850bda60500ad066bef3e2de",\\\\n-                "sha256:9ef4c004f9e5168bd1c1930c6aff25fed5b097de81db6271ffbb2e4fb8b89319",\\\\n-                "sha256:ab77a9f838874f295ed5410c0686fa22547456e0116efb281c66ef5f9d46fe28",\\\\n-                "sha256:b07ef01e36b716d0d65ca60c4db0ac9d094a0e797d9b55290da4dcda91463b6c",\\\\n-                "sha256:d43bc3f3a2d89ae185ef96d903c935c335219231e57685658648396984e2a67a",\\\\n-                "sha256:ef99b8cca5f9358119b07956915faf6e7906f433ab4a603c160ae9de88918371",\\\\n-                "sha256:f281438ee99bd72ad65c0bba1026a32e45c3b636bc067fc145ad291e9ea2faab",\\\\n-                "sha256:fbaf18c1b3e0b31af194a9d853e3739464cf982d279df9d34dd18f1c2a471878"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==1.10.2"\\\\n-        },\\\\n-        "torchvision": {\\\\n-            "hashes": [\\\\n-                "sha256:23ded32f6e6b6048f2da7b9a8933d122038d050641e385b8857ef206e0613d3b",\\\\n-                "sha256:4380da0565bd729338f965b4465367dc35a64bdf9e0654a6ae72cf9bdb46dce7",\\\\n-                "sha256:49315de81faa7e874dbcff58d188eb4d7d0327bc9bb630f2da9d7abe79583708",\\\\n-                "sha256:5f8b6acd544fca8642099c921cea590cd711ecd6b30c73a335c10a525421450e",\\\\n-                "sha256:60932ebfa4cbcdba707821ea365f7ca89cde679c087e96f947d99035132dbd7e",\\\\n-                "sha256:621b49722f97489725660ef0152a447031b21314af67b49a48ee48513f073690",\\\\n-                "sha256:65cbd363ec17a74d47d0e5237c258a98d0cd548e79455a37d9699418f9413512",\\\\n-                "sha256:7715c602d4ef7d66ae37002ccbf1d341fdafefd3331e2dabc9341483eb0bd2b2",\\\\n-                "sha256:b2edd39450c7628e46b53b370a80727aacebfce83458faa4209f6346a39ca845",\\\\n-                "sha256:c19d3d72987e6c0cbd90e37a80b51f93765c9cbc9954074b6396f8f2ea80a7d4",\\\\n-                "sha256:dd1c29ea41b6f7f12477aaf2e359781023a6e127d9a5642b6e96978b56b13674",\\\\n-                "sha256:e54e0f392c3a20206e5f1d09c7eef208e680099c242e4f86b7e51217df812f32"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==0.4.1"\\\\n-        },\\\\n-        "tqdm": {\\\\n-            "hashes": [\\\\n-                "sha256:1be3e4e3198f2d0e47b928e9d9a8ec1b63525db29095cec1467f4c5a4ea8ebf9",\\\\n-                "sha256:7e39a30e3d34a7a6539378e39d7490326253b7ee354878a92255656dc4284457"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==4.35.0"\\\\n-        },\\\\n-        "typing-extensions": {\\\\n-            "hashes": [\\\\n-                "sha256:1a9462dcc3347a79b1f1c0271fbe79e844580bb598bafa1ed208b94da3cdcd42",\\\\n-                "sha256:21c85e0fe4b9a155d0799430b0ad741cdce7e359660ccbd8b530613e8df88ce2"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==4.1.1"\\\\n-        },\\\\n-        "urllib3": {\\\\n-            "hashes": [\\\\n-                "sha256:06330f386d6e4b195fbfc736b297f58c5a892e4440e54d294d7004e3a9bbea1b",\\\\n-                "sha256:cc44da8e1145637334317feebd728bd869a35285b93cbb4cca2577da7e62db4f"\\\\n-            ],\\\\n-            "version": "==1.22"\\\\n-        },\\\\n-        "werkzeug": {\\\\n-            "hashes": [\\\\n-                "sha256:87ae4e5b5366da2347eb3116c0e6c681a0e939a33b2805e2c0cbd282664932c4",\\\\n-                "sha256:a13b74dd3c45f758d4ebdb224be8f1ab8ef58b3c0ffc1783a8c7d9f4f50227e6"\\\\n-            ],\\\\n-            "index": "pypi",\\\\n-            "version": "==0.15.5"\\\\n-        },\\\\n-        "wheel": {\\\\n-            "hashes": [\\\\n-                "sha256:4bdcd7d840138086126cd09254dc6195fb4fc6f01c050a1d7236f2630db1d22a",\\\\n-                "sha256:e9a504e793efbca1b8e0e9cb979a249cf4a0a7b5b8c9e8b65a5e39d49529c1c4"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3, 3.4\\\\\\\'",\\\\n-            "version": "==0.37.1"\\\\n-        },\\\\n-        "wrapt": {\\\\n-            "hashes": [\\\\n-                "sha256:00b6d4ea20a906c0ca56d84f93065b398ab74b927a7a3dbd470f6fc503f95dc3",\\\\n-                "sha256:01c205616a89d09827986bc4e859bcabd64f5a0662a7fe95e0d359424e0e071b",\\\\n-                "sha256:02b41b633c6261feff8ddd8d11c711df6842aba629fdd3da10249a53211a72c4",\\\\n-                "sha256:07f7a7d0f388028b2df1d916e94bbb40624c59b48ecc6cbc232546706fac74c2",\\\\n-                "sha256:11871514607b15cfeb87c547a49bca19fde402f32e2b1c24a632506c0a756656",\\\\n-                "sha256:1b376b3f4896e7930f1f772ac4b064ac12598d1c38d04907e696cc4d794b43d3",\\\\n-                "sha256:21ac0156c4b089b330b7666db40feee30a5d52634cc4560e1905d6529a3897ff",\\\\n-                "sha256:257fd78c513e0fb5cdbe058c27a0624c9884e735bbd131935fd49e9fe719d310",\\\\n-                "sha256:2b39d38039a1fdad98c87279b48bc5dce2c0ca0d73483b12cb72aa9609278e8a",\\\\n-                "sha256:2cf71233a0ed05ccdabe209c606fe0bac7379fdcf687f39b944420d2a09fdb57",\\\\n-                "sha256:2fe803deacd09a233e4762a1adcea5db5d31e6be577a43352936179d14d90069",\\\\n-                "sha256:3232822c7d98d23895ccc443bbdf57c7412c5a65996c30442ebe6ed3df335383",\\\\n-                "sha256:34aa51c45f28ba7f12accd624225e2b1e5a3a45206aa191f6f9aac931d9d56fe",\\\\n-                "sha256:36f582d0c6bc99d5f39cd3ac2a9062e57f3cf606ade29a0a0d6b323462f4dd87",\\\\n-                "sha256:380a85cf89e0e69b7cfbe2ea9f765f004ff419f34194018a6827ac0e3edfed4d",\\\\n-                "sha256:40e7bc81c9e2b2734ea4bc1aceb8a8f0ceaac7c5299bc5d69e37c44d9081d43b",\\\\n-                "sha256:43ca3bbbe97af00f49efb06e352eae40434ca9d915906f77def219b88e85d907",\\\\n-                "sha256:4fcc4649dc762cddacd193e6b55bc02edca674067f5f98166d7713b193932b7f",\\\\n-                "sha256:5a0f54ce2c092aaf439813735584b9537cad479575a09892b8352fea5e988dc0",\\\\n-                "sha256:5a9a0d155deafd9448baff28c08e150d9b24ff010e899311ddd63c45c2445e28",\\\\n-                "sha256:5b02d65b9ccf0ef6c34cba6cf5bf2aab1bb2f49c6090bafeecc9cd81ad4ea1c1",\\\\n-                "sha256:60db23fa423575eeb65ea430cee741acb7c26a1365d103f7b0f6ec412b893853",\\\\n-                "sha256:642c2e7a804fcf18c222e1060df25fc210b9c58db7c91416fb055897fc27e8cc",\\\\n-                "sha256:6a9a25751acb379b466ff6be78a315e2b439d4c94c1e99cb7266d40a537995d3",\\\\n-                "sha256:6b1a564e6cb69922c7fe3a678b9f9a3c54e72b469875aa8018f18b4d1dd1adf3",\\\\n-                "sha256:6d323e1554b3d22cfc03cd3243b5bb815a51f5249fdcbb86fda4bf62bab9e164",\\\\n-                "sha256:6e743de5e9c3d1b7185870f480587b75b1cb604832e380d64f9504a0535912d1",\\\\n-                "sha256:709fe01086a55cf79d20f741f39325018f4df051ef39fe921b1ebe780a66184c",\\\\n-                "sha256:7b7c050ae976e286906dd3f26009e117eb000fb2cf3533398c5ad9ccc86867b1",\\\\n-                "sha256:7d2872609603cb35ca513d7404a94d6d608fc13211563571117046c9d2bcc3d7",\\\\n-                "sha256:7ef58fb89674095bfc57c4069e95d7a31cfdc0939e2a579882ac7d55aadfd2a1",\\\\n-                "sha256:80bb5c256f1415f747011dc3604b59bc1f91c6e7150bd7db03b19170ee06b320",\\\\n-                "sha256:81b19725065dcb43df02b37e03278c011a09e49757287dca60c5aecdd5a0b8ed",\\\\n-                "sha256:833b58d5d0b7e5b9832869f039203389ac7cbf01765639c7309fd50ef619e0b1",\\\\n-                "sha256:88bd7b6bd70a5b6803c1abf6bca012f7ed963e58c68d76ee20b9d751c74a3248",\\\\n-                "sha256:8ad85f7f4e20964db4daadcab70b47ab05c7c1cf2a7c1e51087bfaa83831854c",\\\\n-                "sha256:8c0ce1e99116d5ab21355d8ebe53d9460366704ea38ae4d9f6933188f327b456",\\\\n-                "sha256:8d649d616e5c6a678b26d15ece345354f7c2286acd6db868e65fcc5ff7c24a77",\\\\n-                "sha256:903500616422a40a98a5a3c4ff4ed9d0066f3b4c951fa286018ecdf0750194ef",\\\\n-                "sha256:9736af4641846491aedb3c3f56b9bc5568d92b0692303b5a305301a95dfd38b1",\\\\n-                "sha256:988635d122aaf2bdcef9e795435662bcd65b02f4f4c1ae37fbee7401c440b3a7",\\\\n-                "sha256:9cca3c2cdadb362116235fdbd411735de4328c61425b0aa9f872fd76d02c4e86",\\\\n-                "sha256:9e0fd32e0148dd5dea6af5fee42beb949098564cc23211a88d799e434255a1f4",\\\\n-                "sha256:9f3e6f9e05148ff90002b884fbc2a86bd303ae847e472f44ecc06c2cd2fcdb2d",\\\\n-                "sha256:a85d2b46be66a71bedde836d9e41859879cc54a2a04fad1191eb50c2066f6e9d",\\\\n-                "sha256:a9a52172be0b5aae932bef82a79ec0a0ce87288c7d132946d645eba03f0ad8a8",\\\\n-                "sha256:aa31fdcc33fef9eb2552cbcbfee7773d5a6792c137b359e82879c101e98584c5",\\\\n-                "sha256:b014c23646a467558be7da3d6b9fa409b2c567d2110599b7cf9a0c5992b3b471",\\\\n-                "sha256:b21bb4c09ffabfa0e85e3a6b623e19b80e7acd709b9f91452b8297ace2a8ab00",\\\\n-                "sha256:b5901a312f4d14c59918c221323068fad0540e34324925c8475263841dbdfe68",\\\\n-                "sha256:b9b7a708dd92306328117d8c4b62e2194d00c365f18eff11a9b53c6f923b01e3",\\\\n-                "sha256:d1967f46ea8f2db647c786e78d8cc7e4313dbd1b0aca360592d8027b8508e24d",\\\\n-                "sha256:d52a25136894c63de15a35bc0bdc5adb4b0e173b9c0d07a2be9d3ca64a332735",\\\\n-                "sha256:d77c85fedff92cf788face9bfa3ebaa364448ebb1d765302e9af11bf449ca36d",\\\\n-                "sha256:d79d7d5dc8a32b7093e81e97dad755127ff77bcc899e845f41bf71747af0c569",\\\\n-                "sha256:dbcda74c67263139358f4d188ae5faae95c30929281bc6866d00573783c422b7",\\\\n-                "sha256:ddaea91abf8b0d13443f6dac52e89051a5063c7d014710dcb4d4abb2ff811a59",\\\\n-                "sha256:dee0ce50c6a2dd9056c20db781e9c1cfd33e77d2d569f5d1d9321c641bb903d5",\\\\n-                "sha256:dee60e1de1898bde3b238f18340eec6148986da0455d8ba7848d50470a7a32fb",\\\\n-                "sha256:e2f83e18fe2f4c9e7db597e988f72712c0c3676d337d8b101f6758107c42425b",\\\\n-                "sha256:e3fb1677c720409d5f671e39bac6c9e0e422584e5f518bfd50aa4cbbea02433f",\\\\n-                "sha256:ee2b1b1769f6707a8a445162ea16dddf74285c3964f605877a20e38545c3c462",\\\\n-                "sha256:ee6acae74a2b91865910eef5e7de37dc6895ad96fa23603d1d27ea69df545015",\\\\n-                "sha256:ef3f72c9666bba2bab70d2a8b79f2c6d2c1a42a7f7e2b0ec83bb2f9e383950af"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3, 3.4\\\\\\\'",\\\\n-            "version": "==1.14.1"\\\\n-        },\\\\n-        "zipp": {\\\\n-            "hashes": [\\\\n-                "sha256:71c644c5369f4a6e07636f0aa966270449561fcea2e3d6747b8d23efaa9d7832",\\\\n-                "sha256:9fe5ea21568a0a70e50f273397638d39b03353731e6cbbb3fd8502a33fec40bc"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'3.6\\\\\\\'",\\\\n-            "version": "==3.6.0"\\\\n-        },\\\\n-        "zope.event": {\\\\n-            "hashes": [\\\\n-                "sha256:73d9e3ef750cca14816a9c322c7250b0d7c9dbc337df5d1b807ff8d3d0b9e97c",\\\\n-                "sha256:81d98813046fc86cc4136e3698fee628a3282f9c320db18658c21749235fce80"\\\\n-            ],\\\\n-            "version": "==4.6"\\\\n-        },\\\\n-        "zope.interface": {\\\\n-            "hashes": [\\\\n-                "sha256:008b0b65c05993bb08912f644d140530e775cf1c62a072bf9340c2249e613c32",\\\\n-                "sha256:0217a9615531c83aeedb12e126611b1b1a3175013bbafe57c702ce40000eb9a0",\\\\n-                "sha256:0fb497c6b088818e3395e302e426850f8236d8d9f4ef5b2836feae812a8f699c",\\\\n-                "sha256:17ebf6e0b1d07ed009738016abf0d0a0f80388e009d0ac6e0ead26fc162b3b9c",\\\\n-                "sha256:311196634bb9333aa06f00fc94f59d3a9fddd2305c2c425d86e406ddc6f2260d",\\\\n-                "sha256:3218ab1a7748327e08ef83cca63eea7cf20ea7e2ebcb2522072896e5e2fceedf",\\\\n-                "sha256:404d1e284eda9e233c90128697c71acffd55e183d70628aa0bbb0e7a3084ed8b",\\\\n-                "sha256:4087e253bd3bbbc3e615ecd0b6dd03c4e6a1e46d152d3be6d2ad08fbad742dcc",\\\\n-                "sha256:40f4065745e2c2fa0dff0e7ccd7c166a8ac9748974f960cd39f63d2c19f9231f",\\\\n-                "sha256:5334e2ef60d3d9439c08baedaf8b84dc9bb9522d0dacbc10572ef5609ef8db6d",\\\\n-                "sha256:604cdba8f1983d0ab78edc29aa71c8df0ada06fb147cea436dc37093a0100a4e",\\\\n-                "sha256:6373d7eb813a143cb7795d3e42bd8ed857c82a90571567e681e1b3841a390d16",\\\\n-                "sha256:655796a906fa3ca67273011c9805c1e1baa047781fca80feeb710328cdbed87f",\\\\n-                "sha256:65c3c06afee96c654e590e046c4a24559e65b0a87dbff256cd4bd6f77e1a33f9",\\\\n-                "sha256:696f3d5493eae7359887da55c2afa05acc3db5fc625c49529e84bd9992313296",\\\\n-                "sha256:6e972493cdfe4ad0411fd9abfab7d4d800a7317a93928217f1a5de2bb0f0d87a",\\\\n-                "sha256:7579960be23d1fddecb53898035a0d112ac858c3554018ce615cefc03024e46d",\\\\n-                "sha256:765d703096ca47aa5d93044bf701b00bbce4d903a95b41fff7c3796e747b1f1d",\\\\n-                "sha256:7e66f60b0067a10dd289b29dceabd3d0e6d68be1504fc9d0bc209cf07f56d189",\\\\n-                "sha256:8a2ffadefd0e7206adc86e492ccc60395f7edb5680adedf17a7ee4205c530df4",\\\\n-                "sha256:959697ef2757406bff71467a09d940ca364e724c534efbf3786e86eee8591452",\\\\n-                "sha256:9d783213fab61832dbb10d385a319cb0e45451088abd45f95b5bb88ed0acca1a",\\\\n-                "sha256:a16025df73d24795a0bde05504911d306307c24a64187752685ff6ea23897cb0",\\\\n-                "sha256:a2ad597c8c9e038a5912ac3cf166f82926feff2f6e0dabdab956768de0a258f5",\\\\n-                "sha256:bfee1f3ff62143819499e348f5b8a7f3aa0259f9aca5e0ddae7391d059dce671",\\\\n-                "sha256:d169ccd0756c15bbb2f1acc012f5aab279dffc334d733ca0d9362c5beaebe88e",\\\\n-                "sha256:d514c269d1f9f5cd05ddfed15298d6c418129f3f064765295659798349c43e6f",\\\\n-                "sha256:d692374b578360d36568dd05efb8a5a67ab6d1878c29c582e37ddba80e66c396",\\\\n-                "sha256:dbaeb9cf0ea0b3bc4b36fae54a016933d64c6d52a94810a63c00f440ecb37dd7",\\\\n-                "sha256:dc26c8d44472e035d59d6f1177eb712888447f5799743da9c398b0339ed90b1b",\\\\n-                "sha256:e1574980b48c8c74f83578d1e77e701f8439a5d93f36a5a0af31337467c08fcf",\\\\n-                "sha256:e74a578172525c20d7223eac5f8ad187f10940dac06e40113d62f14f3adb1e8f",\\\\n-                "sha256:e945de62917acbf853ab968d8916290548df18dd62c739d862f359ecd25842a6",\\\\n-                "sha256:f0980d44b8aded808bec5059018d64692f0127f10510eca71f2f0ace8fb11188",\\\\n-                "sha256:f98d4bd7bbb15ca701d19b93263cc5edfd480c3475d163f137385f49e5b3a3a7",\\\\n-                "sha256:fb68d212efd057596dee9e6582daded9f8ef776538afdf5feceb3059df2d2e7b"\\\\n-            ],\\\\n-            "markers": "python_version >= \\\\\\\'2.7\\\\\\\' and python_version not in \\\\\\\'3.0, 3.1, 3.2, 3.3, 3.4\\\\\\\'",\\\\n-            "version": "==5.5.2"\\\\n-        }\\\\n-    },\\\\n-    "develop": {}\\\\n-}\\\\ndiff --git a/README.md b/README.md\\\\ndeleted file mode 100644\\\\nindex 624ca00..0000000\\\\n--- a/README.md\\\\n+++ /dev/null\\\\n@@ -1,170 +0,0 @@\\\\n-# Face Recognition using ARCFACE-Pytorch\\\\n-\\\\n-## Introduction\\\\n- This repo contains face_verify.py and app.py which is able to perform the following task -\\\\n- - Detect faces from an image, video or in webcam and perform face recogntion.\\\\n- - app.py was used to deploy the project.\\\\n- \\\\n-## Required Files\\\\n-- requirements.txt\\\\n-- pretrained model [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate) or [Mobilefacenet @ OneDrive](https://onedrive.live.com/?authkey=%21AIweh1IfiuF9vm4&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21836&parId=root&o=OneUp).\\\\n-- Custom dataset\\\\n-- Newly Trained model (facebank.pth and names.npy)\\\\n-\\\\n-\\\\n-## User Instruction\\\\n-After downloading the project first you have to install the following libraries.\\\\n-### Installation\\\\n-You can install all the dependencies at once by running the following command from your terminal.\\\\n-``` python\\\\n-    $ pip install -r requirements.txt\\\\n-```\\\\n-##### For the installation of torch using "pip" run the following command\\\\n-\\\\n-``` python\\\\n-    $ pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\\\\n-```\\\\n-### Project Setup\\\\n-\\\\n-#### pre-trained model\\\\n-Although i provided the pretrained model in the <b> work_space/model </b> and <b> work_space/save </b> folder, if you want to download the models you can follow the following url:\\\\n-\\\\n-- [IR-SE50 @ BaiduNetdisk](https://pan.baidu.com/s/12BUjjwy1uUTEF9HCx5qvoQ)\\\\n-- [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate)\\\\n-- [Mobilefacenet @ BaiduNetDisk](https://pan.baidu.com/s/1hqNNkcAjQOSxUjofboN6qg)\\\\n-- [Mobilefacenet @ OneDrive](https://onedrive.live.com/?authkey=%21AIweh1IfiuF9vm4&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21836&parId=root&o=OneUp)\\\\n-\\\\n-I have used the <b>IR-SE50</b> as the pretrained model to train with my custom dataset. You need to copy the pretrained model and save it under the <b> work_space/save </b> folder as <b> model_final.pth</b>\\\\n-\\\\n-#### Newly trained model\\\\n-In the <b> data/facebank </b> you will find a trained model named <b> "facebank.pth" </b> which contains the related weights and "names.npy" contains the corresponding labels of the users that are avialable in the facebank folder. For instance in this case\\\\n-the <b> facebank </b> folder will look like this :-\\\\n-\\\\n-    facebank/\\\\n-                ---> Chandler\\\\n-                ---> Joey\\\\n-                ---> Monica\\\\n-                ---> Phoebe\\\\n-                ---> Pias\\\\n-                ---> Rachel\\\\n-                ---> Raihan\\\\n-                ---> Ross\\\\n-                ---> Samiur\\\\n-                ---> Shakil\\\\n-                ---> facebank.pth\\\\n-                ---> names.npy\\\\n-\\\\n-If you have the "facebank.pth" and "names.npy" files in the <b>data/facebank</b> you can execute the following command to see the demo.\\\\n-\\\\n-```python\\\\n-    $ python app.py\\\\n- ```\\\\n- and go to the following url from your web browser.\\\\n- ```url\\\\n-http://localhost:5000\\\\n-```\\\\n-\\\\n-\\\\n-<hr>\\\\n-Note: If you want to run the inference on a video, download a video of related persons (Person that you trained the model with) and replace 0 in the line number 43 of <b> face_verify.py </b> with the path of your video file. For this code you can run the inference on any video of <b> Friends</b> tv series.\\\\n-<hr>\\\\n-\\\\n-#### Now if you want to train with your custom dataset, you need to follow the following steps.\\\\n-\\\\n-#### Dataset preparation \\\\n-\\\\n-First organize your images within the following manner- \\\\n-\\\\n-    data/\\\\n-        raw/\\\\n-             name1/\\\\n-                 photo1.jpg\\\\n-                 photo2.jpg\\\\n-                 ...\\\\n-             name2/\\\\n-                 photo1.jpg\\\\n-                 photo2.jpg\\\\n-                 ...\\\\n-             .....\\\\n-now run the following command\\\\n-```python\\\\n-$ python .\\\\\\\\create-dataset\\\\\\\\align_dataset_mtcnn.py data/raw/ data/processed --image_size 112\\\\n-```\\\\n-\\\\n-You will see a new folder inside the data directory named <b> "processed" </b> which will hold all the images that contains only faces of each user. If more than 1 image appears in any folder for a person, average embedding will be calculated. \\\\n-\\\\n-After executing the script new images for each user in the processed folder will look something like this.\\\\n-<p align="center"> \\\\n-<b> Cropped Images of faces </b>\\\\n-    <img src ="http://muizzer07.pythonanywhere.com/media/files/Picture1.png">\\\\n-</p> \\\\n-\\\\n-Copy all the folders of the users under the <b>data/processed</b> folder and paste in the <b>data/facebank</b> folder.\\\\n-\\\\n-\\\\n-Now to train with your dataset, you need to set <b> args.update == True </b> in line 35 of face_verify.py . After training you will get a new facebank.pth and names.npy in your data/facebank folder which will now only holds the weights and labels of your newly trained dataset. Once the training is done you need to reset <b> args.update==False</b>.\\\\n-However, if this doesn\\\\\\\'t work change the code in following manner-\\\\n-#### Old Code \\\\n-```python\\\\n-    if args.update:\\\\n-        targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\\\\n-        print(\\\\\\\'facebank updated\\\\\\\')\\\\n-    else:\\\\n-        targets, names = load_facebank(conf)\\\\n-        print(\\\\\\\'facebank loaded\\\\\\\')\\\\n-```\\\\n-#### New Code \\\\n-Only keep the follwing lines for training, once the training is done just replace it with the old code.\\\\n-```python\\\\n-        targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\\\\n-        print(\\\\\\\'facebank updated\\\\\\\')\\\\n-````\\\\n-Or you can simply pass a command line arguement such as below if there is new data to train.\\\\n-```python\\\\n-   $python face_verify.py -u\\\\n-```\\\\n-Here the -u parse the command to update the facebank.pth and names.npy.\\\\n-\\\\n-Now you are ready to test the systen with your newly trained users by running-\\\\n-\\\\n-```python\\\\n-    $ python app.py\\\\n-```\\\\n-\\\\n-### Note: You can train with custom dataset as many time as you want, you will only require any of the pre-trained model to train with your custom dataset to generate the <b>facebank.pth</b> and <b>names.npy</b>. Once you get this two files you are ready to test the face recogniton.\\\\n-\\\\n-\\\\n-<hr>\\\\n-\\\\n-### Retrain the pre-trained model\\\\n-\\\\n- If you want to build a new pre-trained model like [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate) and reproduce the result, you will need the large files which contains several dataset of faces under the <b> data/faces_emore </b>.\\\\n- \\\\n- To get these files, first you need to download the [MS1M](https://arxiv.org/abs/1607.08221) dataset from any of the following url-\\\\n-- [emore dataset @ Dropbox](https://www.dropbox.com/s/wpx6tqjf0y5mf6r/faces_ms1m-refine-v2_112x112.zip?dl=0)\\\\n-- [emore dataset @ BaiduDrive](https://pan.baidu.com/s/1eXohwNBHbbKXh5KHyItVhQ)\\\\n-\\\\n-After unzipping the downloaded file execute the following command. It will take few hours depending on your system configuration.\\\\n-\\\\n-```python\\\\n-    $ python prepare_data.py\\\\n-```\\\\n-After that you will see the following files to the "data/faces_emore" folder. \\\\n-\\\\n-    faces_emore/\\\\n-                ---> agedb_30\\\\n-                ---> calfw\\\\n-                ---> cfp_ff\\\\n-                ---> cfp_fp\\\\n-                ---> cfp_fp\\\\n-                ---> cplfw\\\\n-                ---> imgs\\\\n-                ---> lfw\\\\n-                ---> vgg2_fp\\\\n-\\\\n-To know the further training procedure you can see the details in this [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch) repository.\\\\n-\\\\n-## References\\\\n-- [Arcface](https://arxiv.org/pdf/1801.07698.pdf)\\\\n-- [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)\\\\n-- [The one with Face Recognition.](https://towardsdatascience.com/s01e01-3eb397d458d)\\\\ndiff --git a/app.py b/app.py\\\\ndeleted file mode 100644\\\\nindex c84f09b..0000000\\\\n--- a/app.py\\\\n+++ /dev/null\\\\n@@ -1,31 +0,0 @@\\\\n-# Flask utils\\\\n-from flask import Flask, redirect, url_for, request, render_template, Response\\\\n-from werkzeug.utils import secure_filename\\\\n-from gevent.pywsgi import WSGIServer\\\\n-from face_verify import faceRec\\\\n-\\\\n-camera = faceRec()\\\\n-\\\\n-app = Flask(__name__)\\\\n-\\\\n-@app.route("/")\\\\n-def main():\\\\n-    return render_template("index.html")\\\\n-\\\\n-def gen(camera):\\\\n-    while True:\\\\n-        frame = camera.main()\\\\n-        if frame != "":\\\\n-            global_frame = frame\\\\n-            yield (b\\\\\\\'--frame\\\\\\\\r\\\\\\\\n\\\\\\\'\\\\n-                    b\\\\\\\'Content-Type: image/jpeg\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\' + frame + b\\\\\\\'\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\')\\\\n-\\\\n-@app.route(\\\\\\\'/video_feed\\\\\\\')\\\\n-def video_feed():\\\\n-    return Response(gen(camera), mimetype=\\\\\\\'multipart/x-mixed-replace; boundary=frame\\\\\\\')\\\\n-\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    # Serve the app with gevent\\\\n-    http_server = WSGIServer((\\\\\\\'localhost\\\\\\\', 5000), app)\\\\n-    http_server.serve_forever()\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/config.py b/config.py\\\\ndeleted file mode 100644\\\\nindex 656f459..0000000\\\\n--- a/config.py\\\\n+++ /dev/null\\\\n@@ -1,52 +0,0 @@\\\\n-from easydict import EasyDict as edict\\\\n-from pathlib import Path\\\\n-import torch\\\\n-from torch.nn import CrossEntropyLoss\\\\n-from torchvision import transforms as trans\\\\n-\\\\n-def get_config(training = True):\\\\n-    conf = edict()\\\\n-    conf.data_path = Path(\\\\\\\'data\\\\\\\')\\\\n-    conf.work_path = Path(\\\\\\\'work_space/\\\\\\\')\\\\n-    conf.model_path = conf.work_path/\\\\\\\'models\\\\\\\'\\\\n-    conf.log_path = conf.work_path/\\\\\\\'log\\\\\\\'\\\\n-    conf.save_path = conf.work_path/\\\\\\\'save\\\\\\\'\\\\n-    conf.input_size = [112,112]\\\\n-    conf.embedding_size = 512\\\\n-    conf.use_mobilfacenet = False\\\\n-    conf.facebank_path = conf.data_path/\\\\\\\'facebank\\\\\\\'\\\\n-    conf.net_depth = 50\\\\n-    conf.drop_ratio = 0.6\\\\n-    conf.net_mode = \\\\\\\'ir_se\\\\\\\' # or \\\\\\\'ir\\\\\\\'\\\\n-    conf.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\\\\n-    conf.test_transform = trans.Compose([\\\\n-                    trans.ToTensor(),\\\\n-                    trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\\\n-                ])\\\\n-    conf.data_mode = \\\\\\\'emore\\\\\\\'\\\\n-    conf.vgg_folder = conf.data_path/\\\\\\\'faces_vgg_112x112\\\\\\\'\\\\n-    conf.ms1m_folder = conf.data_path/\\\\\\\'faces_ms1m_112x112\\\\\\\'\\\\n-    conf.emore_folder = conf.data_path/\\\\\\\'faces_emore\\\\\\\'\\\\n-    conf.batch_size = 100 # irse net depth 50 \\\\n-#   conf.batch_size = 200 # mobilefacenet\\\\n-#--------------------Training Config ------------------------    \\\\n-    if training:        \\\\n-        conf.log_path = conf.work_path/\\\\\\\'log\\\\\\\'\\\\n-        conf.save_path = conf.work_path/\\\\\\\'save\\\\\\\'\\\\n-    #     conf.weight_decay = 5e-4\\\\n-        conf.lr = 1e-3\\\\n-        conf.milestones = [12,15,18]\\\\n-        conf.momentum = 0.9\\\\n-        conf.pin_memory = True\\\\n-#         conf.num_workers = 4 # when batchsize is 200\\\\n-        conf.num_workers = 3\\\\n-        conf.ce_loss = CrossEntropyLoss()    \\\\n-#--------------------Inference Config ------------------------\\\\n-    else:\\\\n-        conf.facebank_path = conf.data_path/\\\\\\\'facebank\\\\\\\'\\\\n-        conf.threshold = 0.9\\\\n-        conf.face_limit = 10 \\\\n-        #when inference, at maximum detect 10 faces in one image,\\\\n-        conf.min_face_size = 35\\\\n-        # the larger this value, the faster deduction, comes with tradeoff in small faces\\\\n-    return conf\\\\ndiff --git a/create-dataset/align/__init__.py b/create-dataset/align/__init__.py\\\\ndeleted file mode 100644\\\\nindex e69de29..0000000\\\\ndiff --git a/create-dataset/align/align_dataset_mtcnn.py b/create-dataset/align/align_dataset_mtcnn.py\\\\ndeleted file mode 100644\\\\nindex 7d5e735..0000000\\\\n--- a/create-dataset/align/align_dataset_mtcnn.py\\\\n+++ /dev/null\\\\n@@ -1,159 +0,0 @@\\\\n-"""Performs face alignment and stores face thumbnails in the output directory."""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from scipy import misc\\\\n-import sys\\\\n-import os\\\\n-import argparse\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import facenet\\\\n-import align.detect_face\\\\n-import random\\\\n-from time import sleep\\\\n-\\\\n-def main(args):\\\\n-    sleep(random.random())\\\\n-    output_dir = os.path.expanduser(args.output_dir)\\\\n-    if not os.path.exists(output_dir):\\\\n-        os.makedirs(output_dir)\\\\n-    # Store some git revision info in a text file in the log directory\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\n-    facenet.store_revision_info(src_path, output_dir, \\\\\\\' \\\\\\\'.join(sys.argv))\\\\n-    dataset = facenet.get_dataset(args.input_dir)\\\\n-    \\\\n-    print(\\\\\\\'Creating networks and loading parameters\\\\\\\')\\\\n-    \\\\n-    with tf.Graph().as_default():\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-        with sess.as_default():\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\\\\n-    \\\\n-    minsize = 20 # minimum size of face\\\\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\\\\\\\'s threshold\\\\n-    factor = 0.709 # scale factor\\\\n-\\\\n-    # Add a random key to the filename to allow alignment using multiple processes\\\\n-    random_key = np.random.randint(0, high=99999)\\\\n-    bounding_boxes_filename = os.path.join(output_dir, \\\\\\\'bounding_boxes_%05d.txt\\\\\\\' % random_key)\\\\n-    \\\\n-    with open(bounding_boxes_filename, "w") as text_file:\\\\n-        nrof_images_total = 0\\\\n-        nrof_successfully_aligned = 0\\\\n-        if args.random_order:\\\\n-            random.shuffle(dataset)\\\\n-        for cls in dataset:\\\\n-            output_class_dir = os.path.join(output_dir, cls.name)\\\\n-            if not os.path.exists(output_class_dir):\\\\n-                os.makedirs(output_class_dir)\\\\n-                if args.random_order:\\\\n-                    random.shuffle(cls.image_paths)\\\\n-            for image_path in cls.image_paths:\\\\n-                nrof_images_total += 1\\\\n-                filename = os.path.splitext(os.path.split(image_path)[1])[0]\\\\n-                output_filename = os.path.join(output_class_dir, filename+\\\\\\\'.png\\\\\\\')\\\\n-                print(image_path)\\\\n-                if not os.path.exists(output_filename):\\\\n-                    try:\\\\n-                        img = misc.imread(image_path)\\\\n-                    except (IOError, ValueError, IndexError) as e:\\\\n-                        errorMessage = \\\\\\\'{}: {}\\\\\\\'.format(image_path, e)\\\\n-                        print(errorMessage)\\\\n-                    else:\\\\n-                        if img.ndim<2:\\\\n-                            print(\\\\\\\'Unable to align "%s"\\\\\\\' % image_path)\\\\n-                            text_file.write(\\\\\\\'%s\\\\\\\\n\\\\\\\' % (output_filename))\\\\n-                            continue\\\\n-                        if img.ndim == 2:\\\\n-                            img = facenet.to_rgb(img)\\\\n-                        img = img[:,:,0:3]\\\\n-    \\\\n-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\\\\n-                        nrof_faces = bounding_boxes.shape[0]\\\\n-                        if nrof_faces>0:\\\\n-                            det = bounding_boxes[:,0:4]\\\\n-                            det_arr = []\\\\n-                            img_size = np.asarray(img.shape)[0:2]\\\\n-                            if nrof_faces>1:\\\\n-                                if args.detect_multiple_faces:\\\\n-                                    for i in range(nrof_faces):\\\\n-                                        det_arr.append(np.squeeze(det[i]))\\\\n-                                else:\\\\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\\\\n-                                    img_center = img_size / 2\\\\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\\\\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\\\\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\\\\n-                                    det_arr.append(det[index,:])\\\\n-                            else:\\\\n-                                det_arr.append(np.squeeze(det))\\\\n-\\\\n-                            for i, det in enumerate(det_arr):\\\\n-                                det = np.squeeze(det)\\\\n-                                bb = np.zeros(4, dtype=np.int32)\\\\n-                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\\\\n-                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\\\\n-                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\\\\n-                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\\\\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\\\\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\\\\\\\'bilinear\\\\\\\')\\\\n-                                nrof_successfully_aligned += 1\\\\n-                                filename_base, file_extension = os.path.splitext(output_filename)\\\\n-                                if args.detect_multiple_faces:\\\\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\\\\n-                                else:\\\\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\\\\n-                                misc.imsave(output_filename_n, scaled)\\\\n-                                text_file.write(\\\\\\\'%s %d %d %d %d\\\\\\\\n\\\\\\\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\\\\n-                        else:\\\\n-                            print(\\\\\\\'Unable to align "%s"\\\\\\\' % image_path)\\\\n-                            text_file.write(\\\\\\\'%s\\\\\\\\n\\\\\\\' % (output_filename))\\\\n-                            \\\\n-    print(\\\\\\\'Total number of images: %d\\\\\\\' % nrof_images_total)\\\\n-    print(\\\\\\\'Number of successfully aligned images: %d\\\\\\\' % nrof_successfully_aligned)\\\\n-            \\\\n-\\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'input_dir\\\\\\\', type=str, help=\\\\\\\'Directory with unaligned images.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'output_dir\\\\\\\', type=str, help=\\\\\\\'Directory with aligned face thumbnails.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=182)\\\\n-    parser.add_argument(\\\\\\\'--margin\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Margin for the crop around the bounding box (height, width) in pixels.\\\\\\\', default=44)\\\\n-    parser.add_argument(\\\\\\\'--random_order\\\\\\\', \\\\n-        help=\\\\\\\'Shuffles the order of images to enable alignment using multiple processes.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--gpu_memory_fraction\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--detect_multiple_faces\\\\\\\', type=bool,\\\\n-                        help=\\\\\\\'Detect and align multiple faces per image.\\\\\\\', default=False)\\\\n-    return parser.parse_args(argv)\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/create-dataset/align/det1.npy b/create-dataset/align/det1.npy\\\\ndeleted file mode 100644\\\\nindex 7c05a2c..0000000\\\\nBinary files a/create-dataset/align/det1.npy and /dev/null differ\\\\ndiff --git a/create-dataset/align/det2.npy b/create-dataset/align/det2.npy\\\\ndeleted file mode 100644\\\\nindex 85d5bf0..0000000\\\\nBinary files a/create-dataset/align/det2.npy and /dev/null differ\\\\ndiff --git a/create-dataset/align/det3.npy b/create-dataset/align/det3.npy\\\\ndeleted file mode 100644\\\\nindex 90d5ba9..0000000\\\\nBinary files a/create-dataset/align/det3.npy and /dev/null differ\\\\ndiff --git a/create-dataset/align/detect_face.py b/create-dataset/align/detect_face.py\\\\ndeleted file mode 100644\\\\nindex 7f98ca7..0000000\\\\n--- a/create-dataset/align/detect_face.py\\\\n+++ /dev/null\\\\n@@ -1,781 +0,0 @@\\\\n-""" Tensorflow implementation of the face detection / alignment algorithm found at\\\\n-https://github.com/kpzhang93/MTCNN_face_detection_alignment\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-from six import string_types, iteritems\\\\n-\\\\n-import numpy as np\\\\n-import tensorflow as tf\\\\n-#from math import floor\\\\n-import cv2\\\\n-import os\\\\n-\\\\n-def layer(op):\\\\n-    """Decorator for composable network layers."""\\\\n-\\\\n-    def layer_decorated(self, *args, **kwargs):\\\\n-        # Automatically set a name if not provided.\\\\n-        name = kwargs.setdefault(\\\\\\\'name\\\\\\\', self.get_unique_name(op.__name__))\\\\n-        # Figure out the layer inputs.\\\\n-        if len(self.terminals) == 0:\\\\n-            raise RuntimeError(\\\\\\\'No input variables found for layer %s.\\\\\\\' % name)\\\\n-        elif len(self.terminals) == 1:\\\\n-            layer_input = self.terminals[0]\\\\n-        else:\\\\n-            layer_input = list(self.terminals)\\\\n-        # Perform the operation and get the output.\\\\n-        layer_output = op(self, layer_input, *args, **kwargs)\\\\n-        # Add to layer LUT.\\\\n-        self.layers[name] = layer_output\\\\n-        # This output is now the input for the next layer.\\\\n-        self.feed(layer_output)\\\\n-        # Return self for chained calls.\\\\n-        return self\\\\n-\\\\n-    return layer_decorated\\\\n-\\\\n-class Network(object):\\\\n-\\\\n-    def __init__(self, inputs, trainable=True):\\\\n-        # The input nodes for this network\\\\n-        self.inputs = inputs\\\\n-        # The current list of terminal nodes\\\\n-        self.terminals = []\\\\n-        # Mapping from layer names to layers\\\\n-        self.layers = dict(inputs)\\\\n-        # If true, the resulting variables are set as trainable\\\\n-        self.trainable = trainable\\\\n-\\\\n-        self.setup()\\\\n-\\\\n-    def setup(self):\\\\n-        """Construct the network. """\\\\n-        raise NotImplementedError(\\\\\\\'Must be implemented by the subclass.\\\\\\\')\\\\n-\\\\n-    def load(self, data_path, session, ignore_missing=False):\\\\n-        """Load network weights.\\\\n-        data_path: The path to the numpy-serialized network weights\\\\n-        session: The current TensorFlow session\\\\n-        ignore_missing: If true, serialized weights for missing layers are ignored.\\\\n-        """\\\\n-        data_dict = np.load(data_path, encoding=\\\\\\\'latin1\\\\\\\').item() #pylint: disable=no-member\\\\n-\\\\n-        for op_name in data_dict:\\\\n-            with tf.variable_scope(op_name, reuse=True):\\\\n-                for param_name, data in iteritems(data_dict[op_name]):\\\\n-                    try:\\\\n-                        var = tf.get_variable(param_name)\\\\n-                        session.run(var.assign(data))\\\\n-                    except ValueError:\\\\n-                        if not ignore_missing:\\\\n-                            raise\\\\n-\\\\n-    def feed(self, *args):\\\\n-        """Set the input(s) for the next operation by replacing the terminal nodes.\\\\n-        The arguments can be either layer names or the actual layers.\\\\n-        """\\\\n-        assert len(args) != 0\\\\n-        self.terminals = []\\\\n-        for fed_layer in args:\\\\n-            if isinstance(fed_layer, string_types):\\\\n-                try:\\\\n-                    fed_layer = self.layers[fed_layer]\\\\n-                except KeyError:\\\\n-                    raise KeyError(\\\\\\\'Unknown layer name fed: %s\\\\\\\' % fed_layer)\\\\n-            self.terminals.append(fed_layer)\\\\n-        return self\\\\n-\\\\n-    def get_output(self):\\\\n-        """Returns the current network output."""\\\\n-        return self.terminals[-1]\\\\n-\\\\n-    def get_unique_name(self, prefix):\\\\n-        """Returns an index-suffixed unique name for the given prefix.\\\\n-        This is used for auto-generating layer names based on the type-prefix.\\\\n-        """\\\\n-        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\\\\n-        return \\\\\\\'%s_%d\\\\\\\' % (prefix, ident)\\\\n-\\\\n-    def make_var(self, name, shape):\\\\n-        """Creates a new TensorFlow variable."""\\\\n-        return tf.get_variable(name, shape, trainable=self.trainable)\\\\n-\\\\n-    def validate_padding(self, padding):\\\\n-        """Verifies that the padding is one of the supported ones."""\\\\n-        assert padding in (\\\\\\\'SAME\\\\\\\', \\\\\\\'VALID\\\\\\\')\\\\n-\\\\n-    @layer\\\\n-    def conv(self,\\\\n-             inp,\\\\n-             k_h,\\\\n-             k_w,\\\\n-             c_o,\\\\n-             s_h,\\\\n-             s_w,\\\\n-             name,\\\\n-             relu=True,\\\\n-             padding=\\\\\\\'SAME\\\\\\\',\\\\n-             group=1,\\\\n-             biased=True):\\\\n-        # Verify that the padding is acceptable\\\\n-        self.validate_padding(padding)\\\\n-        # Get the number of channels in the input\\\\n-        c_i = int(inp.get_shape()[-1])\\\\n-        # Verify that the grouping parameter is valid\\\\n-        assert c_i % group == 0\\\\n-        assert c_o % group == 0\\\\n-        # Convolution for a given input and kernel\\\\n-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\\\\n-        with tf.variable_scope(name) as scope:\\\\n-            kernel = self.make_var(\\\\\\\'weights\\\\\\\', shape=[k_h, k_w, c_i // group, c_o])\\\\n-            # This is the common-case. Convolve the input without any further complications.\\\\n-            output = convolve(inp, kernel)\\\\n-            # Add the biases\\\\n-            if biased:\\\\n-                biases = self.make_var(\\\\\\\'biases\\\\\\\', [c_o])\\\\n-                output = tf.nn.bias_add(output, biases)\\\\n-            if relu:\\\\n-                # ReLU non-linearity\\\\n-                output = tf.nn.relu(output, name=scope.name)\\\\n-            return output\\\\n-\\\\n-    @layer\\\\n-    def prelu(self, inp, name):\\\\n-        with tf.variable_scope(name):\\\\n-            i = int(inp.get_shape()[-1])\\\\n-            alpha = self.make_var(\\\\\\\'alpha\\\\\\\', shape=(i,))\\\\n-            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\\\\n-        return output\\\\n-\\\\n-    @layer\\\\n-    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\\\\\\\'SAME\\\\\\\'):\\\\n-        self.validate_padding(padding)\\\\n-        return tf.nn.max_pool(inp,\\\\n-                              ksize=[1, k_h, k_w, 1],\\\\n-                              strides=[1, s_h, s_w, 1],\\\\n-                              padding=padding,\\\\n-                              name=name)\\\\n-\\\\n-    @layer\\\\n-    def fc(self, inp, num_out, name, relu=True):\\\\n-        with tf.variable_scope(name):\\\\n-            input_shape = inp.get_shape()\\\\n-            if input_shape.ndims == 4:\\\\n-                # The input is spatial. Vectorize it first.\\\\n-                dim = 1\\\\n-                for d in input_shape[1:].as_list():\\\\n-                    dim *= int(d)\\\\n-                feed_in = tf.reshape(inp, [-1, dim])\\\\n-            else:\\\\n-                feed_in, dim = (inp, input_shape[-1].value)\\\\n-            weights = self.make_var(\\\\\\\'weights\\\\\\\', shape=[dim, num_out])\\\\n-            biases = self.make_var(\\\\\\\'biases\\\\\\\', [num_out])\\\\n-            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\\\\n-            fc = op(feed_in, weights, biases, name=name)\\\\n-            return fc\\\\n-\\\\n-\\\\n-    """\\\\n-    Multi dimensional softmax,\\\\n-    refer to https://github.com/tensorflow/tensorflow/issues/210\\\\n-    compute softmax along the dimension of target\\\\n-    the native softmax only supports batch_size x dimension\\\\n-    """\\\\n-    @layer\\\\n-    def softmax(self, target, axis, name=None):\\\\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\\\\n-        target_exp = tf.exp(target-max_axis)\\\\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\\\\n-        softmax = tf.div(target_exp, normalize, name)\\\\n-        return softmax\\\\n-    \\\\n-class PNet(Network):\\\\n-    def setup(self):\\\\n-        (self.feed(\\\\\\\'data\\\\\\\') #pylint: disable=no-value-for-parameter, no-member\\\\n-             .conv(3, 3, 10, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv1\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'PReLU1\\\\\\\')\\\\n-             .max_pool(2, 2, 2, 2, name=\\\\\\\'pool1\\\\\\\')\\\\n-             .conv(3, 3, 16, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv2\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'PReLU2\\\\\\\')\\\\n-             .conv(3, 3, 32, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv3\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'PReLU3\\\\\\\')\\\\n-             .conv(1, 1, 2, 1, 1, relu=False, name=\\\\\\\'conv4-1\\\\\\\')\\\\n-             .softmax(3,name=\\\\\\\'prob1\\\\\\\'))\\\\n-\\\\n-        (self.feed(\\\\\\\'PReLU3\\\\\\\') #pylint: disable=no-value-for-parameter\\\\n-             .conv(1, 1, 4, 1, 1, relu=False, name=\\\\\\\'conv4-2\\\\\\\'))\\\\n-        \\\\n-class RNet(Network):\\\\n-    def setup(self):\\\\n-        (self.feed(\\\\\\\'data\\\\\\\') #pylint: disable=no-value-for-parameter, no-member\\\\n-             .conv(3, 3, 28, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv1\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu1\\\\\\\')\\\\n-             .max_pool(3, 3, 2, 2, name=\\\\\\\'pool1\\\\\\\')\\\\n-             .conv(3, 3, 48, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv2\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu2\\\\\\\')\\\\n-             .max_pool(3, 3, 2, 2, padding=\\\\\\\'VALID\\\\\\\', name=\\\\\\\'pool2\\\\\\\')\\\\n-             .conv(2, 2, 64, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv3\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu3\\\\\\\')\\\\n-             .fc(128, relu=False, name=\\\\\\\'conv4\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu4\\\\\\\')\\\\n-             .fc(2, relu=False, name=\\\\\\\'conv5-1\\\\\\\')\\\\n-             .softmax(1,name=\\\\\\\'prob1\\\\\\\'))\\\\n-\\\\n-        (self.feed(\\\\\\\'prelu4\\\\\\\') #pylint: disable=no-value-for-parameter\\\\n-             .fc(4, relu=False, name=\\\\\\\'conv5-2\\\\\\\'))\\\\n-\\\\n-class ONet(Network):\\\\n-    def setup(self):\\\\n-        (self.feed(\\\\\\\'data\\\\\\\') #pylint: disable=no-value-for-parameter, no-member\\\\n-             .conv(3, 3, 32, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv1\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu1\\\\\\\')\\\\n-             .max_pool(3, 3, 2, 2, name=\\\\\\\'pool1\\\\\\\')\\\\n-             .conv(3, 3, 64, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv2\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu2\\\\\\\')\\\\n-             .max_pool(3, 3, 2, 2, padding=\\\\\\\'VALID\\\\\\\', name=\\\\\\\'pool2\\\\\\\')\\\\n-             .conv(3, 3, 64, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv3\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu3\\\\\\\')\\\\n-             .max_pool(2, 2, 2, 2, name=\\\\\\\'pool3\\\\\\\')\\\\n-             .conv(2, 2, 128, 1, 1, padding=\\\\\\\'VALID\\\\\\\', relu=False, name=\\\\\\\'conv4\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu4\\\\\\\')\\\\n-             .fc(256, relu=False, name=\\\\\\\'conv5\\\\\\\')\\\\n-             .prelu(name=\\\\\\\'prelu5\\\\\\\')\\\\n-             .fc(2, relu=False, name=\\\\\\\'conv6-1\\\\\\\')\\\\n-             .softmax(1, name=\\\\\\\'prob1\\\\\\\'))\\\\n-\\\\n-        (self.feed(\\\\\\\'prelu5\\\\\\\') #pylint: disable=no-value-for-parameter\\\\n-             .fc(4, relu=False, name=\\\\\\\'conv6-2\\\\\\\'))\\\\n-\\\\n-        (self.feed(\\\\\\\'prelu5\\\\\\\') #pylint: disable=no-value-for-parameter\\\\n-             .fc(10, relu=False, name=\\\\\\\'conv6-3\\\\\\\'))\\\\n-\\\\n-def create_mtcnn(sess, model_path):\\\\n-    if not model_path:\\\\n-        model_path,_ = os.path.split(os.path.realpath(__file__))\\\\n-\\\\n-    with tf.variable_scope(\\\\\\\'pnet\\\\\\\'):\\\\n-        data = tf.placeholder(tf.float32, (None,None,None,3), \\\\\\\'input\\\\\\\')\\\\n-        pnet = PNet({\\\\\\\'data\\\\\\\':data})\\\\n-        pnet.load(os.path.join(model_path, \\\\\\\'det1.npy\\\\\\\'), sess)\\\\n-    with tf.variable_scope(\\\\\\\'rnet\\\\\\\'):\\\\n-        data = tf.placeholder(tf.float32, (None,24,24,3), \\\\\\\'input\\\\\\\')\\\\n-        rnet = RNet({\\\\\\\'data\\\\\\\':data})\\\\n-        rnet.load(os.path.join(model_path, \\\\\\\'det2.npy\\\\\\\'), sess)\\\\n-    with tf.variable_scope(\\\\\\\'onet\\\\\\\'):\\\\n-        data = tf.placeholder(tf.float32, (None,48,48,3), \\\\\\\'input\\\\\\\')\\\\n-        onet = ONet({\\\\\\\'data\\\\\\\':data})\\\\n-        onet.load(os.path.join(model_path, \\\\\\\'det3.npy\\\\\\\'), sess)\\\\n-        \\\\n-    pnet_fun = lambda img : sess.run((\\\\\\\'pnet/conv4-2/BiasAdd:0\\\\\\\', \\\\\\\'pnet/prob1:0\\\\\\\'), feed_dict={\\\\\\\'pnet/input:0\\\\\\\':img})\\\\n-    rnet_fun = lambda img : sess.run((\\\\\\\'rnet/conv5-2/conv5-2:0\\\\\\\', \\\\\\\'rnet/prob1:0\\\\\\\'), feed_dict={\\\\\\\'rnet/input:0\\\\\\\':img})\\\\n-    onet_fun = lambda img : sess.run((\\\\\\\'onet/conv6-2/conv6-2:0\\\\\\\', \\\\\\\'onet/conv6-3/conv6-3:0\\\\\\\', \\\\\\\'onet/prob1:0\\\\\\\'), feed_dict={\\\\\\\'onet/input:0\\\\\\\':img})\\\\n-    return pnet_fun, rnet_fun, onet_fun\\\\n-\\\\n-def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\\\\n-    """Detects faces in an image, and returns bounding boxes and points for them.\\\\n-    img: input image\\\\n-    minsize: minimum faces\\\\\\\' size\\\\n-    pnet, rnet, onet: caffemodel\\\\n-    threshold: threshold=[th1, th2, th3], th1-3 are three steps\\\\\\\'s threshold\\\\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\\\\n-    """\\\\n-    factor_count=0\\\\n-    total_boxes=np.empty((0,9))\\\\n-    points=np.empty(0)\\\\n-    h=img.shape[0]\\\\n-    w=img.shape[1]\\\\n-    minl=np.amin([h, w])\\\\n-    m=12.0/minsize\\\\n-    minl=minl*m\\\\n-    # create scale pyramid\\\\n-    scales=[]\\\\n-    while minl>=12:\\\\n-        scales += [m*np.power(factor, factor_count)]\\\\n-        minl = minl*factor\\\\n-        factor_count += 1\\\\n-\\\\n-    # first stage\\\\n-    for scale in scales:\\\\n-        hs=int(np.ceil(h*scale))\\\\n-        ws=int(np.ceil(w*scale))\\\\n-        im_data = imresample(img, (hs, ws))\\\\n-        im_data = (im_data-127.5)*0.0078125\\\\n-        img_x = np.expand_dims(im_data, 0)\\\\n-        img_y = np.transpose(img_x, (0,2,1,3))\\\\n-        out = pnet(img_y)\\\\n-        out0 = np.transpose(out[0], (0,2,1,3))\\\\n-        out1 = np.transpose(out[1], (0,2,1,3))\\\\n-        \\\\n-        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\\\\n-        \\\\n-        # inter-scale nms\\\\n-        pick = nms(boxes.copy(), 0.5, \\\\\\\'Union\\\\\\\')\\\\n-        if boxes.size>0 and pick.size>0:\\\\n-            boxes = boxes[pick,:]\\\\n-            total_boxes = np.append(total_boxes, boxes, axis=0)\\\\n-\\\\n-    numbox = total_boxes.shape[0]\\\\n-    if numbox>0:\\\\n-        pick = nms(total_boxes.copy(), 0.7, \\\\\\\'Union\\\\\\\')\\\\n-        total_boxes = total_boxes[pick,:]\\\\n-        regw = total_boxes[:,2]-total_boxes[:,0]\\\\n-        regh = total_boxes[:,3]-total_boxes[:,1]\\\\n-        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\\\\n-        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\\\\n-        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\\\\n-        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\\\\n-        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\\\\n-        total_boxes = rerec(total_boxes.copy())\\\\n-        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\\\\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\\\\n-\\\\n-    numbox = total_boxes.shape[0]\\\\n-    if numbox>0:\\\\n-        # second stage\\\\n-        tempimg = np.zeros((24,24,3,numbox))\\\\n-        for k in range(0,numbox):\\\\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\\\\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\\\\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\\\\n-                tempimg[:,:,:,k] = imresample(tmp, (24, 24))\\\\n-            else:\\\\n-                return np.empty()\\\\n-        tempimg = (tempimg-127.5)*0.0078125\\\\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\\\\n-        out = rnet(tempimg1)\\\\n-        out0 = np.transpose(out[0])\\\\n-        out1 = np.transpose(out[1])\\\\n-        score = out1[1,:]\\\\n-        ipass = np.where(score>threshold[1])\\\\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\\\\n-        mv = out0[:,ipass[0]]\\\\n-        if total_boxes.shape[0]>0:\\\\n-            pick = nms(total_boxes, 0.7, \\\\\\\'Union\\\\\\\')\\\\n-            total_boxes = total_boxes[pick,:]\\\\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\\\\n-            total_boxes = rerec(total_boxes.copy())\\\\n-\\\\n-    numbox = total_boxes.shape[0]\\\\n-    if numbox>0:\\\\n-        # third stage\\\\n-        total_boxes = np.fix(total_boxes).astype(np.int32)\\\\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\\\\n-        tempimg = np.zeros((48,48,3,numbox))\\\\n-        for k in range(0,numbox):\\\\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\\\\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\\\\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\\\\n-                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\\\\n-            else:\\\\n-                return np.empty()\\\\n-        tempimg = (tempimg-127.5)*0.0078125\\\\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\\\\n-        out = onet(tempimg1)\\\\n-        out0 = np.transpose(out[0])\\\\n-        out1 = np.transpose(out[1])\\\\n-        out2 = np.transpose(out[2])\\\\n-        score = out2[1,:]\\\\n-        points = out1\\\\n-        ipass = np.where(score>threshold[2])\\\\n-        points = points[:,ipass[0]]\\\\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\\\\n-        mv = out0[:,ipass[0]]\\\\n-\\\\n-        w = total_boxes[:,2]-total_boxes[:,0]+1\\\\n-        h = total_boxes[:,3]-total_boxes[:,1]+1\\\\n-        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\\\\n-        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\\\\n-        if total_boxes.shape[0]>0:\\\\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\\\\n-            pick = nms(total_boxes.copy(), 0.7, \\\\\\\'Min\\\\\\\')\\\\n-            total_boxes = total_boxes[pick,:]\\\\n-            points = points[:,pick]\\\\n-                \\\\n-    return total_boxes, points\\\\n-\\\\n-\\\\n-def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\\\\n-    """Detects faces in a list of images\\\\n-    images: list containing input images\\\\n-    detection_window_size_ratio: ratio of minimum face size to smallest image dimension\\\\n-    pnet, rnet, onet: caffemodel\\\\n-    threshold: threshold=[th1 th2 th3], th1-3 are three steps\\\\\\\'s threshold [0-1]\\\\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\\\\n-    """\\\\n-    all_scales = [None] * len(images)\\\\n-    images_with_boxes = [None] * len(images)\\\\n-\\\\n-    for i in range(len(images)):\\\\n-        images_with_boxes[i] = {\\\\\\\'total_boxes\\\\\\\': np.empty((0, 9))}\\\\n-\\\\n-    # create scale pyramid\\\\n-    for index, img in enumerate(images):\\\\n-        all_scales[index] = []\\\\n-        h = img.shape[0]\\\\n-        w = img.shape[1]\\\\n-        minsize = int(detection_window_size_ratio * np.minimum(w, h))\\\\n-        factor_count = 0\\\\n-        minl = np.amin([h, w])\\\\n-        if minsize <= 12:\\\\n-            minsize = 12\\\\n-\\\\n-        m = 12.0 / minsize\\\\n-        minl = minl * m\\\\n-        while minl >= 12:\\\\n-            all_scales[index].append(m * np.power(factor, factor_count))\\\\n-            minl = minl * factor\\\\n-            factor_count += 1\\\\n-\\\\n-    # # # # # # # # # # # # #\\\\n-    # first stage - fast proposal network (pnet) to obtain face candidates\\\\n-    # # # # # # # # # # # # #\\\\n-\\\\n-    images_obj_per_resolution = {}\\\\n-\\\\n-    # TODO: use some type of rounding to number module 8 to increase probability that pyramid images will have the same resolution across input images\\\\n-\\\\n-    for index, scales in enumerate(all_scales):\\\\n-        h = images[index].shape[0]\\\\n-        w = images[index].shape[1]\\\\n-\\\\n-        for scale in scales:\\\\n-            hs = int(np.ceil(h * scale))\\\\n-            ws = int(np.ceil(w * scale))\\\\n-\\\\n-            if (ws, hs) not in images_obj_per_resolution:\\\\n-                images_obj_per_resolution[(ws, hs)] = []\\\\n-\\\\n-            im_data = imresample(images[index], (hs, ws))\\\\n-            im_data = (im_data - 127.5) * 0.0078125\\\\n-            img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\\\\n-            images_obj_per_resolution[(ws, hs)].append({\\\\\\\'scale\\\\\\\': scale, \\\\\\\'image\\\\\\\': img_y, \\\\\\\'index\\\\\\\': index})\\\\n-\\\\n-    for resolution in images_obj_per_resolution:\\\\n-        images_per_resolution = [i[\\\\\\\'image\\\\\\\'] for i in images_obj_per_resolution[resolution]]\\\\n-        outs = pnet(images_per_resolution)\\\\n-\\\\n-        for index in range(len(outs[0])):\\\\n-            scale = images_obj_per_resolution[resolution][index][\\\\\\\'scale\\\\\\\']\\\\n-            image_index = images_obj_per_resolution[resolution][index][\\\\\\\'index\\\\\\\']\\\\n-            out0 = np.transpose(outs[0][index], (1, 0, 2))\\\\n-            out1 = np.transpose(outs[1][index], (1, 0, 2))\\\\n-\\\\n-            boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\\\\n-\\\\n-            # inter-scale nms\\\\n-            pick = nms(boxes.copy(), 0.5, \\\\\\\'Union\\\\\\\')\\\\n-            if boxes.size > 0 and pick.size > 0:\\\\n-                boxes = boxes[pick, :]\\\\n-                images_with_boxes[image_index][\\\\\\\'total_boxes\\\\\\\'] = np.append(images_with_boxes[image_index][\\\\\\\'total_boxes\\\\\\\'],\\\\n-                                                                          boxes,\\\\n-                                                                          axis=0)\\\\n-\\\\n-    for index, image_obj in enumerate(images_with_boxes):\\\\n-        numbox = image_obj[\\\\\\\'total_boxes\\\\\\\'].shape[0]\\\\n-        if numbox > 0:\\\\n-            h = images[index].shape[0]\\\\n-            w = images[index].shape[1]\\\\n-            pick = nms(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy(), 0.7, \\\\\\\'Union\\\\\\\')\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = image_obj[\\\\\\\'total_boxes\\\\\\\'][pick, :]\\\\n-            regw = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 2] - image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 0]\\\\n-            regh = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 3] - image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 1]\\\\n-            qq1 = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 0] + image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 5] * regw\\\\n-            qq2 = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 1] + image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 6] * regh\\\\n-            qq3 = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 2] + image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 7] * regw\\\\n-            qq4 = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 3] + image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 8] * regh\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 4]]))\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = rerec(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy())\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 0:4] = np.fix(image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 0:4]).astype(np.int32)\\\\n-            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy(), w, h)\\\\n-\\\\n-            numbox = image_obj[\\\\\\\'total_boxes\\\\\\\'].shape[0]\\\\n-            tempimg = np.zeros((24, 24, 3, numbox))\\\\n-\\\\n-            if numbox > 0:\\\\n-                for k in range(0, numbox):\\\\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\\\\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\\\\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\\\\n-                        tempimg[:, :, :, k] = imresample(tmp, (24, 24))\\\\n-                    else:\\\\n-                        return np.empty()\\\\n-\\\\n-                tempimg = (tempimg - 127.5) * 0.0078125\\\\n-                image_obj[\\\\\\\'rnet_input\\\\\\\'] = np.transpose(tempimg, (3, 1, 0, 2))\\\\n-\\\\n-    # # # # # # # # # # # # #\\\\n-    # second stage - refinement of face candidates with rnet\\\\n-    # # # # # # # # # # # # #\\\\n-\\\\n-    bulk_rnet_input = np.empty((0, 24, 24, 3))\\\\n-    for index, image_obj in enumerate(images_with_boxes):\\\\n-        if \\\\\\\'rnet_input\\\\\\\' in image_obj:\\\\n-            bulk_rnet_input = np.append(bulk_rnet_input, image_obj[\\\\\\\'rnet_input\\\\\\\'], axis=0)\\\\n-\\\\n-    out = rnet(bulk_rnet_input)\\\\n-    out0 = np.transpose(out[0])\\\\n-    out1 = np.transpose(out[1])\\\\n-    score = out1[1, :]\\\\n-\\\\n-    i = 0\\\\n-    for index, image_obj in enumerate(images_with_boxes):\\\\n-        if \\\\\\\'rnet_input\\\\\\\' not in image_obj:\\\\n-            continue\\\\n-\\\\n-        rnet_input_count = image_obj[\\\\\\\'rnet_input\\\\\\\'].shape[0]\\\\n-        score_per_image = score[i:i + rnet_input_count]\\\\n-        out0_per_image = out0[:, i:i + rnet_input_count]\\\\n-\\\\n-        ipass = np.where(score_per_image > threshold[1])\\\\n-        image_obj[\\\\\\\'total_boxes\\\\\\\'] = np.hstack([image_obj[\\\\\\\'total_boxes\\\\\\\'][ipass[0], 0:4].copy(),\\\\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\\\\n-\\\\n-        mv = out0_per_image[:, ipass[0]]\\\\n-\\\\n-        if image_obj[\\\\\\\'total_boxes\\\\\\\'].shape[0] > 0:\\\\n-            h = images[index].shape[0]\\\\n-            w = images[index].shape[1]\\\\n-            pick = nms(image_obj[\\\\\\\'total_boxes\\\\\\\'], 0.7, \\\\\\\'Union\\\\\\\')\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = image_obj[\\\\\\\'total_boxes\\\\\\\'][pick, :]\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = bbreg(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy(), np.transpose(mv[:, pick]))\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = rerec(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy())\\\\n-\\\\n-            numbox = image_obj[\\\\\\\'total_boxes\\\\\\\'].shape[0]\\\\n-\\\\n-            if numbox > 0:\\\\n-                tempimg = np.zeros((48, 48, 3, numbox))\\\\n-                image_obj[\\\\\\\'total_boxes\\\\\\\'] = np.fix(image_obj[\\\\\\\'total_boxes\\\\\\\']).astype(np.int32)\\\\n-                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy(), w, h)\\\\n-\\\\n-                for k in range(0, numbox):\\\\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\\\\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\\\\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\\\\n-                        tempimg[:, :, :, k] = imresample(tmp, (48, 48))\\\\n-                    else:\\\\n-                        return np.empty()\\\\n-                tempimg = (tempimg - 127.5) * 0.0078125\\\\n-                image_obj[\\\\\\\'onet_input\\\\\\\'] = np.transpose(tempimg, (3, 1, 0, 2))\\\\n-\\\\n-        i += rnet_input_count\\\\n-\\\\n-    # # # # # # # # # # # # #\\\\n-    # third stage - further refinement and facial landmarks positions with onet\\\\n-    # # # # # # # # # # # # #\\\\n-\\\\n-    bulk_onet_input = np.empty((0, 48, 48, 3))\\\\n-    for index, image_obj in enumerate(images_with_boxes):\\\\n-        if \\\\\\\'onet_input\\\\\\\' in image_obj:\\\\n-            bulk_onet_input = np.append(bulk_onet_input, image_obj[\\\\\\\'onet_input\\\\\\\'], axis=0)\\\\n-\\\\n-    out = onet(bulk_onet_input)\\\\n-\\\\n-    out0 = np.transpose(out[0])\\\\n-    out1 = np.transpose(out[1])\\\\n-    out2 = np.transpose(out[2])\\\\n-    score = out2[1, :]\\\\n-    points = out1\\\\n-\\\\n-    i = 0\\\\n-    ret = []\\\\n-    for index, image_obj in enumerate(images_with_boxes):\\\\n-        if \\\\\\\'onet_input\\\\\\\' not in image_obj:\\\\n-            ret.append(None)\\\\n-            continue\\\\n-\\\\n-        onet_input_count = image_obj[\\\\\\\'onet_input\\\\\\\'].shape[0]\\\\n-\\\\n-        out0_per_image = out0[:, i:i + onet_input_count]\\\\n-        score_per_image = score[i:i + onet_input_count]\\\\n-        points_per_image = points[:, i:i + onet_input_count]\\\\n-\\\\n-        ipass = np.where(score_per_image > threshold[2])\\\\n-        points_per_image = points_per_image[:, ipass[0]]\\\\n-\\\\n-        image_obj[\\\\\\\'total_boxes\\\\\\\'] = np.hstack([image_obj[\\\\\\\'total_boxes\\\\\\\'][ipass[0], 0:4].copy(),\\\\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\\\\n-        mv = out0_per_image[:, ipass[0]]\\\\n-\\\\n-        w = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 2] - image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 0] + 1\\\\n-        h = image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 3] - image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 1] + 1\\\\n-        points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 0], (5, 1)) - 1\\\\n-        points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'][:, 1], (5, 1)) - 1\\\\n-\\\\n-        if image_obj[\\\\\\\'total_boxes\\\\\\\'].shape[0] > 0:\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = bbreg(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy(), np.transpose(mv))\\\\n-            pick = nms(image_obj[\\\\\\\'total_boxes\\\\\\\'].copy(), 0.7, \\\\\\\'Min\\\\\\\')\\\\n-            image_obj[\\\\\\\'total_boxes\\\\\\\'] = image_obj[\\\\\\\'total_boxes\\\\\\\'][pick, :]\\\\n-            points_per_image = points_per_image[:, pick]\\\\n-\\\\n-            ret.append((image_obj[\\\\\\\'total_boxes\\\\\\\'], points_per_image))\\\\n-        else:\\\\n-            ret.append(None)\\\\n-\\\\n-        i += onet_input_count\\\\n-\\\\n-    return ret\\\\n-\\\\n-\\\\n-# function [boundingbox] = bbreg(boundingbox,reg)\\\\n-def bbreg(boundingbox,reg):\\\\n-    """Calibrate bounding boxes"""\\\\n-    if reg.shape[1]==1:\\\\n-        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\\\\n-\\\\n-    w = boundingbox[:,2]-boundingbox[:,0]+1\\\\n-    h = boundingbox[:,3]-boundingbox[:,1]+1\\\\n-    b1 = boundingbox[:,0]+reg[:,0]*w\\\\n-    b2 = boundingbox[:,1]+reg[:,1]*h\\\\n-    b3 = boundingbox[:,2]+reg[:,2]*w\\\\n-    b4 = boundingbox[:,3]+reg[:,3]*h\\\\n-    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\\\\n-    return boundingbox\\\\n- \\\\n-def generateBoundingBox(imap, reg, scale, t):\\\\n-    """Use heatmap to generate bounding boxes"""\\\\n-    stride=2\\\\n-    cellsize=12\\\\n-\\\\n-    imap = np.transpose(imap)\\\\n-    dx1 = np.transpose(reg[:,:,0])\\\\n-    dy1 = np.transpose(reg[:,:,1])\\\\n-    dx2 = np.transpose(reg[:,:,2])\\\\n-    dy2 = np.transpose(reg[:,:,3])\\\\n-    y, x = np.where(imap >= t)\\\\n-    if y.shape[0]==1:\\\\n-        dx1 = np.flipud(dx1)\\\\n-        dy1 = np.flipud(dy1)\\\\n-        dx2 = np.flipud(dx2)\\\\n-        dy2 = np.flipud(dy2)\\\\n-    score = imap[(y,x)]\\\\n-    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\\\\n-    if reg.size==0:\\\\n-        reg = np.empty((0,3))\\\\n-    bb = np.transpose(np.vstack([y,x]))\\\\n-    q1 = np.fix((stride*bb+1)/scale)\\\\n-    q2 = np.fix((stride*bb+cellsize-1+1)/scale)\\\\n-    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\\\\n-    return boundingbox, reg\\\\n- \\\\n-# function pick = nms(boxes,threshold,type)\\\\n-def nms(boxes, threshold, method):\\\\n-    if boxes.size==0:\\\\n-        return np.empty((0,3))\\\\n-    x1 = boxes[:,0]\\\\n-    y1 = boxes[:,1]\\\\n-    x2 = boxes[:,2]\\\\n-    y2 = boxes[:,3]\\\\n-    s = boxes[:,4]\\\\n-    area = (x2-x1+1) * (y2-y1+1)\\\\n-    I = np.argsort(s)\\\\n-    pick = np.zeros_like(s, dtype=np.int16)\\\\n-    counter = 0\\\\n-    while I.size>0:\\\\n-        i = I[-1]\\\\n-        pick[counter] = i\\\\n-        counter += 1\\\\n-        idx = I[0:-1]\\\\n-        xx1 = np.maximum(x1[i], x1[idx])\\\\n-        yy1 = np.maximum(y1[i], y1[idx])\\\\n-        xx2 = np.minimum(x2[i], x2[idx])\\\\n-        yy2 = np.minimum(y2[i], y2[idx])\\\\n-        w = np.maximum(0.0, xx2-xx1+1)\\\\n-        h = np.maximum(0.0, yy2-yy1+1)\\\\n-        inter = w * h\\\\n-        if method is \\\\\\\'Min\\\\\\\':\\\\n-            o = inter / np.minimum(area[i], area[idx])\\\\n-        else:\\\\n-            o = inter / (area[i] + area[idx] - inter)\\\\n-        I = I[np.where(o<=threshold)]\\\\n-    pick = pick[0:counter]\\\\n-    return pick\\\\n-\\\\n-# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\\\\n-def pad(total_boxes, w, h):\\\\n-    """Compute the padding coordinates (pad the bounding boxes to square)"""\\\\n-    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\\\\n-    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\\\\n-    numbox = total_boxes.shape[0]\\\\n-\\\\n-    dx = np.ones((numbox), dtype=np.int32)\\\\n-    dy = np.ones((numbox), dtype=np.int32)\\\\n-    edx = tmpw.copy().astype(np.int32)\\\\n-    edy = tmph.copy().astype(np.int32)\\\\n-\\\\n-    x = total_boxes[:,0].copy().astype(np.int32)\\\\n-    y = total_boxes[:,1].copy().astype(np.int32)\\\\n-    ex = total_boxes[:,2].copy().astype(np.int32)\\\\n-    ey = total_boxes[:,3].copy().astype(np.int32)\\\\n-\\\\n-    tmp = np.where(ex>w)\\\\n-    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\\\\n-    ex[tmp] = w\\\\n-    \\\\n-    tmp = np.where(ey>h)\\\\n-    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\\\\n-    ey[tmp] = h\\\\n-\\\\n-    tmp = np.where(x<1)\\\\n-    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\\\\n-    x[tmp] = 1\\\\n-\\\\n-    tmp = np.where(y<1)\\\\n-    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\\\\n-    y[tmp] = 1\\\\n-    \\\\n-    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\\\\n-\\\\n-# function [bboxA] = rerec(bboxA)\\\\n-def rerec(bboxA):\\\\n-    """Convert bboxA to square."""\\\\n-    h = bboxA[:,3]-bboxA[:,1]\\\\n-    w = bboxA[:,2]-bboxA[:,0]\\\\n-    l = np.maximum(w, h)\\\\n-    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\\\\n-    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\\\\n-    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\\\\n-    return bboxA\\\\n-\\\\n-def imresample(img, sz):\\\\n-    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\\\\n-    return im_data\\\\n-\\\\n-    # This method is kept for debugging purpose\\\\n-#     h=img.shape[0]\\\\n-#     w=img.shape[1]\\\\n-#     hs, ws = sz\\\\n-#     dx = float(w) / ws\\\\n-#     dy = float(h) / hs\\\\n-#     im_data = np.zeros((hs,ws,3))\\\\n-#     for a1 in range(0,hs):\\\\n-#         for a2 in range(0,ws):\\\\n-#             for a3 in range(0,3):\\\\n-#                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\\\n-#     return im_data\\\\n-\\\\ndiff --git a/create-dataset/align_dataset_mtcnn.py b/create-dataset/align_dataset_mtcnn.py\\\\ndeleted file mode 100644\\\\nindex 2d8110e..0000000\\\\n--- a/create-dataset/align_dataset_mtcnn.py\\\\n+++ /dev/null\\\\n@@ -1,160 +0,0 @@\\\\n-"""Performs face alignment and stores face thumbnails in the output directory."""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from scipy import misc\\\\n-import sys\\\\n-import os\\\\n-import argparse\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import facenet\\\\n-import align.detect_face\\\\n-import random\\\\n-import imageio\\\\n-from time import sleep\\\\n-\\\\n-def main(args):\\\\n-    sleep(random.random())\\\\n-    output_dir = os.path.expanduser(args.output_dir)\\\\n-    if not os.path.exists(output_dir):\\\\n-        os.makedirs(output_dir)\\\\n-    # Store some git revision info in a text file in the log directory\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\n-    facenet.store_revision_info(src_path, output_dir, \\\\\\\' \\\\\\\'.join(sys.argv))\\\\n-    dataset = facenet.get_dataset(args.input_dir)\\\\n-    \\\\n-    print(\\\\\\\'Creating networks and loading parameters\\\\\\\')\\\\n-    \\\\n-    with tf.Graph().as_default():\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-        with sess.as_default():\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\\\\n-    \\\\n-    minsize = 20 # minimum size of face\\\\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\\\\\\\'s threshold\\\\n-    factor = 0.709 # scale factor\\\\n-\\\\n-    # Add a random key to the filename to allow alignment using multiple processes\\\\n-    random_key = np.random.randint(0, high=99999)\\\\n-    bounding_boxes_filename = os.path.join(output_dir, \\\\\\\'bounding_boxes_%05d.txt\\\\\\\' % random_key)\\\\n-    \\\\n-    with open(bounding_boxes_filename, "w") as text_file:\\\\n-        nrof_images_total = 0\\\\n-        nrof_successfully_aligned = 0\\\\n-        if args.random_order:\\\\n-            random.shuffle(dataset)\\\\n-        for cls in dataset:\\\\n-            output_class_dir = os.path.join(output_dir, cls.name)\\\\n-            if not os.path.exists(output_class_dir):\\\\n-                os.makedirs(output_class_dir)\\\\n-                if args.random_order:\\\\n-                    random.shuffle(cls.image_paths)\\\\n-            for image_path in cls.image_paths:\\\\n-                nrof_images_total += 1\\\\n-                filename = os.path.splitext(os.path.split(image_path)[1])[0]\\\\n-                output_filename = os.path.join(output_class_dir, filename+\\\\\\\'.png\\\\\\\')\\\\n-                print(image_path)\\\\n-                if not os.path.exists(output_filename):\\\\n-                    try:\\\\n-                        img = imageio.imread(image_path)\\\\n-                    except (IOError, ValueError, IndexError) as e:\\\\n-                        errorMessage = \\\\\\\'{}: {}\\\\\\\'.format(image_path, e)\\\\n-                        print(errorMessage)\\\\n-                    else:\\\\n-                        if img.ndim<2:\\\\n-                            print(\\\\\\\'Unable to align "%s"\\\\\\\' % image_path)\\\\n-                            text_file.write(\\\\\\\'%s\\\\\\\\n\\\\\\\' % (output_filename))\\\\n-                            continue\\\\n-                        if img.ndim == 2:\\\\n-                            img = facenet.to_rgb(img)\\\\n-                        img = img[:,:,0:3]\\\\n-    \\\\n-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\\\\n-                        nrof_faces = bounding_boxes.shape[0]\\\\n-                        if nrof_faces>0:\\\\n-                            det = bounding_boxes[:,0:4]\\\\n-                            det_arr = []\\\\n-                            img_size = np.asarray(img.shape)[0:2]\\\\n-                            if nrof_faces>1:\\\\n-                                if args.detect_multiple_faces:\\\\n-                                    for i in range(nrof_faces):\\\\n-                                        det_arr.append(np.squeeze(det[i]))\\\\n-                                else:\\\\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\\\\n-                                    img_center = img_size / 2\\\\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\\\\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\\\\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\\\\n-                                    det_arr.append(det[index,:])\\\\n-                            else:\\\\n-                                det_arr.append(np.squeeze(det))\\\\n-\\\\n-                            for i, det in enumerate(det_arr):\\\\n-                                det = np.squeeze(det)\\\\n-                                bb = np.zeros(4, dtype=np.int32)\\\\n-                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\\\\n-                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\\\\n-                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\\\\n-                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\\\\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\\\\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\\\\\\\'bilinear\\\\\\\')\\\\n-                                nrof_successfully_aligned += 1\\\\n-                                filename_base, file_extension = os.path.splitext(output_filename)\\\\n-                                if args.detect_multiple_faces:\\\\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\\\\n-                                else:\\\\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\\\\n-                                misc.imsave(output_filename_n, scaled)\\\\n-                                text_file.write(\\\\\\\'%s %d %d %d %d\\\\\\\\n\\\\\\\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\\\\n-                        else:\\\\n-                            print(\\\\\\\'Unable to align "%s"\\\\\\\' % image_path)\\\\n-                            text_file.write(\\\\\\\'%s\\\\\\\\n\\\\\\\' % (output_filename))\\\\n-                            \\\\n-    print(\\\\\\\'Total number of images: %d\\\\\\\' % nrof_images_total)\\\\n-    print(\\\\\\\'Number of successfully aligned images: %d\\\\\\\' % nrof_successfully_aligned)\\\\n-            \\\\n-\\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'input_dir\\\\\\\', type=str, help=\\\\\\\'Directory with unaligned images.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'output_dir\\\\\\\', type=str, help=\\\\\\\'Directory with aligned face thumbnails.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=182)\\\\n-    parser.add_argument(\\\\\\\'--margin\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Margin for the crop around the bounding box (height, width) in pixels.\\\\\\\', default=44)\\\\n-    parser.add_argument(\\\\\\\'--random_order\\\\\\\', \\\\n-        help=\\\\\\\'Shuffles the order of images to enable alignment using multiple processes.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--gpu_memory_fraction\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--detect_multiple_faces\\\\\\\', type=bool,\\\\n-                        help=\\\\\\\'Detect and align multiple faces per image.\\\\\\\', default=False)\\\\n-    return parser.parse_args(argv)\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/create-dataset/facenet.py b/create-dataset/facenet.py\\\\ndeleted file mode 100644\\\\nindex 0e05676..0000000\\\\n--- a/create-dataset/facenet.py\\\\n+++ /dev/null\\\\n@@ -1,571 +0,0 @@\\\\n-"""Functions for building the face recognition network.\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-# pylint: disable=missing-docstring\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import os\\\\n-from subprocess import Popen, PIPE\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-from scipy import misc\\\\n-from sklearn.model_selection import KFold\\\\n-from scipy import interpolate\\\\n-from tensorflow.python.training import training\\\\n-import random\\\\n-import re\\\\n-from tensorflow.python.platform import gfile\\\\n-import math\\\\n-from six import iteritems\\\\n-\\\\n-def triplet_loss(anchor, positive, negative, alpha):\\\\n-    """Calculate the triplet loss according to the FaceNet paper\\\\n-    \\\\n-    Args:\\\\n-      anchor: the embeddings for the anchor images.\\\\n-      positive: the embeddings for the positive images.\\\\n-      negative: the embeddings for the negative images.\\\\n-  \\\\n-    Returns:\\\\n-      the triplet loss according to the FaceNet paper as a float tensor.\\\\n-    """\\\\n-    with tf.variable_scope(\\\\\\\'triplet_loss\\\\\\\'):\\\\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\\\\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\\\\n-        \\\\n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\\\\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\\\\n-      \\\\n-    return loss\\\\n-  \\\\n-def center_loss(features, label, alfa, nrof_classes):\\\\n-    """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\\\\n-       (http://ydwen.github.io/papers/WenECCV16.pdf)\\\\n-    """\\\\n-    nrof_features = features.get_shape()[1]\\\\n-    centers = tf.get_variable(\\\\\\\'centers\\\\\\\', [nrof_classes, nrof_features], dtype=tf.float32,\\\\n-        initializer=tf.constant_initializer(0), trainable=False)\\\\n-    label = tf.reshape(label, [-1])\\\\n-    centers_batch = tf.gather(centers, label)\\\\n-    diff = (1 - alfa) * (centers_batch - features)\\\\n-    centers = tf.scatter_sub(centers, label, diff)\\\\n-    with tf.control_dependencies([centers]):\\\\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\\\\n-    return loss, centers\\\\n-\\\\n-def get_image_paths_and_labels(dataset):\\\\n-    image_paths_flat = []\\\\n-    labels_flat = []\\\\n-    for i in range(len(dataset)):\\\\n-        image_paths_flat += dataset[i].image_paths\\\\n-        labels_flat += [i] * len(dataset[i].image_paths)\\\\n-    return image_paths_flat, labels_flat\\\\n-\\\\n-def shuffle_examples(image_paths, labels):\\\\n-    shuffle_list = list(zip(image_paths, labels))\\\\n-    random.shuffle(shuffle_list)\\\\n-    image_paths_shuff, labels_shuff = zip(*shuffle_list)\\\\n-    return image_paths_shuff, labels_shuff\\\\n-\\\\n-def random_rotate_image(image):\\\\n-    angle = np.random.uniform(low=-10.0, high=10.0)\\\\n-    return misc.imrotate(image, angle, \\\\\\\'bicubic\\\\\\\')\\\\n-  \\\\n-# 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\\\\n-RANDOM_ROTATE = 1\\\\n-RANDOM_CROP = 2\\\\n-RANDOM_FLIP = 4\\\\n-FIXED_STANDARDIZATION = 8\\\\n-FLIP = 16\\\\n-def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\\\\n-    images_and_labels_list = []\\\\n-    for _ in range(nrof_preprocess_threads):\\\\n-        filenames, label, control = input_queue.dequeue()\\\\n-        images = []\\\\n-        for filename in tf.unstack(filenames):\\\\n-            file_contents = tf.read_file(filename)\\\\n-            image = tf.image.decode_image(file_contents, 3)\\\\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\\\\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \\\\n-                            lambda:tf.identity(image))\\\\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \\\\n-                            lambda:tf.random_crop(image, image_size + (3,)), \\\\n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\\\\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\\\\n-                            lambda:tf.image.random_flip_left_right(image),\\\\n-                            lambda:tf.identity(image))\\\\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\\\\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\\\\n-                            lambda:tf.image.per_image_standardization(image))\\\\n-            image = tf.cond(get_control_flag(control[0], FLIP),\\\\n-                            lambda:tf.image.flip_left_right(image),\\\\n-                            lambda:tf.identity(image))\\\\n-            #pylint: disable=no-member\\\\n-            image.set_shape(image_size + (3,))\\\\n-            images.append(image)\\\\n-        images_and_labels_list.append([images, label])\\\\n-\\\\n-    image_batch, label_batch = tf.train.batch_join(\\\\n-        images_and_labels_list, batch_size=batch_size_placeholder, \\\\n-        shapes=[image_size + (3,), ()], enqueue_many=True,\\\\n-        capacity=4 * nrof_preprocess_threads * 100,\\\\n-        allow_smaller_final_batch=True)\\\\n-    \\\\n-    return image_batch, label_batch\\\\n-\\\\n-def get_control_flag(control, field):\\\\n-    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\\\\n-  \\\\n-def _add_loss_summaries(total_loss):\\\\n-    """Add summaries for losses.\\\\n-  \\\\n-    Generates moving average for all losses and associated summaries for\\\\n-    visualizing the performance of the network.\\\\n-  \\\\n-    Args:\\\\n-      total_loss: Total loss from loss().\\\\n-    Returns:\\\\n-      loss_averages_op: op for generating moving averages of losses.\\\\n-    """\\\\n-    # Compute the moving average of all individual losses and the total loss.\\\\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\\\\\\\'avg\\\\\\\')\\\\n-    losses = tf.get_collection(\\\\\\\'losses\\\\\\\')\\\\n-    loss_averages_op = loss_averages.apply(losses + [total_loss])\\\\n-  \\\\n-    # Attach a scalar summmary to all individual losses and the total loss; do the\\\\n-    # same for the averaged version of the losses.\\\\n-    for l in losses + [total_loss]:\\\\n-        # Name each loss as \\\\\\\'(raw)\\\\\\\' and name the moving average version of the loss\\\\n-        # as the original loss name.\\\\n-        tf.summary.scalar(l.op.name +\\\\\\\' (raw)\\\\\\\', l)\\\\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\\\\n-  \\\\n-    return loss_averages_op\\\\n-\\\\n-def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\\\\n-    # Generate moving averages of all losses and associated summaries.\\\\n-    loss_averages_op = _add_loss_summaries(total_loss)\\\\n-\\\\n-    # Compute gradients.\\\\n-    with tf.control_dependencies([loss_averages_op]):\\\\n-        if optimizer==\\\\\\\'ADAGRAD\\\\\\\':\\\\n-            opt = tf.train.AdagradOptimizer(learning_rate)\\\\n-        elif optimizer==\\\\\\\'ADADELTA\\\\\\\':\\\\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\\\\n-        elif optimizer==\\\\\\\'ADAM\\\\\\\':\\\\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\\\\n-        elif optimizer==\\\\\\\'RMSPROP\\\\\\\':\\\\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\\\\n-        elif optimizer==\\\\\\\'MOM\\\\\\\':\\\\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\\\\n-        else:\\\\n-            raise ValueError(\\\\\\\'Invalid optimization algorithm\\\\\\\')\\\\n-    \\\\n-        grads = opt.compute_gradients(total_loss, update_gradient_vars)\\\\n-        \\\\n-    # Apply gradients.\\\\n-    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\\\\n-  \\\\n-    # Add histograms for trainable variables.\\\\n-    if log_histograms:\\\\n-        for var in tf.trainable_variables():\\\\n-            tf.summary.histogram(var.op.name, var)\\\\n-   \\\\n-    # Add histograms for gradients.\\\\n-    if log_histograms:\\\\n-        for grad, var in grads:\\\\n-            if grad is not None:\\\\n-                tf.summary.histogram(var.op.name + \\\\\\\'/gradients\\\\\\\', grad)\\\\n-  \\\\n-    # Track the moving averages of all trainable variables.\\\\n-    variable_averages = tf.train.ExponentialMovingAverage(\\\\n-        moving_average_decay, global_step)\\\\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\\\\n-  \\\\n-    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\\\\n-        train_op = tf.no_op(name=\\\\\\\'train\\\\\\\')\\\\n-  \\\\n-    return train_op\\\\n-\\\\n-def prewhiten(x):\\\\n-    mean = np.mean(x)\\\\n-    std = np.std(x)\\\\n-    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\\\\n-    y = np.multiply(np.subtract(x, mean), 1/std_adj)\\\\n-    return y  \\\\n-\\\\n-def crop(image, random_crop, image_size):\\\\n-    if image.shape[1]>image_size:\\\\n-        sz1 = int(image.shape[1]//2)\\\\n-        sz2 = int(image_size//2)\\\\n-        if random_crop:\\\\n-            diff = sz1-sz2\\\\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\\\\n-        else:\\\\n-            (h, v) = (0,0)\\\\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\\\\n-    return image\\\\n-  \\\\n-def flip(image, random_flip):\\\\n-    if random_flip and np.random.choice([True, False]):\\\\n-        image = np.fliplr(image)\\\\n-    return image\\\\n-\\\\n-def to_rgb(img):\\\\n-    w, h = img.shape\\\\n-    ret = np.empty((w, h, 3), dtype=np.uint8)\\\\n-    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\\\\n-    return ret\\\\n-  \\\\n-def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\\\\n-    nrof_samples = len(image_paths)\\\\n-    images = np.zeros((nrof_samples, image_size, image_size, 3))\\\\n-    for i in range(nrof_samples):\\\\n-        img = misc.imread(image_paths[i])\\\\n-        if img.ndim == 2:\\\\n-            img = to_rgb(img)\\\\n-        if do_prewhiten:\\\\n-            img = prewhiten(img)\\\\n-        img = crop(img, do_random_crop, image_size)\\\\n-        img = flip(img, do_random_flip)\\\\n-        images[i,:,:,:] = img\\\\n-    return images\\\\n-\\\\n-def get_label_batch(label_data, batch_size, batch_index):\\\\n-    nrof_examples = np.size(label_data, 0)\\\\n-    j = batch_index*batch_size % nrof_examples\\\\n-    if j+batch_size<=nrof_examples:\\\\n-        batch = label_data[j:j+batch_size]\\\\n-    else:\\\\n-        x1 = label_data[j:nrof_examples]\\\\n-        x2 = label_data[0:nrof_examples-j]\\\\n-        batch = np.vstack([x1,x2])\\\\n-    batch_int = batch.astype(np.int64)\\\\n-    return batch_int\\\\n-\\\\n-def get_batch(image_data, batch_size, batch_index):\\\\n-    nrof_examples = np.size(image_data, 0)\\\\n-    j = batch_index*batch_size % nrof_examples\\\\n-    if j+batch_size<=nrof_examples:\\\\n-        batch = image_data[j:j+batch_size,:,:,:]\\\\n-    else:\\\\n-        x1 = image_data[j:nrof_examples,:,:,:]\\\\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\\\\n-        batch = np.vstack([x1,x2])\\\\n-    batch_float = batch.astype(np.float32)\\\\n-    return batch_float\\\\n-\\\\n-def get_triplet_batch(triplets, batch_index, batch_size):\\\\n-    ax, px, nx = triplets\\\\n-    a = get_batch(ax, int(batch_size/3), batch_index)\\\\n-    p = get_batch(px, int(batch_size/3), batch_index)\\\\n-    n = get_batch(nx, int(batch_size/3), batch_index)\\\\n-    batch = np.vstack([a, p, n])\\\\n-    return batch\\\\n-\\\\n-def get_learning_rate_from_file(filename, epoch):\\\\n-    with open(filename, \\\\\\\'r\\\\\\\') as f:\\\\n-        for line in f.readlines():\\\\n-            line = line.split(\\\\\\\'#\\\\\\\', 1)[0]\\\\n-            if line:\\\\n-                par = line.strip().split(\\\\\\\':\\\\\\\')\\\\n-                e = int(par[0])\\\\n-                if par[1]==\\\\\\\'-\\\\\\\':\\\\n-                    lr = -1\\\\n-                else:\\\\n-                    lr = float(par[1])\\\\n-                if e <= epoch:\\\\n-                    learning_rate = lr\\\\n-                else:\\\\n-                    return learning_rate\\\\n-\\\\n-class ImageClass():\\\\n-    "Stores the paths to images for a given class"\\\\n-    def __init__(self, name, image_paths):\\\\n-        self.name = name\\\\n-        self.image_paths = image_paths\\\\n-  \\\\n-    def __str__(self):\\\\n-        return self.name + \\\\\\\', \\\\\\\' + str(len(self.image_paths)) + \\\\\\\' images\\\\\\\'\\\\n-  \\\\n-    def __len__(self):\\\\n-        return len(self.image_paths)\\\\n-  \\\\n-def get_dataset(path, has_class_directories=True):\\\\n-    dataset = []\\\\n-    path_exp = os.path.expanduser(path)\\\\n-    classes = [path for path in os.listdir(path_exp) \\\\\\\\\\\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\\\\n-    classes.sort()\\\\n-    nrof_classes = len(classes)\\\\n-    for i in range(nrof_classes):\\\\n-        class_name = classes[i]\\\\n-        facedir = os.path.join(path_exp, class_name)\\\\n-        image_paths = get_image_paths(facedir)\\\\n-        dataset.append(ImageClass(class_name, image_paths))\\\\n-  \\\\n-    return dataset\\\\n-\\\\n-def get_image_paths(facedir):\\\\n-    image_paths = []\\\\n-    if os.path.isdir(facedir):\\\\n-        images = os.listdir(facedir)\\\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\\\n-    return image_paths\\\\n-  \\\\n-def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\\\n-    if mode==\\\\\\\'SPLIT_CLASSES\\\\\\\':\\\\n-        nrof_classes = len(dataset)\\\\n-        class_indices = np.arange(nrof_classes)\\\\n-        np.random.shuffle(class_indices)\\\\n-        split = int(round(nrof_classes*(1-split_ratio)))\\\\n-        train_set = [dataset[i] for i in class_indices[0:split]]\\\\n-        test_set = [dataset[i] for i in class_indices[split:-1]]\\\\n-    elif mode==\\\\\\\'SPLIT_IMAGES\\\\\\\':\\\\n-        train_set = []\\\\n-        test_set = []\\\\n-        for cls in dataset:\\\\n-            paths = cls.image_paths\\\\n-            np.random.shuffle(paths)\\\\n-            nrof_images_in_class = len(paths)\\\\n-            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\\\\n-            if split==nrof_images_in_class:\\\\n-                split = nrof_images_in_class-1\\\\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\\\\n-                train_set.append(ImageClass(cls.name, paths[:split]))\\\\n-                test_set.append(ImageClass(cls.name, paths[split:]))\\\\n-    else:\\\\n-        raise ValueError(\\\\\\\'Invalid train/test split mode "%s"\\\\\\\' % mode)\\\\n-    return train_set, test_set\\\\n-\\\\n-def load_model(model, input_map=None):\\\\n-    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\\\\n-    #  or if it is a protobuf file with a frozen graph\\\\n-    model_exp = os.path.expanduser(model)\\\\n-    if (os.path.isfile(model_exp)):\\\\n-        print(\\\\\\\'Model filename: %s\\\\\\\' % model_exp)\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\'rb\\\\\\\') as f:\\\\n-            graph_def = tf.GraphDef()\\\\n-            graph_def.ParseFromString(f.read())\\\\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\'\\\\\\\')\\\\n-    else:\\\\n-        print(\\\\\\\'Model directory: %s\\\\\\\' % model_exp)\\\\n-        meta_file, ckpt_file = get_model_filenames(model_exp)\\\\n-        \\\\n-        print(\\\\\\\'Metagraph file: %s\\\\\\\' % meta_file)\\\\n-        print(\\\\\\\'Checkpoint file: %s\\\\\\\' % ckpt_file)\\\\n-      \\\\n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\\\\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\\\\n-    \\\\n-def get_model_filenames(model_dir):\\\\n-    files = os.listdir(model_dir)\\\\n-    meta_files = [s for s in files if s.endswith(\\\\\\\'.meta\\\\\\\')]\\\\n-    if len(meta_files)==0:\\\\n-        raise ValueError(\\\\\\\'No meta file found in the model directory (%s)\\\\\\\' % model_dir)\\\\n-    elif len(meta_files)>1:\\\\n-        raise ValueError(\\\\\\\'There should not be more than one meta file in the model directory (%s)\\\\\\\' % model_dir)\\\\n-    meta_file = meta_files[0]\\\\n-    ckpt = tf.train.get_checkpoint_state(model_dir)\\\\n-    if ckpt and ckpt.model_checkpoint_path:\\\\n-        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\\\\n-        return meta_file, ckpt_file\\\\n-\\\\n-    meta_files = [s for s in files if \\\\\\\'.ckpt\\\\\\\' in s]\\\\n-    max_step = -1\\\\n-    for f in files:\\\\n-        step_str = re.match(r\\\\\\\'(^model-[\\\\\\\\w\\\\\\\\- ]+.ckpt-(\\\\\\\\d+))\\\\\\\', f)\\\\n-        if step_str is not None and len(step_str.groups())>=2:\\\\n-            step = int(step_str.groups()[1])\\\\n-            if step > max_step:\\\\n-                max_step = step\\\\n-                ckpt_file = step_str.groups()[0]\\\\n-    return meta_file, ckpt_file\\\\n-  \\\\n-def distance(embeddings1, embeddings2, distance_metric=0):\\\\n-    if distance_metric==0:\\\\n-        # Euclidian distance\\\\n-        diff = np.subtract(embeddings1, embeddings2)\\\\n-        dist = np.sum(np.square(diff),1)\\\\n-    elif distance_metric==1:\\\\n-        # Distance based on cosine similarity\\\\n-        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\\\\n-        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\\\\n-        similarity = dot / norm\\\\n-        dist = np.arccos(similarity) / math.pi\\\\n-    else:\\\\n-        raise \\\\\\\'Undefined distance metric %d\\\\\\\' % distance_metric \\\\n-        \\\\n-    return dist\\\\n-\\\\n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\\\\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\\\\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\\\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\\\n-    nrof_thresholds = len(thresholds)\\\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\\\n-    \\\\n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\\\\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\\\\n-    accuracy = np.zeros((nrof_folds))\\\\n-    \\\\n-    indices = np.arange(nrof_pairs)\\\\n-    \\\\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\\\n-        if subtract_mean:\\\\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\\\\n-        else:\\\\n-          mean = 0.0\\\\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\\\\n-        \\\\n-        # Find the best threshold for the fold\\\\n-        acc_train = np.zeros((nrof_thresholds))\\\\n-        for threshold_idx, threshold in enumerate(thresholds):\\\\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\\\\n-        best_threshold_index = np.argmax(acc_train)\\\\n-        for threshold_idx, threshold in enumerate(thresholds):\\\\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\\\\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\\\\n-          \\\\n-        tpr = np.mean(tprs,0)\\\\n-        fpr = np.mean(fprs,0)\\\\n-    return tpr, fpr, accuracy\\\\n-\\\\n-def calculate_accuracy(threshold, dist, actual_issame):\\\\n-    predict_issame = np.less(dist, threshold)\\\\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\\\\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\\\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\\\\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\\\\n-  \\\\n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\\\\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\\\\n-    acc = float(tp+tn)/dist.size\\\\n-    return tpr, fpr, acc\\\\n-\\\\n-\\\\n-  \\\\n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\\\\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\\\\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\\\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\\\n-    nrof_thresholds = len(thresholds)\\\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\\\n-    \\\\n-    val = np.zeros(nrof_folds)\\\\n-    far = np.zeros(nrof_folds)\\\\n-    \\\\n-    indices = np.arange(nrof_pairs)\\\\n-    \\\\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\\\n-        if subtract_mean:\\\\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\\\\n-        else:\\\\n-          mean = 0.0\\\\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\\\\n-      \\\\n-        # Find the threshold that gives FAR = far_target\\\\n-        far_train = np.zeros(nrof_thresholds)\\\\n-        for threshold_idx, threshold in enumerate(thresholds):\\\\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\\\\n-        if np.max(far_train)>=far_target:\\\\n-            f = interpolate.interp1d(far_train, thresholds, kind=\\\\\\\'slinear\\\\\\\')\\\\n-            threshold = f(far_target)\\\\n-        else:\\\\n-            threshold = 0.0\\\\n-    \\\\n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\\\\n-  \\\\n-    val_mean = np.mean(val)\\\\n-    far_mean = np.mean(far)\\\\n-    val_std = np.std(val)\\\\n-    return val_mean, val_std, far_mean\\\\n-\\\\n-\\\\n-def calculate_val_far(threshold, dist, actual_issame):\\\\n-    predict_issame = np.less(dist, threshold)\\\\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\\\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\\\n-    n_same = np.sum(actual_issame)\\\\n-    n_diff = np.sum(np.logical_not(actual_issame))\\\\n-    val = float(true_accept) / float(n_same)\\\\n-    far = float(false_accept) / float(n_diff)\\\\n-    return val, far\\\\n-\\\\n-def store_revision_info(src_path, output_dir, arg_string):\\\\n-    try:\\\\n-        # Get git hash\\\\n-        cmd = [\\\\\\\'git\\\\\\\', \\\\\\\'rev-parse\\\\\\\', \\\\\\\'HEAD\\\\\\\']\\\\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\\\\n-        (stdout, _) = gitproc.communicate()\\\\n-        git_hash = stdout.strip()\\\\n-    except OSError as e:\\\\n-        git_hash = \\\\\\\' \\\\\\\'.join(cmd) + \\\\\\\': \\\\\\\' +  e.strerror\\\\n-  \\\\n-    try:\\\\n-        # Get local changes\\\\n-        cmd = [\\\\\\\'git\\\\\\\', \\\\\\\'diff\\\\\\\', \\\\\\\'HEAD\\\\\\\']\\\\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\\\\n-        (stdout, _) = gitproc.communicate()\\\\n-        git_diff = stdout.strip()\\\\n-    except OSError as e:\\\\n-        git_diff = \\\\\\\' \\\\\\\'.join(cmd) + \\\\\\\': \\\\\\\' +  e.strerror\\\\n-    \\\\n-    # Store a text file in the log directory\\\\n-    rev_info_filename = os.path.join(output_dir, \\\\\\\'revision_info.txt\\\\\\\')\\\\n-    with open(rev_info_filename, "w") as text_file:\\\\n-        text_file.write(\\\\\\\'arguments: %s\\\\\\\\n--------------------\\\\\\\\n\\\\\\\' % arg_string)\\\\n-        text_file.write(\\\\\\\'tensorflow version: %s\\\\\\\\n--------------------\\\\\\\\n\\\\\\\' % tf.__version__)  # @UndefinedVariable\\\\n-        text_file.write(\\\\\\\'git hash: %s\\\\\\\\n--------------------\\\\\\\\n\\\\\\\' % git_hash)\\\\n-        text_file.write(\\\\\\\'%s\\\\\\\' % git_diff)\\\\n-\\\\n-def list_variables(filename):\\\\n-    reader = training.NewCheckpointReader(filename)\\\\n-    variable_map = reader.get_variable_to_shape_map()\\\\n-    names = sorted(variable_map.keys())\\\\n-    return names\\\\n-\\\\n-def put_images_on_grid(images, shape=(16,8)):\\\\n-    nrof_images = images.shape[0]\\\\n-    img_size = images.shape[1]\\\\n-    bw = 3\\\\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\\\\n-    for i in range(shape[1]):\\\\n-        x_start = i*(img_size+bw)+bw\\\\n-        for j in range(shape[0]):\\\\n-            img_index = i*shape[0]+j\\\\n-            if img_index>=nrof_images:\\\\n-                break\\\\n-            y_start = j*(img_size+bw)+bw\\\\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\\\\n-        if img_index>=nrof_images:\\\\n-            break\\\\n-    return img\\\\n-\\\\n-def write_arguments_to_file(args, filename):\\\\n-    with open(filename, \\\\\\\'w\\\\\\\') as f:\\\\n-        for key, value in iteritems(vars(args)):\\\\n-            f.write(\\\\\\\'%s: %s\\\\\\\\n\\\\\\\' % (key, str(value)))\\\\ndiff --git a/data/__init__.py b/data/__init__.py\\\\ndeleted file mode 100644\\\\nindex e69de29..0000000\\\\ndiff --git a/data/data_pipe.py b/data/data_pipe.py\\\\ndeleted file mode 100644\\\\nindex bd67f02..0000000\\\\n--- a/data/data_pipe.py\\\\n+++ /dev/null\\\\n@@ -1,122 +0,0 @@\\\\n-from pathlib import Path\\\\n-from torch.utils.data import Dataset, ConcatDataset, DataLoader\\\\n-from torchvision import transforms as trans\\\\n-from torchvision.datasets import ImageFolder\\\\n-from PIL import Image, ImageFile\\\\n-ImageFile.LOAD_TRUNCATED_IMAGES = True\\\\n-import numpy as np\\\\n-import cv2\\\\n-import bcolz\\\\n-import pickle\\\\n-import torch\\\\n-import mxnet as mx\\\\n-from tqdm import tqdm\\\\n-\\\\n-def de_preprocess(tensor):\\\\n-    return tensor*0.5 + 0.5\\\\n-    \\\\n-def get_train_dataset(imgs_folder):\\\\n-    train_transform = trans.Compose([\\\\n-        trans.RandomHorizontalFlip(),\\\\n-        trans.ToTensor(),\\\\n-        trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\\\n-    ])\\\\n-    ds = ImageFolder(imgs_folder, train_transform)\\\\n-    class_num = ds[-1][1] + 1\\\\n-    return ds, class_num\\\\n-\\\\n-def get_train_loader(conf):\\\\n-    if conf.data_mode in [\\\\\\\'ms1m\\\\\\\', \\\\\\\'concat\\\\\\\']:\\\\n-        ms1m_ds, ms1m_class_num = get_train_dataset(conf.ms1m_folder/\\\\\\\'imgs\\\\\\\')\\\\n-        print(\\\\\\\'ms1m loader generated\\\\\\\')\\\\n-    if conf.data_mode in [\\\\\\\'vgg\\\\\\\', \\\\\\\'concat\\\\\\\']:\\\\n-        vgg_ds, vgg_class_num = get_train_dataset(conf.vgg_folder/\\\\\\\'imgs\\\\\\\')\\\\n-        print(\\\\\\\'vgg loader generated\\\\\\\')        \\\\n-    if conf.data_mode == \\\\\\\'vgg\\\\\\\':\\\\n-        ds = vgg_ds\\\\n-        class_num = vgg_class_num\\\\n-    elif conf.data_mode == \\\\\\\'ms1m\\\\\\\':\\\\n-        ds = ms1m_ds\\\\n-        class_num = ms1m_class_num\\\\n-    elif conf.data_mode == \\\\\\\'concat\\\\\\\':\\\\n-        for i,(url,label) in enumerate(vgg_ds.imgs):\\\\n-            vgg_ds.imgs[i] = (url, label + ms1m_class_num)\\\\n-        ds = ConcatDataset([ms1m_ds,vgg_ds])\\\\n-        class_num = vgg_class_num + ms1m_class_num\\\\n-    elif conf.data_mode == \\\\\\\'emore\\\\\\\':\\\\n-        ds, class_num = get_train_dataset(conf.emore_folder/\\\\\\\'imgs\\\\\\\')\\\\n-    loader = DataLoader(ds, batch_size=conf.batch_size, shuffle=True, pin_memory=conf.pin_memory, num_workers=conf.num_workers)\\\\n-    return loader, class_num \\\\n-    \\\\n-def load_bin(path, rootdir, transform, image_size=[112,112]):\\\\n-    if not rootdir.exists():\\\\n-        rootdir.mkdir()\\\\n-    bins, issame_list = pickle.load(open(path, \\\\\\\'rb\\\\\\\'), encoding=\\\\\\\'bytes\\\\\\\')\\\\n-    data = bcolz.fill([len(bins), 3, image_size[0], image_size[1]], dtype=np.float32, rootdir=rootdir, mode=\\\\\\\'w\\\\\\\')\\\\n-    for i in range(len(bins)):\\\\n-        _bin = bins[i]\\\\n-        img = mx.image.imdecode(_bin).asnumpy()\\\\n-        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\\\n-        img = Image.fromarray(img.astype(np.uint8))\\\\n-        data[i, ...] = transform(img)\\\\n-        i += 1\\\\n-        if i % 1000 == 0:\\\\n-            print(\\\\\\\'loading bin\\\\\\\', i)\\\\n-    print(data.shape)\\\\n-    np.save(str(rootdir)+\\\\\\\'_list\\\\\\\', np.array(issame_list))\\\\n-    return data, issame_list\\\\n-\\\\n-def get_val_pair(path, name):\\\\n-    carray = bcolz.carray(rootdir = path/name, mode=\\\\\\\'r\\\\\\\')\\\\n-    issame = np.load(path/\\\\\\\'{}_list.npy\\\\\\\'.format(name))\\\\n-    return carray, issame\\\\n-\\\\n-def get_val_data(data_path):\\\\n-    agedb_30, agedb_30_issame = get_val_pair(data_path, \\\\\\\'agedb_30\\\\\\\')\\\\n-    cfp_fp, cfp_fp_issame = get_val_pair(data_path, \\\\\\\'cfp_fp\\\\\\\')\\\\n-    lfw, lfw_issame = get_val_pair(data_path, \\\\\\\'lfw\\\\\\\')\\\\n-    return agedb_30, cfp_fp, lfw, agedb_30_issame, cfp_fp_issame, lfw_issame\\\\n-\\\\n-def load_mx_rec(rec_path):\\\\n-    save_path = rec_path/\\\\\\\'imgs\\\\\\\'\\\\n-    if not save_path.exists():\\\\n-        save_path.mkdir()\\\\n-    imgrec = mx.recordio.MXIndexedRecordIO(str(rec_path/\\\\\\\'train.idx\\\\\\\'), str(rec_path/\\\\\\\'train.rec\\\\\\\'), \\\\\\\'r\\\\\\\')\\\\n-    img_info = imgrec.read_idx(0)\\\\n-    header,_ = mx.recordio.unpack(img_info)\\\\n-    max_idx = int(header.label[0])\\\\n-    for idx in tqdm(range(1,max_idx)):\\\\n-        img_info = imgrec.read_idx(idx)\\\\n-        header, img = mx.recordio.unpack_img(img_info)\\\\n-        label = int(header.label)\\\\n-        img = Image.fromarray(img)\\\\n-        label_path = save_path/str(label)\\\\n-        if not label_path.exists():\\\\n-            label_path.mkdir()\\\\n-        img.save(label_path/\\\\\\\'{}.jpg\\\\\\\'.format(idx), quality=95)\\\\n-\\\\n-# class train_dataset(Dataset):\\\\n-#     def __init__(self, imgs_bcolz, label_bcolz, h_flip=True):\\\\n-#         self.imgs = bcolz.carray(rootdir = imgs_bcolz)\\\\n-#         self.labels = bcolz.carray(rootdir = label_bcolz)\\\\n-#         self.h_flip = h_flip\\\\n-#         self.length = len(self.imgs) - 1\\\\n-#         if h_flip:\\\\n-#             self.transform = trans.Compose([\\\\n-#                 trans.ToPILImage(),\\\\n-#                 trans.RandomHorizontalFlip(),\\\\n-#                 trans.ToTensor(),\\\\n-#                 trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\\\n-#             ])\\\\n-#         self.class_num = self.labels[-1] + 1\\\\n-        \\\\n-#     def __len__(self):\\\\n-#         return self.length\\\\n-    \\\\n-#     def __getitem__(self, index):\\\\n-#         img = torch.tensor(self.imgs[index+1], dtype=torch.float)\\\\n-#         label = torch.tensor(self.labels[index+1], dtype=torch.long)\\\\n-#         if self.h_flip:\\\\n-#             img = de_preprocess(img)\\\\n-#             img = self.transform(img)\\\\n-#         return img, label\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/data/facebank/Chandler/Chandler.02.png b/data/facebank/Chandler/Chandler.02.png\\\\ndeleted file mode 100644\\\\nindex e824825..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.02.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.03.png b/data/facebank/Chandler/Chandler.03.png\\\\ndeleted file mode 100644\\\\nindex c8ad160..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.03.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.04.png b/data/facebank/Chandler/Chandler.04.png\\\\ndeleted file mode 100644\\\\nindex cd13e74..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.04.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.05.png b/data/facebank/Chandler/Chandler.05.png\\\\ndeleted file mode 100644\\\\nindex 6ac06fb..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.05.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.06.png b/data/facebank/Chandler/Chandler.06.png\\\\ndeleted file mode 100644\\\\nindex e598bd4..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.06.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.07.png b/data/facebank/Chandler/Chandler.07.png\\\\ndeleted file mode 100644\\\\nindex 6a3d942..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.07.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.08.png b/data/facebank/Chandler/Chandler.08.png\\\\ndeleted file mode 100644\\\\nindex 15fdae4..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.08.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.09.png b/data/facebank/Chandler/Chandler.09.png\\\\ndeleted file mode 100644\\\\nindex 962a80d..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.09.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.10.png b/data/facebank/Chandler/Chandler.10.png\\\\ndeleted file mode 100644\\\\nindex bc7707c..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.10.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.11.png b/data/facebank/Chandler/Chandler.11.png\\\\ndeleted file mode 100644\\\\nindex 08f3f50..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.11.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.17.png b/data/facebank/Chandler/Chandler.17.png\\\\ndeleted file mode 100644\\\\nindex 0db2c6c..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.17.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler.png b/data/facebank/Chandler/Chandler.png\\\\ndeleted file mode 100644\\\\nindex f998d6f..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler12.png b/data/facebank/Chandler/Chandler12.png\\\\ndeleted file mode 100644\\\\nindex 4b5720a..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler12.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler13.png b/data/facebank/Chandler/Chandler13.png\\\\ndeleted file mode 100644\\\\nindex aea4252..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler13.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler14.png b/data/facebank/Chandler/Chandler14.png\\\\ndeleted file mode 100644\\\\nindex 5dfdbf3..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler14.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler15.png b/data/facebank/Chandler/Chandler15.png\\\\ndeleted file mode 100644\\\\nindex 4f1efef..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler15.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler16.png b/data/facebank/Chandler/Chandler16.png\\\\ndeleted file mode 100644\\\\nindex 987c894..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler16.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler18.png b/data/facebank/Chandler/Chandler18.png\\\\ndeleted file mode 100644\\\\nindex dc5576f..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler18.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler19.png b/data/facebank/Chandler/Chandler19.png\\\\ndeleted file mode 100644\\\\nindex e407adc..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler19.png and /dev/null differ\\\\ndiff --git a/data/facebank/Chandler/Chandler20.png b/data/facebank/Chandler/Chandler20.png\\\\ndeleted file mode 100644\\\\nindex 6fc1ab3..0000000\\\\nBinary files a/data/facebank/Chandler/Chandler20.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.01.png b/data/facebank/Joey/Joey.01.png\\\\ndeleted file mode 100644\\\\nindex 3c685d0..0000000\\\\nBinary files a/data/facebank/Joey/Joey.01.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.02.png b/data/facebank/Joey/Joey.02.png\\\\ndeleted file mode 100644\\\\nindex 9ca5128..0000000\\\\nBinary files a/data/facebank/Joey/Joey.02.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.03.png b/data/facebank/Joey/Joey.03.png\\\\ndeleted file mode 100644\\\\nindex 6d2caca..0000000\\\\nBinary files a/data/facebank/Joey/Joey.03.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.04.png b/data/facebank/Joey/Joey.04.png\\\\ndeleted file mode 100644\\\\nindex f698017..0000000\\\\nBinary files a/data/facebank/Joey/Joey.04.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.05.png b/data/facebank/Joey/Joey.05.png\\\\ndeleted file mode 100644\\\\nindex 11c4831..0000000\\\\nBinary files a/data/facebank/Joey/Joey.05.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.06.png b/data/facebank/Joey/Joey.06.png\\\\ndeleted file mode 100644\\\\nindex 96f7ed3..0000000\\\\nBinary files a/data/facebank/Joey/Joey.06.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.07.png b/data/facebank/Joey/Joey.07.png\\\\ndeleted file mode 100644\\\\nindex 1e165fa..0000000\\\\nBinary files a/data/facebank/Joey/Joey.07.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.08.png b/data/facebank/Joey/Joey.08.png\\\\ndeleted file mode 100644\\\\nindex 09144d8..0000000\\\\nBinary files a/data/facebank/Joey/Joey.08.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.09.png b/data/facebank/Joey/Joey.09.png\\\\ndeleted file mode 100644\\\\nindex 7ed417e..0000000\\\\nBinary files a/data/facebank/Joey/Joey.09.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.11.png b/data/facebank/Joey/Joey.11.png\\\\ndeleted file mode 100644\\\\nindex 580bc53..0000000\\\\nBinary files a/data/facebank/Joey/Joey.11.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.12.png b/data/facebank/Joey/Joey.12.png\\\\ndeleted file mode 100644\\\\nindex f298964..0000000\\\\nBinary files a/data/facebank/Joey/Joey.12.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.13.png b/data/facebank/Joey/Joey.13.png\\\\ndeleted file mode 100644\\\\nindex 0498bc2..0000000\\\\nBinary files a/data/facebank/Joey/Joey.13.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.14.png b/data/facebank/Joey/Joey.14.png\\\\ndeleted file mode 100644\\\\nindex 54b8ddc..0000000\\\\nBinary files a/data/facebank/Joey/Joey.14.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.15.png b/data/facebank/Joey/Joey.15.png\\\\ndeleted file mode 100644\\\\nindex c2a4757..0000000\\\\nBinary files a/data/facebank/Joey/Joey.15.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.16.png b/data/facebank/Joey/Joey.16.png\\\\ndeleted file mode 100644\\\\nindex e3bc9b8..0000000\\\\nBinary files a/data/facebank/Joey/Joey.16.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.17.png b/data/facebank/Joey/Joey.17.png\\\\ndeleted file mode 100644\\\\nindex 4e9312e..0000000\\\\nBinary files a/data/facebank/Joey/Joey.17.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.18.png b/data/facebank/Joey/Joey.18.png\\\\ndeleted file mode 100644\\\\nindex 936d8a5..0000000\\\\nBinary files a/data/facebank/Joey/Joey.18.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.19.png b/data/facebank/Joey/Joey.19.png\\\\ndeleted file mode 100644\\\\nindex bdfb07d..0000000\\\\nBinary files a/data/facebank/Joey/Joey.19.png and /dev/null differ\\\\ndiff --git a/data/facebank/Joey/Joey.20.png b/data/facebank/Joey/Joey.20.png\\\\ndeleted file mode 100644\\\\nindex b78b6bf..0000000\\\\nBinary files a/data/facebank/Joey/Joey.20.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica.02.png b/data/facebank/Monica/Monica.02.png\\\\ndeleted file mode 100644\\\\nindex 1a05511..0000000\\\\nBinary files a/data/facebank/Monica/Monica.02.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica.03.png b/data/facebank/Monica/Monica.03.png\\\\ndeleted file mode 100644\\\\nindex 1f58157..0000000\\\\nBinary files a/data/facebank/Monica/Monica.03.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica.04.png b/data/facebank/Monica/Monica.04.png\\\\ndeleted file mode 100644\\\\nindex 05a254a..0000000\\\\nBinary files a/data/facebank/Monica/Monica.04.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica.05.png b/data/facebank/Monica/Monica.05.png\\\\ndeleted file mode 100644\\\\nindex f97be0d..0000000\\\\nBinary files a/data/facebank/Monica/Monica.05.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica.08.png b/data/facebank/Monica/Monica.08.png\\\\ndeleted file mode 100644\\\\nindex 4ccbc47..0000000\\\\nBinary files a/data/facebank/Monica/Monica.08.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica.png b/data/facebank/Monica/Monica.png\\\\ndeleted file mode 100644\\\\nindex 758c3c6..0000000\\\\nBinary files a/data/facebank/Monica/Monica.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica10.png b/data/facebank/Monica/Monica10.png\\\\ndeleted file mode 100644\\\\nindex 8994753..0000000\\\\nBinary files a/data/facebank/Monica/Monica10.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica11.png b/data/facebank/Monica/Monica11.png\\\\ndeleted file mode 100644\\\\nindex 357b8bc..0000000\\\\nBinary files a/data/facebank/Monica/Monica11.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica12.png b/data/facebank/Monica/Monica12.png\\\\ndeleted file mode 100644\\\\nindex dc9ac29..0000000\\\\nBinary files a/data/facebank/Monica/Monica12.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica13.png b/data/facebank/Monica/Monica13.png\\\\ndeleted file mode 100644\\\\nindex d7c2cf6..0000000\\\\nBinary files a/data/facebank/Monica/Monica13.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica14.png b/data/facebank/Monica/Monica14.png\\\\ndeleted file mode 100644\\\\nindex a528414..0000000\\\\nBinary files a/data/facebank/Monica/Monica14.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica15.png b/data/facebank/Monica/Monica15.png\\\\ndeleted file mode 100644\\\\nindex 5bfe1a9..0000000\\\\nBinary files a/data/facebank/Monica/Monica15.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica16.png b/data/facebank/Monica/Monica16.png\\\\ndeleted file mode 100644\\\\nindex f94e645..0000000\\\\nBinary files a/data/facebank/Monica/Monica16.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica17.png b/data/facebank/Monica/Monica17.png\\\\ndeleted file mode 100644\\\\nindex bf24311..0000000\\\\nBinary files a/data/facebank/Monica/Monica17.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica18.png b/data/facebank/Monica/Monica18.png\\\\ndeleted file mode 100644\\\\nindex b04f261..0000000\\\\nBinary files a/data/facebank/Monica/Monica18.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica19.png b/data/facebank/Monica/Monica19.png\\\\ndeleted file mode 100644\\\\nindex 0eeb0d6..0000000\\\\nBinary files a/data/facebank/Monica/Monica19.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica20.png b/data/facebank/Monica/Monica20.png\\\\ndeleted file mode 100644\\\\nindex e5d4e24..0000000\\\\nBinary files a/data/facebank/Monica/Monica20.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica6.png b/data/facebank/Monica/Monica6.png\\\\ndeleted file mode 100644\\\\nindex 7112741..0000000\\\\nBinary files a/data/facebank/Monica/Monica6.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica7.png b/data/facebank/Monica/Monica7.png\\\\ndeleted file mode 100644\\\\nindex 3567770..0000000\\\\nBinary files a/data/facebank/Monica/Monica7.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica8.png b/data/facebank/Monica/Monica8.png\\\\ndeleted file mode 100644\\\\nindex de723c2..0000000\\\\nBinary files a/data/facebank/Monica/Monica8.png and /dev/null differ\\\\ndiff --git a/data/facebank/Monica/Monica9.png b/data/facebank/Monica/Monica9.png\\\\ndeleted file mode 100644\\\\nindex c7ae768..0000000\\\\nBinary files a/data/facebank/Monica/Monica9.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe06.png b/data/facebank/Phoebe/Pheobe06.png\\\\ndeleted file mode 100644\\\\nindex 3270a78..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe06.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe07.png b/data/facebank/Phoebe/Pheobe07.png\\\\ndeleted file mode 100644\\\\nindex 25cb4fd..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe07.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe08.png b/data/facebank/Phoebe/Pheobe08.png\\\\ndeleted file mode 100644\\\\nindex cd5d7e0..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe08.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe09.png b/data/facebank/Phoebe/Pheobe09.png\\\\ndeleted file mode 100644\\\\nindex 4df4c47..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe09.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe10.png b/data/facebank/Phoebe/Pheobe10.png\\\\ndeleted file mode 100644\\\\nindex 8637633..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe10.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe11.png b/data/facebank/Phoebe/Pheobe11.png\\\\ndeleted file mode 100644\\\\nindex 7ad6ed9..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe11.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe12.png b/data/facebank/Phoebe/Pheobe12.png\\\\ndeleted file mode 100644\\\\nindex 7ce09dc..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe12.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe13.png b/data/facebank/Phoebe/Pheobe13.png\\\\ndeleted file mode 100644\\\\nindex 17b6c11..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe13.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe15.png b/data/facebank/Phoebe/Pheobe15.png\\\\ndeleted file mode 100644\\\\nindex 6c47f10..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe15.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe16.png b/data/facebank/Phoebe/Pheobe16.png\\\\ndeleted file mode 100644\\\\nindex 56775ea..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe16.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe17.png b/data/facebank/Phoebe/Pheobe17.png\\\\ndeleted file mode 100644\\\\nindex 0897eb9..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe17.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe18.png b/data/facebank/Phoebe/Pheobe18.png\\\\ndeleted file mode 100644\\\\nindex 2579394..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe18.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe19.png b/data/facebank/Phoebe/Pheobe19.png\\\\ndeleted file mode 100644\\\\nindex 111693b..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe19.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Pheobe20.png b/data/facebank/Phoebe/Pheobe20.png\\\\ndeleted file mode 100644\\\\nindex 82ecd08..0000000\\\\nBinary files a/data/facebank/Phoebe/Pheobe20.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Phoebe.02.png b/data/facebank/Phoebe/Phoebe.02.png\\\\ndeleted file mode 100644\\\\nindex 5a67bbc..0000000\\\\nBinary files a/data/facebank/Phoebe/Phoebe.02.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Phoebe.03.png b/data/facebank/Phoebe/Phoebe.03.png\\\\ndeleted file mode 100644\\\\nindex 43a1ae2..0000000\\\\nBinary files a/data/facebank/Phoebe/Phoebe.03.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Phoebe.04.png b/data/facebank/Phoebe/Phoebe.04.png\\\\ndeleted file mode 100644\\\\nindex e221b0d..0000000\\\\nBinary files a/data/facebank/Phoebe/Phoebe.04.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Phoebe.05.png b/data/facebank/Phoebe/Phoebe.05.png\\\\ndeleted file mode 100644\\\\nindex 96a6629..0000000\\\\nBinary files a/data/facebank/Phoebe/Phoebe.05.png and /dev/null differ\\\\ndiff --git a/data/facebank/Phoebe/Phoebe.png b/data/facebank/Phoebe/Phoebe.png\\\\ndeleted file mode 100644\\\\nindex befbda6..0000000\\\\nBinary files a/data/facebank/Phoebe/Phoebe.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel.02.png b/data/facebank/Rachel/Rachel.02.png\\\\ndeleted file mode 100644\\\\nindex e62a99a..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel.02.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel.03.png b/data/facebank/Rachel/Rachel.03.png\\\\ndeleted file mode 100644\\\\nindex f4ac8fa..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel.03.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel.04.png b/data/facebank/Rachel/Rachel.04.png\\\\ndeleted file mode 100644\\\\nindex 1ff0d0c..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel.04.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel.05.png b/data/facebank/Rachel/Rachel.05.png\\\\ndeleted file mode 100644\\\\nindex 2d69283..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel.05.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel01.png b/data/facebank/Rachel/Rachel01.png\\\\ndeleted file mode 100644\\\\nindex 91e0fd9..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel01.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel06.png b/data/facebank/Rachel/Rachel06.png\\\\ndeleted file mode 100644\\\\nindex be80ce0..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel06.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel07.png b/data/facebank/Rachel/Rachel07.png\\\\ndeleted file mode 100644\\\\nindex 7f1c0d8..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel07.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel08.png b/data/facebank/Rachel/Rachel08.png\\\\ndeleted file mode 100644\\\\nindex 2295162..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel08.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel09.png b/data/facebank/Rachel/Rachel09.png\\\\ndeleted file mode 100644\\\\nindex 49dfe08..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel09.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel10.png b/data/facebank/Rachel/Rachel10.png\\\\ndeleted file mode 100644\\\\nindex bae25a1..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel10.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel11.png b/data/facebank/Rachel/Rachel11.png\\\\ndeleted file mode 100644\\\\nindex 0e39cef..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel11.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel12.png b/data/facebank/Rachel/Rachel12.png\\\\ndeleted file mode 100644\\\\nindex d0945d5..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel12.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel13.png b/data/facebank/Rachel/Rachel13.png\\\\ndeleted file mode 100644\\\\nindex 9a942f2..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel13.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel14.png b/data/facebank/Rachel/Rachel14.png\\\\ndeleted file mode 100644\\\\nindex 3786268..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel14.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel15.png b/data/facebank/Rachel/Rachel15.png\\\\ndeleted file mode 100644\\\\nindex c8b5b67..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel15.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel16.png b/data/facebank/Rachel/Rachel16.png\\\\ndeleted file mode 100644\\\\nindex 8fa13b5..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel16.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel17.png b/data/facebank/Rachel/Rachel17.png\\\\ndeleted file mode 100644\\\\nindex dcd563d..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel17.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel18.png b/data/facebank/Rachel/Rachel18.png\\\\ndeleted file mode 100644\\\\nindex 5d58bc1..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel18.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel19.png b/data/facebank/Rachel/Rachel19.png\\\\ndeleted file mode 100644\\\\nindex 5f9778a..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel19.png and /dev/null differ\\\\ndiff --git a/data/facebank/Rachel/Rachel20.png b/data/facebank/Rachel/Rachel20.png\\\\ndeleted file mode 100644\\\\nindex 7d62102..0000000\\\\nBinary files a/data/facebank/Rachel/Rachel20.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/Ross03.png b/data/facebank/Ross/Ross03.png\\\\ndeleted file mode 100644\\\\nindex d8c2c7d..0000000\\\\nBinary files a/data/facebank/Ross/Ross03.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/Ross04.png b/data/facebank/Ross/Ross04.png\\\\ndeleted file mode 100644\\\\nindex b3ccc1b..0000000\\\\nBinary files a/data/facebank/Ross/Ross04.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/Ross05.png b/data/facebank/Ross/Ross05.png\\\\ndeleted file mode 100644\\\\nindex 95fda3f..0000000\\\\nBinary files a/data/facebank/Ross/Ross05.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross01.png b/data/facebank/Ross/ross01.png\\\\ndeleted file mode 100644\\\\nindex 3459b69..0000000\\\\nBinary files a/data/facebank/Ross/ross01.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross02.png b/data/facebank/Ross/ross02.png\\\\ndeleted file mode 100644\\\\nindex 332c896..0000000\\\\nBinary files a/data/facebank/Ross/ross02.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross06.png b/data/facebank/Ross/ross06.png\\\\ndeleted file mode 100644\\\\nindex 8cc4528..0000000\\\\nBinary files a/data/facebank/Ross/ross06.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross07.png b/data/facebank/Ross/ross07.png\\\\ndeleted file mode 100644\\\\nindex f26cf1d..0000000\\\\nBinary files a/data/facebank/Ross/ross07.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross08.png b/data/facebank/Ross/ross08.png\\\\ndeleted file mode 100644\\\\nindex 76b0a88..0000000\\\\nBinary files a/data/facebank/Ross/ross08.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross09.png b/data/facebank/Ross/ross09.png\\\\ndeleted file mode 100644\\\\nindex 50d07c8..0000000\\\\nBinary files a/data/facebank/Ross/ross09.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross10.png b/data/facebank/Ross/ross10.png\\\\ndeleted file mode 100644\\\\nindex 64651c2..0000000\\\\nBinary files a/data/facebank/Ross/ross10.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross11.png b/data/facebank/Ross/ross11.png\\\\ndeleted file mode 100644\\\\nindex bfc016b..0000000\\\\nBinary files a/data/facebank/Ross/ross11.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross12.png b/data/facebank/Ross/ross12.png\\\\ndeleted file mode 100644\\\\nindex d65cc34..0000000\\\\nBinary files a/data/facebank/Ross/ross12.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross13.png b/data/facebank/Ross/ross13.png\\\\ndeleted file mode 100644\\\\nindex 2671298..0000000\\\\nBinary files a/data/facebank/Ross/ross13.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross14.png b/data/facebank/Ross/ross14.png\\\\ndeleted file mode 100644\\\\nindex c239eea..0000000\\\\nBinary files a/data/facebank/Ross/ross14.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross15.png b/data/facebank/Ross/ross15.png\\\\ndeleted file mode 100644\\\\nindex 7326319..0000000\\\\nBinary files a/data/facebank/Ross/ross15.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross16.png b/data/facebank/Ross/ross16.png\\\\ndeleted file mode 100644\\\\nindex 7786cdd..0000000\\\\nBinary files a/data/facebank/Ross/ross16.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross17.png b/data/facebank/Ross/ross17.png\\\\ndeleted file mode 100644\\\\nindex 0c70f90..0000000\\\\nBinary files a/data/facebank/Ross/ross17.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross18.png b/data/facebank/Ross/ross18.png\\\\ndeleted file mode 100644\\\\nindex 06d916d..0000000\\\\nBinary files a/data/facebank/Ross/ross18.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross19.png b/data/facebank/Ross/ross19.png\\\\ndeleted file mode 100644\\\\nindex 8f79393..0000000\\\\nBinary files a/data/facebank/Ross/ross19.png and /dev/null differ\\\\ndiff --git a/data/facebank/Ross/ross20.png b/data/facebank/Ross/ross20.png\\\\ndeleted file mode 100644\\\\nindex 6621203..0000000\\\\nBinary files a/data/facebank/Ross/ross20.png and /dev/null differ\\\\ndiff --git a/data/facebank/Suyash/image1.png b/data/facebank/Suyash/image1.png\\\\ndeleted file mode 100644\\\\nindex 0d8a120..0000000\\\\nBinary files a/data/facebank/Suyash/image1.png and /dev/null differ\\\\ndiff --git a/data/facebank/Suyash/image2.png b/data/facebank/Suyash/image2.png\\\\ndeleted file mode 100644\\\\nindex 6d14e96..0000000\\\\nBinary files a/data/facebank/Suyash/image2.png and /dev/null differ\\\\ndiff --git a/data/facebank/Suyash/image3.png b/data/facebank/Suyash/image3.png\\\\ndeleted file mode 100644\\\\nindex 7266fe4..0000000\\\\nBinary files a/data/facebank/Suyash/image3.png and /dev/null differ\\\\ndiff --git a/data/facebank/Suyash/image4.png b/data/facebank/Suyash/image4.png\\\\ndeleted file mode 100644\\\\nindex ecfef1a..0000000\\\\nBinary files a/data/facebank/Suyash/image4.png and /dev/null differ\\\\ndiff --git a/data/facebank/Suyash/image5.png b/data/facebank/Suyash/image5.png\\\\ndeleted file mode 100644\\\\nindex 23495e2..0000000\\\\nBinary files a/data/facebank/Suyash/image5.png and /dev/null differ\\\\ndiff --git a/data/facebank/facebank.pth b/data/facebank/facebank.pth\\\\ndeleted file mode 100644\\\\nindex 5fdbbbc..0000000\\\\n--- a/data/facebank/facebank.pth\\\\n+++ /dev/null\\\\n@@ -1,3 +0,0 @@\\\\n-version https://git-lfs.github.com/spec/v1\\\\n-oid sha256:01da0a10ca9fae085c25f535aed1241340a3bad79a49c1f2ec28b07aaa3307d0\\\\n-size 15083\\\\ndiff --git a/data/facebank/names.npy b/data/facebank/names.npy\\\\ndeleted file mode 100644\\\\nindex 108dbb2..0000000\\\\nBinary files a/data/facebank/names.npy and /dev/null differ\\\\ndiff --git a/face_verify.py b/face_verify.py\\\\ndeleted file mode 100644\\\\nindex 7f7d96b..0000000\\\\n--- a/face_verify.py\\\\n+++ /dev/null\\\\n@@ -1,84 +0,0 @@\\\\n-import cv2\\\\n-from PIL import Image\\\\n-import argparse\\\\n-from pathlib import Path\\\\n-from multiprocessing import Process, Pipe,Value,Array\\\\n-import torch\\\\n-from config import get_config\\\\n-from mtcnn import MTCNN\\\\n-from Learner import face_learner\\\\n-from utils import load_facebank, draw_box_name, prepare_facebank\\\\n-\\\\n-\\\\n-parser = argparse.ArgumentParser(description=\\\\\\\'for face verification\\\\\\\')\\\\n-parser.add_argument("-s", "--save", help="whether save",action="store_true")\\\\n-parser.add_argument(\\\\\\\'-th\\\\\\\',\\\\\\\'--threshold\\\\\\\',help=\\\\\\\'threshold to decide identical faces\\\\\\\',default=1.54, type=float)\\\\n-parser.add_argument("-u", "--update", help="whether perform update the facebank",action="store_true")\\\\n-parser.add_argument("-tta", "--tta", help="whether test time augmentation",action="store_true")\\\\n-parser.add_argument("-c", "--score", help="whether show the confidence score",action="store_true")\\\\n-args = parser.parse_args()\\\\n-\\\\n-conf = get_config(False)\\\\n-\\\\n-mtcnn = MTCNN()\\\\n-print(\\\\\\\'arcface loaded\\\\\\\')\\\\n-\\\\n-learner = face_learner(conf, True)\\\\n-learner.threshold = args.threshold\\\\n-if conf.device.type == \\\\\\\'cpu\\\\\\\':\\\\n-    learner.load_state(conf, \\\\\\\'cpu_final.pth\\\\\\\', True, True)\\\\n-else:\\\\n-    learner.load_state(conf, \\\\\\\'final.pth\\\\\\\', True, True)\\\\n-learner.model.eval()\\\\n-print(\\\\\\\'learner loaded\\\\\\\')\\\\n-\\\\n-if args.update:\\\\n-    targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\\\\n-    print(\\\\\\\'facebank updated\\\\\\\')\\\\n-else:\\\\n-    targets, names = load_facebank(conf)\\\\n-    print(\\\\\\\'facebank loaded\\\\\\\')\\\\n-\\\\n-# inital camera\\\\n-\\\\n-# cap = cv2.VideoCapture(r\\\\\\\'C:/Users/suyas/Desktop/Face-Recognition-master/test2.mp4\\\\\\\')\\\\n-cap = cv2.VideoCapture(0)\\\\n-cap.set(3,500)\\\\n-cap.set(4,500)\\\\n-\\\\n-\\\\n-class faceRec:\\\\n-    def __init__(self):\\\\n-        self.width = 800\\\\n-        self.height = 800\\\\n-        self.image = None\\\\n-    def main(self): \\\\n-        while cap.isOpened():\\\\n-            isSuccess,frame = cap.read()\\\\n-            if isSuccess:            \\\\n-                try:\\\\n-    #                 image = Image.fromarray(frame[...,::-1]) #bgr to rgb\\\\n-                    image = Image.fromarray(frame)\\\\n-                    bboxes, faces = mtcnn.align_multi(image, conf.face_limit, conf.min_face_size)\\\\n-                    bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\\\\n-                    bboxes = bboxes.astype(int)\\\\n-                    bboxes = bboxes + [-1,-1,1,1] # personal choice    \\\\n-                    results, score = learner.infer(conf, faces, targets, args.tta)\\\\n-                    for idx,bbox in enumerate(bboxes):\\\\n-                        if args.score:\\\\n-                            frame = draw_box_name(bbox, names[results[idx] + 1] + \\\\\\\'_{:.2f}\\\\\\\'.format(score[idx]), frame)\\\\n-                        else:\\\\n-                            frame = draw_box_name(bbox, names[results[idx] + 1], frame)\\\\n-                except:\\\\n-                    pass    \\\\n-                ret, jpeg = cv2.imencode(\\\\\\\'.jpg\\\\\\\', frame)\\\\n-                return jpeg.tostring()\\\\n-                # cv2.imshow(\\\\\\\'Arc Face Recognizer\\\\\\\', frame)\\\\n-\\\\n-\\\\n-            if cv2.waitKey(1)&0xFF == ord(\\\\\\\'q\\\\\\\'):\\\\n-                break\\\\n-\\\\n-        cap.release()\\\\n-\\\\n-        cv2.destroyAllWindows()    \\\\ndiff --git a/model.py b/model.py\\\\ndeleted file mode 100644\\\\nindex 6645455..0000000\\\\n--- a/model.py\\\\n+++ /dev/null\\\\n@@ -1,306 +0,0 @@\\\\n-from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\\\\n-import torch.nn.functional as F\\\\n-import torch\\\\n-from collections import namedtuple\\\\n-import math\\\\n-import pdb\\\\n-\\\\n-##################################  Original Arcface Model #############################################################\\\\n-\\\\n-class Flatten(Module):\\\\n-    def forward(self, input):\\\\n-        return input.view(input.size(0), -1)\\\\n-\\\\n-def l2_norm(input,axis=1):\\\\n-    norm = torch.norm(input,2,axis,True)\\\\n-    output = torch.div(input, norm)\\\\n-    return output\\\\n-\\\\n-class SEModule(Module):\\\\n-    def __init__(self, channels, reduction):\\\\n-        super(SEModule, self).__init__()\\\\n-        self.avg_pool = AdaptiveAvgPool2d(1)\\\\n-        self.fc1 = Conv2d(\\\\n-            channels, channels // reduction, kernel_size=1, padding=0 ,bias=False)\\\\n-        self.relu = ReLU(inplace=True)\\\\n-        self.fc2 = Conv2d(\\\\n-            channels // reduction, channels, kernel_size=1, padding=0 ,bias=False)\\\\n-        self.sigmoid = Sigmoid()\\\\n-\\\\n-    def forward(self, x):\\\\n-        module_input = x\\\\n-        x = self.avg_pool(x)\\\\n-        x = self.fc1(x)\\\\n-        x = self.relu(x)\\\\n-        x = self.fc2(x)\\\\n-        x = self.sigmoid(x)\\\\n-        return module_input * x\\\\n-\\\\n-class bottleneck_IR(Module):\\\\n-    def __init__(self, in_channel, depth, stride):\\\\n-        super(bottleneck_IR, self).__init__()\\\\n-        if in_channel == depth:\\\\n-            self.shortcut_layer = MaxPool2d(1, stride)\\\\n-        else:\\\\n-            self.shortcut_layer = Sequential(\\\\n-                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), BatchNorm2d(depth))\\\\n-        self.res_layer = Sequential(\\\\n-            BatchNorm2d(in_channel),\\\\n-            Conv2d(in_channel, depth, (3, 3), (1, 1), 1 ,bias=False), PReLU(depth),\\\\n-            Conv2d(depth, depth, (3, 3), stride, 1 ,bias=False), BatchNorm2d(depth))\\\\n-\\\\n-    def forward(self, x):\\\\n-        shortcut = self.shortcut_layer(x)\\\\n-        res = self.res_layer(x)\\\\n-        return res + shortcut\\\\n-\\\\n-class bottleneck_IR_SE(Module):\\\\n-    def __init__(self, in_channel, depth, stride):\\\\n-        super(bottleneck_IR_SE, self).__init__()\\\\n-        if in_channel == depth:\\\\n-            self.shortcut_layer = MaxPool2d(1, stride)\\\\n-        else:\\\\n-            self.shortcut_layer = Sequential(\\\\n-                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), \\\\n-                BatchNorm2d(depth))\\\\n-        self.res_layer = Sequential(\\\\n-            BatchNorm2d(in_channel),\\\\n-            Conv2d(in_channel, depth, (3,3), (1,1),1 ,bias=False),\\\\n-            PReLU(depth),\\\\n-            Conv2d(depth, depth, (3,3), stride, 1 ,bias=False),\\\\n-            BatchNorm2d(depth),\\\\n-            SEModule(depth,16)\\\\n-            )\\\\n-    def forward(self,x):\\\\n-        shortcut = self.shortcut_layer(x)\\\\n-        res = self.res_layer(x)\\\\n-        return res + shortcut\\\\n-\\\\n-class Bottleneck(namedtuple(\\\\\\\'Block\\\\\\\', [\\\\\\\'in_channel\\\\\\\', \\\\\\\'depth\\\\\\\', \\\\\\\'stride\\\\\\\'])):\\\\n-    \\\\\\\'\\\\\\\'\\\\\\\'A named tuple describing a ResNet block.\\\\\\\'\\\\\\\'\\\\\\\'\\\\n-    \\\\n-def get_block(in_channel, depth, num_units, stride = 2):\\\\n-    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units-1)]\\\\n-\\\\n-def get_blocks(num_layers):\\\\n-    if num_layers == 50:\\\\n-        blocks = [\\\\n-            get_block(in_channel=64, depth=64, num_units = 3),\\\\n-            get_block(in_channel=64, depth=128, num_units=4),\\\\n-            get_block(in_channel=128, depth=256, num_units=14),\\\\n-            get_block(in_channel=256, depth=512, num_units=3)\\\\n-        ]\\\\n-    elif num_layers == 100:\\\\n-        blocks = [\\\\n-            get_block(in_channel=64, depth=64, num_units=3),\\\\n-            get_block(in_channel=64, depth=128, num_units=13),\\\\n-            get_block(in_channel=128, depth=256, num_units=30),\\\\n-            get_block(in_channel=256, depth=512, num_units=3)\\\\n-        ]\\\\n-    elif num_layers == 152:\\\\n-        blocks = [\\\\n-            get_block(in_channel=64, depth=64, num_units=3),\\\\n-            get_block(in_channel=64, depth=128, num_units=8),\\\\n-            get_block(in_channel=128, depth=256, num_units=36),\\\\n-            get_block(in_channel=256, depth=512, num_units=3)\\\\n-        ]\\\\n-    return blocks\\\\n-\\\\n-class Backbone(Module):\\\\n-    def __init__(self, num_layers, drop_ratio, mode=\\\\\\\'ir\\\\\\\'):\\\\n-        super(Backbone, self).__init__()\\\\n-        assert num_layers in [50, 100, 152], \\\\\\\'num_layers should be 50,100, or 152\\\\\\\'\\\\n-        assert mode in [\\\\\\\'ir\\\\\\\', \\\\\\\'ir_se\\\\\\\'], \\\\\\\'mode should be ir or ir_se\\\\\\\'\\\\n-        blocks = get_blocks(num_layers)\\\\n-        if mode == \\\\\\\'ir\\\\\\\':\\\\n-            unit_module = bottleneck_IR\\\\n-        elif mode == \\\\\\\'ir_se\\\\\\\':\\\\n-            unit_module = bottleneck_IR_SE\\\\n-        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1 ,bias=False), \\\\n-                                      BatchNorm2d(64), \\\\n-                                      PReLU(64))\\\\n-        self.output_layer = Sequential(BatchNorm2d(512), \\\\n-                                       Dropout(drop_ratio),\\\\n-                                       Flatten(),\\\\n-                                       Linear(512 * 7 * 7, 512),\\\\n-                                       BatchNorm1d(512))\\\\n-        modules = []\\\\n-        for block in blocks:\\\\n-            for bottleneck in block:\\\\n-                modules.append(\\\\n-                    unit_module(bottleneck.in_channel,\\\\n-                                bottleneck.depth,\\\\n-                                bottleneck.stride))\\\\n-        self.body = Sequential(*modules)\\\\n-    \\\\n-    def forward(self,x):\\\\n-        x = self.input_layer(x)\\\\n-        x = self.body(x)\\\\n-        x = self.output_layer(x)\\\\n-        return l2_norm(x)\\\\n-\\\\n-##################################  MobileFaceNet #############################################################\\\\n-    \\\\n-class Conv_block(Module):\\\\n-    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\\\\n-        super(Conv_block, self).__init__()\\\\n-        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\\\\n-        self.bn = BatchNorm2d(out_c)\\\\n-        self.prelu = PReLU(out_c)\\\\n-    def forward(self, x):\\\\n-        x = self.conv(x)\\\\n-        x = self.bn(x)\\\\n-        x = self.prelu(x)\\\\n-        return x\\\\n-\\\\n-class Linear_block(Module):\\\\n-    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\\\\n-        super(Linear_block, self).__init__()\\\\n-        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\\\\n-        self.bn = BatchNorm2d(out_c)\\\\n-    def forward(self, x):\\\\n-        x = self.conv(x)\\\\n-        x = self.bn(x)\\\\n-        return x\\\\n-\\\\n-class Depth_Wise(Module):\\\\n-     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\\\\n-        super(Depth_Wise, self).__init__()\\\\n-        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\\\\n-        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\\\\n-        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\\\\n-        self.residual = residual\\\\n-     def forward(self, x):\\\\n-        if self.residual:\\\\n-            short_cut = x\\\\n-        x = self.conv(x)\\\\n-        x = self.conv_dw(x)\\\\n-        x = self.project(x)\\\\n-        if self.residual:\\\\n-            output = short_cut + x\\\\n-        else:\\\\n-            output = x\\\\n-        return output\\\\n-\\\\n-class Residual(Module):\\\\n-    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\\\\n-        super(Residual, self).__init__()\\\\n-        modules = []\\\\n-        for _ in range(num_block):\\\\n-            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\\\\n-        self.model = Sequential(*modules)\\\\n-    def forward(self, x):\\\\n-        return self.model(x)\\\\n-\\\\n-class MobileFaceNet(Module):\\\\n-    def __init__(self, embedding_size):\\\\n-        super(MobileFaceNet, self).__init__()\\\\n-        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\\\\n-        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\\\\n-        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\\\\n-        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\\\\n-        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\\\\n-        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\\\\n-        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\\\\n-        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\\\\n-        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\\\\n-        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\\\\n-        self.conv_6_flatten = Flatten()\\\\n-        self.linear = Linear(512, embedding_size, bias=False)\\\\n-        self.bn = BatchNorm1d(embedding_size)\\\\n-    \\\\n-    def forward(self, x):\\\\n-        out = self.conv1(x)\\\\n-\\\\n-        out = self.conv2_dw(out)\\\\n-\\\\n-        out = self.conv_23(out)\\\\n-\\\\n-        out = self.conv_3(out)\\\\n-        \\\\n-        out = self.conv_34(out)\\\\n-\\\\n-        out = self.conv_4(out)\\\\n-\\\\n-        out = self.conv_45(out)\\\\n-\\\\n-        out = self.conv_5(out)\\\\n-\\\\n-        out = self.conv_6_sep(out)\\\\n-\\\\n-        out = self.conv_6_dw(out)\\\\n-\\\\n-        out = self.conv_6_flatten(out)\\\\n-\\\\n-        out = self.linear(out)\\\\n-\\\\n-        out = self.bn(out)\\\\n-        return l2_norm(out)\\\\n-\\\\n-##################################  Arcface head #############################################################\\\\n-\\\\n-class Arcface(Module):\\\\n-    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \\\\n-    def __init__(self, embedding_size=512, classnum=51332,  s=64., m=0.5):\\\\n-        super(Arcface, self).__init__()\\\\n-        self.classnum = classnum\\\\n-        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\\\\n-        # initial kernel\\\\n-        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\\\\n-        self.m = m # the margin value, default is 0.5\\\\n-        self.s = s # scalar value default is 64, see normface https://arxiv.org/abs/1704.06369\\\\n-        self.cos_m = math.cos(m)\\\\n-        self.sin_m = math.sin(m)\\\\n-        self.mm = self.sin_m * m  # issue 1\\\\n-        self.threshold = math.cos(math.pi - m)\\\\n-    def forward(self, embbedings, label):\\\\n-        # weights norm\\\\n-        nB = len(embbedings)\\\\n-        kernel_norm = l2_norm(self.kernel,axis=0)\\\\n-        # cos(theta+m)\\\\n-        cos_theta = torch.mm(embbedings,kernel_norm)\\\\n-#         output = torch.mm(embbedings,kernel_norm)\\\\n-        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\\\\n-        cos_theta_2 = torch.pow(cos_theta, 2)\\\\n-        sin_theta_2 = 1 - cos_theta_2\\\\n-        sin_theta = torch.sqrt(sin_theta_2)\\\\n-        cos_theta_m = (cos_theta * self.cos_m - sin_theta * self.sin_m)\\\\n-        # this condition controls the theta+m should in range [0, pi]\\\\n-        #      0<=theta+m<=pi\\\\n-        #     -m<=theta<=pi-m\\\\n-        cond_v = cos_theta - self.threshold\\\\n-        cond_mask = cond_v <= 0\\\\n-        keep_val = (cos_theta - self.mm) # when theta not in [0,pi], use cosface instead\\\\n-        cos_theta_m[cond_mask] = keep_val[cond_mask]\\\\n-        output = cos_theta * 1.0 # a little bit hacky way to prevent in_place operation on cos_theta\\\\n-        idx_ = torch.arange(0, nB, dtype=torch.long)\\\\n-        output[idx_, label] = cos_theta_m[idx_, label]\\\\n-        output *= self.s # scale up in order to make softmax work, first introduced in normface\\\\n-        return output\\\\n-\\\\n-##################################  Cosface head #############################################################    \\\\n-    \\\\n-class Am_softmax(Module):\\\\n-    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \\\\n-    def __init__(self,embedding_size=512,classnum=51332):\\\\n-        super(Am_softmax, self).__init__()\\\\n-        self.classnum = classnum\\\\n-        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\\\\n-        # initial kernel\\\\n-        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\\\\n-        self.m = 0.35 # additive margin recommended by the paper\\\\n-        self.s = 30. # see normface https://arxiv.org/abs/1704.06369\\\\n-    def forward(self,embbedings,label):\\\\n-        kernel_norm = l2_norm(self.kernel,axis=0)\\\\n-        cos_theta = torch.mm(embbedings,kernel_norm)\\\\n-        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\\\\n-        phi = cos_theta - self.m\\\\n-        label = label.view(-1,1) #size=(B,1)\\\\n-        index = cos_theta.data * 0.0 #size=(B,Classnum)\\\\n-        index.scatter_(1,label.data.view(-1,1),1)\\\\n-        index = index.byte()\\\\n-        output = cos_theta * 1.0\\\\n-        output[index] = phi[index] #only change the correct predicted output\\\\n-        output *= self.s # scale up in order to make softmax work, first introduced in normface\\\\n-        return output\\\\n-\\\\ndiff --git a/mtcnn.py b/mtcnn.py\\\\ndeleted file mode 100644\\\\nindex 90a72c2..0000000\\\\n--- a/mtcnn.py\\\\n+++ /dev/null\\\\n@@ -1,151 +0,0 @@\\\\n-import numpy as np\\\\n-import torch\\\\n-from PIL import Image\\\\n-from torch.autograd import Variable\\\\n-from mtcnn_pytorch.src.get_nets import PNet, RNet, ONet\\\\n-from mtcnn_pytorch.src.box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\\\\n-from mtcnn_pytorch.src.first_stage import run_first_stage\\\\n-from mtcnn_pytorch.src.align_trans import get_reference_facial_points, warp_and_crop_face\\\\n-device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\\\\n-# device = \\\\\\\'cpu\\\\\\\'\\\\n-\\\\n-class MTCNN():\\\\n-    def __init__(self):\\\\n-        self.pnet = PNet().to(device)\\\\n-        self.rnet = RNet().to(device)\\\\n-        self.onet = ONet().to(device)\\\\n-        self.pnet.eval()\\\\n-        self.rnet.eval()\\\\n-        self.onet.eval()\\\\n-        self.refrence = get_reference_facial_points(default_square= True)\\\\n-        \\\\n-    def align(self, img):\\\\n-        _, landmarks = self.detect_faces(img)\\\\n-        facial5points = [[landmarks[0][j],landmarks[0][j+5]] for j in range(5)]\\\\n-        warped_face = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112,112))\\\\n-        return Image.fromarray(warped_face)\\\\n-    \\\\n-    def align_multi(self, img, limit=None, min_face_size=30.0):\\\\n-        boxes, landmarks = self.detect_faces(img, min_face_size)\\\\n-        if limit:\\\\n-            boxes = boxes[:limit]\\\\n-            landmarks = landmarks[:limit]\\\\n-        faces = []\\\\n-        for landmark in landmarks:\\\\n-            facial5points = [[landmark[j],landmark[j+5]] for j in range(5)]\\\\n-            warped_face = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112,112))\\\\n-            faces.append(Image.fromarray(warped_face))\\\\n-        return boxes, faces\\\\n-\\\\n-    def detect_faces(self, image, min_face_size=20.0,\\\\n-                    thresholds=[0.9, 0.8, 0.8],\\\\n-                    nms_thresholds=[0.9, 0.9, 0.8]):\\\\n-        """\\\\n-        Arguments:\\\\n-            image: an instance of PIL.Image.\\\\n-            min_face_size: a float number.\\\\n-            thresholds: a list of length 3.\\\\n-            nms_thresholds: a list of length 3.\\\\n-\\\\n-        Returns:\\\\n-            two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\\\\n-            bounding boxes and facial landmarks.\\\\n-        """\\\\n-\\\\n-        # BUILD AN IMAGE PYRAMID\\\\n-        width, height = image.size\\\\n-        min_length = min(height, width)\\\\n-\\\\n-        min_detection_size = 12\\\\n-        factor = 0.707  # sqrt(0.5)\\\\n-\\\\n-        # scales for scaling the image\\\\n-        scales = []\\\\n-\\\\n-        # scales the image so that\\\\n-        # minimum size that we can detect equals to\\\\n-        # minimum face size that we want to detect\\\\n-        m = min_detection_size/min_face_size\\\\n-        min_length *= m\\\\n-\\\\n-        factor_count = 0\\\\n-        while min_length > min_detection_size:\\\\n-            scales.append(m*factor**factor_count)\\\\n-            min_length *= factor\\\\n-            factor_count += 1\\\\n-\\\\n-        # STAGE 1\\\\n-\\\\n-        # it will be returned\\\\n-        bounding_boxes = []\\\\n-\\\\n-        with torch.no_grad():\\\\n-            # run P-Net on different scales\\\\n-            for s in scales:\\\\n-                boxes = run_first_stage(image, self.pnet, scale=s, threshold=thresholds[0])\\\\n-                bounding_boxes.append(boxes)\\\\n-\\\\n-            # collect boxes (and offsets, and scores) from different scales\\\\n-            bounding_boxes = [i for i in bounding_boxes if i is not None]\\\\n-            bounding_boxes = np.vstack(bounding_boxes)\\\\n-\\\\n-            keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\\\\n-            bounding_boxes = bounding_boxes[keep]\\\\n-\\\\n-            # use offsets predicted by pnet to transform bounding boxes\\\\n-            bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\\\\n-            # shape [n_boxes, 5]\\\\n-\\\\n-            bounding_boxes = convert_to_square(bounding_boxes)\\\\n-            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\\\n-\\\\n-            # STAGE 2\\\\n-\\\\n-            img_boxes = get_image_boxes(bounding_boxes, image, size=24)\\\\n-            img_boxes = torch.FloatTensor(img_boxes).to(device)\\\\n-\\\\n-            output = self.rnet(img_boxes)\\\\n-            offsets = output[0].cpu().data.numpy()  # shape [n_boxes, 4]\\\\n-            probs = output[1].cpu().data.numpy()  # shape [n_boxes, 2]\\\\n-\\\\n-            keep = np.where(probs[:, 1] > thresholds[1])[0]\\\\n-            bounding_boxes = bounding_boxes[keep]\\\\n-            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\\\n-            offsets = offsets[keep]\\\\n-\\\\n-            keep = nms(bounding_boxes, nms_thresholds[1])\\\\n-            bounding_boxes = bounding_boxes[keep]\\\\n-            bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\\\\n-            bounding_boxes = convert_to_square(bounding_boxes)\\\\n-            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\\\n-\\\\n-            # STAGE 3\\\\n-\\\\n-            img_boxes = get_image_boxes(bounding_boxes, image, size=48)\\\\n-            if len(img_boxes) == 0: \\\\n-                return [], []\\\\n-            img_boxes = torch.FloatTensor(img_boxes).to(device)\\\\n-            output = self.onet(img_boxes)\\\\n-            landmarks = output[0].cpu().data.numpy()  # shape [n_boxes, 10]\\\\n-            offsets = output[1].cpu().data.numpy()  # shape [n_boxes, 4]\\\\n-            probs = output[2].cpu().data.numpy()  # shape [n_boxes, 2]\\\\n-\\\\n-            keep = np.where(probs[:, 1] > thresholds[2])[0]\\\\n-            bounding_boxes = bounding_boxes[keep]\\\\n-            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\\\n-            offsets = offsets[keep]\\\\n-            landmarks = landmarks[keep]\\\\n-\\\\n-            # compute landmark points\\\\n-            width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\\\\n-            height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\\\\n-            xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\\\\n-            landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\\\\n-            landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\\\\n-\\\\n-            bounding_boxes = calibrate_box(bounding_boxes, offsets)\\\\n-            keep = nms(bounding_boxes, nms_thresholds[2], mode=\\\\\\\'min\\\\\\\')\\\\n-            bounding_boxes = bounding_boxes[keep]\\\\n-            landmarks = landmarks[keep]\\\\n-\\\\n-        return bounding_boxes, landmarks\\\\ndiff --git a/mtcnn_pytorch/src/__init__.py b/mtcnn_pytorch/src/__init__.py\\\\ndeleted file mode 100644\\\\nindex 617ba38..0000000\\\\n--- a/mtcnn_pytorch/src/__init__.py\\\\n+++ /dev/null\\\\n@@ -1,2 +0,0 @@\\\\n-from .visualization_utils import show_bboxes\\\\n-from .detector import detect_faces\\\\ndiff --git a/mtcnn_pytorch/src/align_trans.py b/mtcnn_pytorch/src/align_trans.py\\\\ndeleted file mode 100644\\\\nindex 72d4351..0000000\\\\n--- a/mtcnn_pytorch/src/align_trans.py\\\\n+++ /dev/null\\\\n@@ -1,304 +0,0 @@\\\\n-# -*- coding: utf-8 -*-\\\\n-"""\\\\n-Created on Mon Apr 24 15:43:29 2017\\\\n-@author: zhaoy\\\\n-"""\\\\n-import numpy as np\\\\n-import cv2\\\\n-\\\\n-# from scipy.linalg import lstsq\\\\n-# from scipy.ndimage import geometric_transform  # , map_coordinates\\\\n-\\\\n-from mtcnn_pytorch.src.matlab_cp2tform import get_similarity_transform_for_cv2\\\\n-\\\\n-# reference facial points, a list of coordinates (x,y)\\\\n-REFERENCE_FACIAL_POINTS = [\\\\n-    [30.29459953,  51.69630051],\\\\n-    [65.53179932,  51.50139999],\\\\n-    [48.02519989,  71.73660278],\\\\n-    [33.54930115,  92.3655014],\\\\n-    [62.72990036,  92.20410156]\\\\n-]\\\\n-\\\\n-DEFAULT_CROP_SIZE = (96, 112)\\\\n-\\\\n-\\\\n-class FaceWarpException(Exception):\\\\n-    def __str__(self):\\\\n-        return \\\\\\\'In File {}:{}\\\\\\\'.format(\\\\n-            __file__, super.__str__(self))\\\\n-\\\\n-\\\\n-def get_reference_facial_points(output_size=None,\\\\n-                                inner_padding_factor=0.0,\\\\n-                                outer_padding=(0, 0),\\\\n-                                default_square=False):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        get reference 5 key points according to crop settings:\\\\n-        0. Set default crop_size:\\\\n-            if default_square: \\\\n-                crop_size = (112, 112)\\\\n-            else: \\\\n-                crop_size = (96, 112)\\\\n-        1. Pad the crop_size by inner_padding_factor in each side;\\\\n-        2. Resize crop_size into (output_size - outer_padding*2),\\\\n-            pad into output_size with outer_padding;\\\\n-        3. Output reference_5point;\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @output_size: (w, h) or None\\\\n-            size of aligned face image\\\\n-        @inner_padding_factor: (w_factor, h_factor)\\\\n-            padding factor for inner (w, h)\\\\n-        @outer_padding: (w_pad, h_pad)\\\\n-            each row is a pair of coordinates (x, y)\\\\n-        @default_square: True or False\\\\n-            if True:\\\\n-                default crop_size = (112, 112)\\\\n-            else:\\\\n-                default crop_size = (96, 112);\\\\n-        !!! make sure, if output_size is not None:\\\\n-                (output_size - outer_padding) \\\\n-                = some_scale * (default crop_size * (1.0 + inner_padding_factor))\\\\n-    Returns:\\\\n-    ----------\\\\n-        @reference_5point: 5x2 np.array\\\\n-            each row is a pair of transformed coordinates (x, y)\\\\n-    """\\\\n-    #print(\\\\\\\'\\\\\\\\n===> get_reference_facial_points():\\\\\\\')\\\\n-\\\\n-    #print(\\\\\\\'---> Params:\\\\\\\')\\\\n-    #print(\\\\\\\'            output_size: \\\\\\\', output_size)\\\\n-    #print(\\\\\\\'            inner_padding_factor: \\\\\\\', inner_padding_factor)\\\\n-    #print(\\\\\\\'            outer_padding:\\\\\\\', outer_padding)\\\\n-    #print(\\\\\\\'            default_square: \\\\\\\', default_square)\\\\n-\\\\n-    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\\\\n-    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\\\\n-\\\\n-    # 0) make the inner region a square\\\\n-    if default_square:\\\\n-        size_diff = max(tmp_crop_size) - tmp_crop_size\\\\n-        tmp_5pts += size_diff / 2\\\\n-        tmp_crop_size += size_diff\\\\n-\\\\n-    #print(\\\\\\\'---> default:\\\\\\\')\\\\n-    #print(\\\\\\\'              crop_size = \\\\\\\', tmp_crop_size)\\\\n-    #print(\\\\\\\'              reference_5pts = \\\\\\\', tmp_5pts)\\\\n-\\\\n-    if (output_size and\\\\n-            output_size[0] == tmp_crop_size[0] and\\\\n-            output_size[1] == tmp_crop_size[1]):\\\\n-        #print(\\\\\\\'output_size == DEFAULT_CROP_SIZE {}: return default reference points\\\\\\\'.format(tmp_crop_size))\\\\n-        return tmp_5pts\\\\n-\\\\n-    if (inner_padding_factor == 0 and\\\\n-            outer_padding == (0, 0)):\\\\n-        if output_size is None:\\\\n-            #print(\\\\\\\'No paddings to do: return default reference points\\\\\\\')\\\\n-            return tmp_5pts\\\\n-        else:\\\\n-            raise FaceWarpException(\\\\n-                \\\\\\\'No paddings to do, output_size must be None or {}\\\\\\\'.format(tmp_crop_size))\\\\n-\\\\n-    # check output size\\\\n-    if not (0 <= inner_padding_factor <= 1.0):\\\\n-        raise FaceWarpException(\\\\\\\'Not (0 <= inner_padding_factor <= 1.0)\\\\\\\')\\\\n-\\\\n-    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\\\\n-            and output_size is None):\\\\n-        output_size = tmp_crop_size * \\\\\\\\\\\\n-            (1 + inner_padding_factor * 2).astype(np.int32)\\\\n-        output_size += np.array(outer_padding)\\\\n-        #print(\\\\\\\'              deduced from paddings, output_size = \\\\\\\', output_size)\\\\n-\\\\n-    if not (outer_padding[0] < output_size[0]\\\\n-            and outer_padding[1] < output_size[1]):\\\\n-        raise FaceWarpException(\\\\\\\'Not (outer_padding[0] < output_size[0]\\\\\\\'\\\\n-                                \\\\\\\'and outer_padding[1] < output_size[1])\\\\\\\')\\\\n-\\\\n-    # 1) pad the inner region according inner_padding_factor\\\\n-    #print(\\\\\\\'---> STEP1: pad the inner region according inner_padding_factor\\\\\\\')\\\\n-    if inner_padding_factor > 0:\\\\n-        size_diff = tmp_crop_size * inner_padding_factor * 2\\\\n-        tmp_5pts += size_diff / 2\\\\n-        tmp_crop_size += np.round(size_diff).astype(np.int32)\\\\n-\\\\n-    #print(\\\\\\\'              crop_size = \\\\\\\', tmp_crop_size)\\\\n-    #print(\\\\\\\'              reference_5pts = \\\\\\\', tmp_5pts)\\\\n-\\\\n-    # 2) resize the padded inner region\\\\n-    #print(\\\\\\\'---> STEP2: resize the padded inner region\\\\\\\')\\\\n-    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\\\\n-    #print(\\\\\\\'              crop_size = \\\\\\\', tmp_crop_size)\\\\n-    #print(\\\\\\\'              size_bf_outer_pad = \\\\\\\', size_bf_outer_pad)\\\\n-\\\\n-    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\\\\n-        raise FaceWarpException(\\\\\\\'Must have (output_size - outer_padding)\\\\\\\'\\\\n-                                \\\\\\\'= some_scale * (crop_size * (1.0 + inner_padding_factor)\\\\\\\')\\\\n-\\\\n-    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\\\\n-    #print(\\\\\\\'              resize scale_factor = \\\\\\\', scale_factor)\\\\n-    tmp_5pts = tmp_5pts * scale_factor\\\\n-#    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\\\\n-#    tmp_5pts = tmp_5pts + size_diff / 2\\\\n-    tmp_crop_size = size_bf_outer_pad\\\\n-    #print(\\\\\\\'              crop_size = \\\\\\\', tmp_crop_size)\\\\n-    #print(\\\\\\\'              reference_5pts = \\\\\\\', tmp_5pts)\\\\n-\\\\n-    # 3) add outer_padding to make output_size\\\\n-    reference_5point = tmp_5pts + np.array(outer_padding)\\\\n-    tmp_crop_size = output_size\\\\n-    #print(\\\\\\\'---> STEP3: add outer_padding to make output_size\\\\\\\')\\\\n-    #print(\\\\\\\'              crop_size = \\\\\\\', tmp_crop_size)\\\\n-    #print(\\\\\\\'              reference_5pts = \\\\\\\', tmp_5pts)\\\\n-\\\\n-    #print(\\\\\\\'===> end get_reference_facial_points\\\\\\\\n\\\\\\\')\\\\n-\\\\n-    return reference_5point\\\\n-\\\\n-\\\\n-def get_affine_transform_matrix(src_pts, dst_pts):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        get affine transform matrix \\\\\\\'tfm\\\\\\\' from src_pts to dst_pts\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @src_pts: Kx2 np.array\\\\n-            source points matrix, each row is a pair of coordinates (x, y)\\\\n-        @dst_pts: Kx2 np.array\\\\n-            destination points matrix, each row is a pair of coordinates (x, y)\\\\n-    Returns:\\\\n-    ----------\\\\n-        @tfm: 2x3 np.array\\\\n-            transform matrix from src_pts to dst_pts\\\\n-    """\\\\n-\\\\n-    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\\\\n-    n_pts = src_pts.shape[0]\\\\n-    ones = np.ones((n_pts, 1), src_pts.dtype)\\\\n-    src_pts_ = np.hstack([src_pts, ones])\\\\n-    dst_pts_ = np.hstack([dst_pts, ones])\\\\n-\\\\n-#    #print((\\\\\\\'src_pts_:\\\\\\\\n\\\\\\\' + str(src_pts_))\\\\n-#    #print((\\\\\\\'dst_pts_:\\\\\\\\n\\\\\\\' + str(dst_pts_))\\\\n-\\\\n-    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\\\\n-\\\\n-#    #print((\\\\\\\'np.linalg.lstsq return A: \\\\\\\\n\\\\\\\' + str(A))\\\\n-#    #print((\\\\\\\'np.linalg.lstsq return res: \\\\\\\\n\\\\\\\' + str(res))\\\\n-#    #print((\\\\\\\'np.linalg.lstsq return rank: \\\\\\\\n\\\\\\\' + str(rank))\\\\n-#    #print((\\\\\\\'np.linalg.lstsq return s: \\\\\\\\n\\\\\\\' + str(s))\\\\n-\\\\n-    if rank == 3:\\\\n-        tfm = np.float32([\\\\n-            [A[0, 0], A[1, 0], A[2, 0]],\\\\n-            [A[0, 1], A[1, 1], A[2, 1]]\\\\n-        ])\\\\n-    elif rank == 2:\\\\n-        tfm = np.float32([\\\\n-            [A[0, 0], A[1, 0], 0],\\\\n-            [A[0, 1], A[1, 1], 0]\\\\n-        ])\\\\n-\\\\n-    return tfm\\\\n-\\\\n-\\\\n-def warp_and_crop_face(src_img,\\\\n-                       facial_pts,\\\\n-                       reference_pts=None,\\\\n-                       crop_size=(96, 112),\\\\n-                       align_type=\\\\\\\'smilarity\\\\\\\'):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        apply affine transform \\\\\\\'trans\\\\\\\' to uv\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @src_img: 3x3 np.array\\\\n-            input image\\\\n-        @facial_pts: could be\\\\n-            1)a list of K coordinates (x,y)\\\\n-        or\\\\n-            2) Kx2 or 2xK np.array\\\\n-            each row or col is a pair of coordinates (x, y)\\\\n-        @reference_pts: could be\\\\n-            1) a list of K coordinates (x,y)\\\\n-        or\\\\n-            2) Kx2 or 2xK np.array\\\\n-            each row or col is a pair of coordinates (x, y)\\\\n-        or\\\\n-            3) None\\\\n-            if None, use default reference facial points\\\\n-        @crop_size: (w, h)\\\\n-            output face image size\\\\n-        @align_type: transform type, could be one of\\\\n-            1) \\\\\\\'similarity\\\\\\\': use similarity transform\\\\n-            2) \\\\\\\'cv2_affine\\\\\\\': use the first 3 points to do affine transform,\\\\n-                    by calling cv2.getAffineTransform()\\\\n-            3) \\\\\\\'affine\\\\\\\': use all points to do affine transform\\\\n-    Returns:\\\\n-    ----------\\\\n-        @face_img: output face image with size (w, h) = @crop_size\\\\n-    """\\\\n-\\\\n-    if reference_pts is None:\\\\n-        if crop_size[0] == 96 and crop_size[1] == 112:\\\\n-            reference_pts = REFERENCE_FACIAL_POINTS\\\\n-        else:\\\\n-            default_square = False\\\\n-            inner_padding_factor = 0\\\\n-            outer_padding = (0, 0)\\\\n-            output_size = crop_size\\\\n-\\\\n-            reference_pts = get_reference_facial_points(output_size,\\\\n-                                                        inner_padding_factor,\\\\n-                                                        outer_padding,\\\\n-                                                        default_square)\\\\n-\\\\n-    ref_pts = np.float32(reference_pts)\\\\n-    ref_pts_shp = ref_pts.shape\\\\n-    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\\\\n-        raise FaceWarpException(\\\\n-            \\\\\\\'reference_pts.shape must be (K,2) or (2,K) and K>2\\\\\\\')\\\\n-\\\\n-    if ref_pts_shp[0] == 2:\\\\n-        ref_pts = ref_pts.T\\\\n-\\\\n-    src_pts = np.float32(facial_pts)\\\\n-    src_pts_shp = src_pts.shape\\\\n-    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\\\\n-        raise FaceWarpException(\\\\n-            \\\\\\\'facial_pts.shape must be (K,2) or (2,K) and K>2\\\\\\\')\\\\n-\\\\n-    if src_pts_shp[0] == 2:\\\\n-        src_pts = src_pts.T\\\\n-\\\\n-#    #print(\\\\\\\'--->src_pts:\\\\\\\\n\\\\\\\', src_pts\\\\n-#    #print(\\\\\\\'--->ref_pts\\\\\\\\n\\\\\\\', ref_pts\\\\n-\\\\n-    if src_pts.shape != ref_pts.shape:\\\\n-        raise FaceWarpException(\\\\n-            \\\\\\\'facial_pts and reference_pts must have the same shape\\\\\\\')\\\\n-\\\\n-    if align_type is \\\\\\\'cv2_affine\\\\\\\':\\\\n-        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\\\\n-#        #print((\\\\\\\'cv2.getAffineTransform() returns tfm=\\\\\\\\n\\\\\\\' + str(tfm))\\\\n-    elif align_type is \\\\\\\'affine\\\\\\\':\\\\n-        tfm = get_affine_transform_matrix(src_pts, ref_pts)\\\\n-#        #print((\\\\\\\'get_affine_transform_matrix() returns tfm=\\\\\\\\n\\\\\\\' + str(tfm))\\\\n-    else:\\\\n-        tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\\\\n-#        #print((\\\\\\\'get_similarity_transform_for_cv2() returns tfm=\\\\\\\\n\\\\\\\' + str(tfm))\\\\n-\\\\n-#    #print(\\\\\\\'--->Transform matrix: \\\\\\\'\\\\n-#    #print((\\\\\\\'type(tfm):\\\\\\\' + str(type(tfm)))\\\\n-#    #print((\\\\\\\'tfm.dtype:\\\\\\\' + str(tfm.dtype))\\\\n-#    #print( tfm\\\\n-\\\\n-    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\\\\n-\\\\n-    return face_img\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/mtcnn_pytorch/src/box_utils.py b/mtcnn_pytorch/src/box_utils.py\\\\ndeleted file mode 100644\\\\nindex 3557387..0000000\\\\n--- a/mtcnn_pytorch/src/box_utils.py\\\\n+++ /dev/null\\\\n@@ -1,238 +0,0 @@\\\\n-import numpy as np\\\\n-from PIL import Image\\\\n-\\\\n-\\\\n-def nms(boxes, overlap_threshold=0.5, mode=\\\\\\\'union\\\\\\\'):\\\\n-    """Non-maximum suppression.\\\\n-\\\\n-    Arguments:\\\\n-        boxes: a float numpy array of shape [n, 5],\\\\n-            where each row is (xmin, ymin, xmax, ymax, score).\\\\n-        overlap_threshold: a float number.\\\\n-        mode: \\\\\\\'union\\\\\\\' or \\\\\\\'min\\\\\\\'.\\\\n-\\\\n-    Returns:\\\\n-        list with indices of the selected boxes\\\\n-    """\\\\n-\\\\n-    # if there are no boxes, return the empty list\\\\n-    if len(boxes) == 0:\\\\n-        return []\\\\n-\\\\n-    # list of picked indices\\\\n-    pick = []\\\\n-\\\\n-    # grab the coordinates of the bounding boxes\\\\n-    x1, y1, x2, y2, score = [boxes[:, i] for i in range(5)]\\\\n-\\\\n-    area = (x2 - x1 + 1.0)*(y2 - y1 + 1.0)\\\\n-    ids = np.argsort(score)  # in increasing order\\\\n-\\\\n-    while len(ids) > 0:\\\\n-\\\\n-        # grab index of the largest value\\\\n-        last = len(ids) - 1\\\\n-        i = ids[last]\\\\n-        pick.append(i)\\\\n-\\\\n-        # compute intersections\\\\n-        # of the box with the largest score\\\\n-        # with the rest of boxes\\\\n-\\\\n-        # left top corner of intersection boxes\\\\n-        ix1 = np.maximum(x1[i], x1[ids[:last]])\\\\n-        iy1 = np.maximum(y1[i], y1[ids[:last]])\\\\n-\\\\n-        # right bottom corner of intersection boxes\\\\n-        ix2 = np.minimum(x2[i], x2[ids[:last]])\\\\n-        iy2 = np.minimum(y2[i], y2[ids[:last]])\\\\n-\\\\n-        # width and height of intersection boxes\\\\n-        w = np.maximum(0.0, ix2 - ix1 + 1.0)\\\\n-        h = np.maximum(0.0, iy2 - iy1 + 1.0)\\\\n-\\\\n-        # intersections\\\\\\\' areas\\\\n-        inter = w * h\\\\n-        if mode == \\\\\\\'min\\\\\\\':\\\\n-            overlap = inter/np.minimum(area[i], area[ids[:last]])\\\\n-        elif mode == \\\\\\\'union\\\\\\\':\\\\n-            # intersection over union (IoU)\\\\n-            overlap = inter/(area[i] + area[ids[:last]] - inter)\\\\n-\\\\n-        # delete all boxes where overlap is too big\\\\n-        ids = np.delete(\\\\n-            ids,\\\\n-            np.concatenate([[last], np.where(overlap > overlap_threshold)[0]])\\\\n-        )\\\\n-\\\\n-    return pick\\\\n-\\\\n-\\\\n-def convert_to_square(bboxes):\\\\n-    """Convert bounding boxes to a square form.\\\\n-\\\\n-    Arguments:\\\\n-        bboxes: a float numpy array of shape [n, 5].\\\\n-\\\\n-    Returns:\\\\n-        a float numpy array of shape [n, 5],\\\\n-            squared bounding boxes.\\\\n-    """\\\\n-\\\\n-    square_bboxes = np.zeros_like(bboxes)\\\\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\\\\n-    h = y2 - y1 + 1.0\\\\n-    w = x2 - x1 + 1.0\\\\n-    max_side = np.maximum(h, w)\\\\n-    square_bboxes[:, 0] = x1 + w*0.5 - max_side*0.5\\\\n-    square_bboxes[:, 1] = y1 + h*0.5 - max_side*0.5\\\\n-    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\\\\n-    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\\\\n-    return square_bboxes\\\\n-\\\\n-\\\\n-def calibrate_box(bboxes, offsets):\\\\n-    """Transform bounding boxes to be more like true bounding boxes.\\\\n-    \\\\\\\'offsets\\\\\\\' is one of the outputs of the nets.\\\\n-\\\\n-    Arguments:\\\\n-        bboxes: a float numpy array of shape [n, 5].\\\\n-        offsets: a float numpy array of shape [n, 4].\\\\n-\\\\n-    Returns:\\\\n-        a float numpy array of shape [n, 5].\\\\n-    """\\\\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\\\\n-    w = x2 - x1 + 1.0\\\\n-    h = y2 - y1 + 1.0\\\\n-    w = np.expand_dims(w, 1)\\\\n-    h = np.expand_dims(h, 1)\\\\n-\\\\n-    # this is what happening here:\\\\n-    # tx1, ty1, tx2, ty2 = [offsets[:, i] for i in range(4)]\\\\n-    # x1_true = x1 + tx1*w\\\\n-    # y1_true = y1 + ty1*h\\\\n-    # x2_true = x2 + tx2*w\\\\n-    # y2_true = y2 + ty2*h\\\\n-    # below is just more compact form of this\\\\n-\\\\n-    # are offsets always such that\\\\n-    # x1 < x2 and y1 < y2 ?\\\\n-\\\\n-    translation = np.hstack([w, h, w, h])*offsets\\\\n-    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\\\\n-    return bboxes\\\\n-\\\\n-\\\\n-def get_image_boxes(bounding_boxes, img, size=24):\\\\n-    """Cut out boxes from the image.\\\\n-\\\\n-    Arguments:\\\\n-        bounding_boxes: a float numpy array of shape [n, 5].\\\\n-        img: an instance of PIL.Image.\\\\n-        size: an integer, size of cutouts.\\\\n-\\\\n-    Returns:\\\\n-        a float numpy array of shape [n, 3, size, size].\\\\n-    """\\\\n-\\\\n-    num_boxes = len(bounding_boxes)\\\\n-    width, height = img.size\\\\n-\\\\n-    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\\\\n-    img_boxes = np.zeros((num_boxes, 3, size, size), \\\\\\\'float32\\\\\\\')\\\\n-\\\\n-    for i in range(num_boxes):\\\\n-        img_box = np.zeros((h[i], w[i], 3), \\\\\\\'uint8\\\\\\\')\\\\n-\\\\n-        img_array = np.asarray(img, \\\\\\\'uint8\\\\\\\')\\\\n-        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] =\\\\\\\\\\\\n-            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\\\\n-\\\\n-        # resize\\\\n-        img_box = Image.fromarray(img_box)\\\\n-        img_box = img_box.resize((size, size), Image.BILINEAR)\\\\n-        img_box = np.asarray(img_box, \\\\\\\'float32\\\\\\\')\\\\n-\\\\n-        img_boxes[i, :, :, :] = _preprocess(img_box)\\\\n-\\\\n-    return img_boxes\\\\n-\\\\n-\\\\n-def correct_bboxes(bboxes, width, height):\\\\n-    """Crop boxes that are too big and get coordinates\\\\n-    with respect to cutouts.\\\\n-\\\\n-    Arguments:\\\\n-        bboxes: a float numpy array of shape [n, 5],\\\\n-            where each row is (xmin, ymin, xmax, ymax, score).\\\\n-        width: a float number.\\\\n-        height: a float number.\\\\n-\\\\n-    Returns:\\\\n-        dy, dx, edy, edx: a int numpy arrays of shape [n],\\\\n-            coordinates of the boxes with respect to the cutouts.\\\\n-        y, x, ey, ex: a int numpy arrays of shape [n],\\\\n-            corrected ymin, xmin, ymax, xmax.\\\\n-        h, w: a int numpy arrays of shape [n],\\\\n-            just heights and widths of boxes.\\\\n-\\\\n-        in the following order:\\\\n-            [dy, edy, dx, edx, y, ey, x, ex, w, h].\\\\n-    """\\\\n-\\\\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\\\\n-    w, h = x2 - x1 + 1.0,  y2 - y1 + 1.0\\\\n-    num_boxes = bboxes.shape[0]\\\\n-\\\\n-    # \\\\\\\'e\\\\\\\' stands for end\\\\n-    # (x, y) -> (ex, ey)\\\\n-    x, y, ex, ey = x1, y1, x2, y2\\\\n-\\\\n-    # we need to cut out a box from the image.\\\\n-    # (x, y, ex, ey) are corrected coordinates of the box\\\\n-    # in the image.\\\\n-    # (dx, dy, edx, edy) are coordinates of the box in the cutout\\\\n-    # from the image.\\\\n-    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\\\\n-    edx, edy = w.copy() - 1.0, h.copy() - 1.0\\\\n-\\\\n-    # if box\\\\\\\'s bottom right corner is too far right\\\\n-    ind = np.where(ex > width - 1.0)[0]\\\\n-    edx[ind] = w[ind] + width - 2.0 - ex[ind]\\\\n-    ex[ind] = width - 1.0\\\\n-\\\\n-    # if box\\\\\\\'s bottom right corner is too low\\\\n-    ind = np.where(ey > height - 1.0)[0]\\\\n-    edy[ind] = h[ind] + height - 2.0 - ey[ind]\\\\n-    ey[ind] = height - 1.0\\\\n-\\\\n-    # if box\\\\\\\'s top left corner is too far left\\\\n-    ind = np.where(x < 0.0)[0]\\\\n-    dx[ind] = 0.0 - x[ind]\\\\n-    x[ind] = 0.0\\\\n-\\\\n-    # if box\\\\\\\'s top left corner is too high\\\\n-    ind = np.where(y < 0.0)[0]\\\\n-    dy[ind] = 0.0 - y[ind]\\\\n-    y[ind] = 0.0\\\\n-\\\\n-    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\\\\n-    return_list = [i.astype(\\\\\\\'int32\\\\\\\') for i in return_list]\\\\n-\\\\n-    return return_list\\\\n-\\\\n-\\\\n-def _preprocess(img):\\\\n-    """Preprocessing step before feeding the network.\\\\n-\\\\n-    Arguments:\\\\n-        img: a float numpy array of shape [h, w, c].\\\\n-\\\\n-    Returns:\\\\n-        a float numpy array of shape [1, c, h, w].\\\\n-    """\\\\n-    img = img.transpose((2, 0, 1))\\\\n-    img = np.expand_dims(img, 0)\\\\n-    img = (img - 127.5)*0.0078125\\\\n-    return img\\\\ndiff --git a/mtcnn_pytorch/src/detector.py b/mtcnn_pytorch/src/detector.py\\\\ndeleted file mode 100644\\\\nindex f66eaa5..0000000\\\\n--- a/mtcnn_pytorch/src/detector.py\\\\n+++ /dev/null\\\\n@@ -1,126 +0,0 @@\\\\n-import numpy as np\\\\n-import torch\\\\n-from torch.autograd import Variable\\\\n-from .get_nets import PNet, RNet, ONet\\\\n-from .box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\\\\n-from .first_stage import run_first_stage\\\\n-\\\\n-\\\\n-def detect_faces(image, min_face_size=20.0,\\\\n-                 thresholds=[0.6, 0.7, 0.8],\\\\n-                 nms_thresholds=[0.7, 0.7, 0.7]):\\\\n-    """\\\\n-    Arguments:\\\\n-        image: an instance of PIL.Image.\\\\n-        min_face_size: a float number.\\\\n-        thresholds: a list of length 3.\\\\n-        nms_thresholds: a list of length 3.\\\\n-\\\\n-    Returns:\\\\n-        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\\\\n-        bounding boxes and facial landmarks.\\\\n-    """\\\\n-\\\\n-    # LOAD MODELS\\\\n-    pnet = PNet()\\\\n-    rnet = RNet()\\\\n-    onet = ONet()\\\\n-    onet.eval()\\\\n-\\\\n-    # BUILD AN IMAGE PYRAMID\\\\n-    width, height = image.size\\\\n-    min_length = min(height, width)\\\\n-\\\\n-    min_detection_size = 12\\\\n-    factor = 0.707  # sqrt(0.5)\\\\n-\\\\n-    # scales for scaling the image\\\\n-    scales = []\\\\n-\\\\n-    # scales the image so that\\\\n-    # minimum size that we can detect equals to\\\\n-    # minimum face size that we want to detect\\\\n-    m = min_detection_size/min_face_size\\\\n-    min_length *= m\\\\n-\\\\n-    factor_count = 0\\\\n-    while min_length > min_detection_size:\\\\n-        scales.append(m*factor**factor_count)\\\\n-        min_length *= factor\\\\n-        factor_count += 1\\\\n-\\\\n-    # STAGE 1\\\\n-\\\\n-    # it will be returned\\\\n-    bounding_boxes = []\\\\n-    \\\\n-    with torch.no_grad():\\\\n-        # run P-Net on different scales\\\\n-        for s in scales:\\\\n-            boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\\\\n-            bounding_boxes.append(boxes)\\\\n-\\\\n-        # collect boxes (and offsets, and scores) from different scales\\\\n-        bounding_boxes = [i for i in bounding_boxes if i is not None]\\\\n-        bounding_boxes = np.vstack(bounding_boxes)\\\\n-\\\\n-        keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\\\\n-        bounding_boxes = bounding_boxes[keep]\\\\n-\\\\n-        # use offsets predicted by pnet to transform bounding boxes\\\\n-        bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\\\\n-        # shape [n_boxes, 5]\\\\n-\\\\n-        bounding_boxes = convert_to_square(bounding_boxes)\\\\n-        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\\\n-\\\\n-        # STAGE 2\\\\n-\\\\n-        img_boxes = get_image_boxes(bounding_boxes, image, size=24)\\\\n-        img_boxes = torch.FloatTensor(img_boxes)\\\\n-\\\\n-        output = rnet(img_boxes)\\\\n-        offsets = output[0].data.numpy()  # shape [n_boxes, 4]\\\\n-        probs = output[1].data.numpy()  # shape [n_boxes, 2]\\\\n-\\\\n-        keep = np.where(probs[:, 1] > thresholds[1])[0]\\\\n-        bounding_boxes = bounding_boxes[keep]\\\\n-        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\\\n-        offsets = offsets[keep]\\\\n-\\\\n-        keep = nms(bounding_boxes, nms_thresholds[1])\\\\n-        bounding_boxes = bounding_boxes[keep]\\\\n-        bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\\\\n-        bounding_boxes = convert_to_square(bounding_boxes)\\\\n-        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\\\\n-\\\\n-        # STAGE 3\\\\n-\\\\n-        img_boxes = get_image_boxes(bounding_boxes, image, size=48)\\\\n-        if len(img_boxes) == 0: \\\\n-            return [], []\\\\n-        img_boxes = torch.FloatTensor(img_boxes)\\\\n-        output = onet(img_boxes)\\\\n-        landmarks = output[0].data.numpy()  # shape [n_boxes, 10]\\\\n-        offsets = output[1].data.numpy()  # shape [n_boxes, 4]\\\\n-        probs = output[2].data.numpy()  # shape [n_boxes, 2]\\\\n-\\\\n-        keep = np.where(probs[:, 1] > thresholds[2])[0]\\\\n-        bounding_boxes = bounding_boxes[keep]\\\\n-        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\\\\n-        offsets = offsets[keep]\\\\n-        landmarks = landmarks[keep]\\\\n-\\\\n-        # compute landmark points\\\\n-        width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\\\\n-        height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\\\\n-        xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\\\\n-        landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\\\\n-        landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\\\\n-\\\\n-        bounding_boxes = calibrate_box(bounding_boxes, offsets)\\\\n-        keep = nms(bounding_boxes, nms_thresholds[2], mode=\\\\\\\'min\\\\\\\')\\\\n-        bounding_boxes = bounding_boxes[keep]\\\\n-        landmarks = landmarks[keep]\\\\n-\\\\n-    return bounding_boxes, landmarks\\\\ndiff --git a/mtcnn_pytorch/src/first_stage.py b/mtcnn_pytorch/src/first_stage.py\\\\ndeleted file mode 100644\\\\nindex 55ed04a..0000000\\\\n--- a/mtcnn_pytorch/src/first_stage.py\\\\n+++ /dev/null\\\\n@@ -1,99 +0,0 @@\\\\n-import torch\\\\n-from torch.autograd import Variable\\\\n-import math\\\\n-from PIL import Image\\\\n-import numpy as np\\\\n-from .box_utils import nms, _preprocess\\\\n-device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\\\\n-# device = \\\\\\\'cpu\\\\\\\'\\\\n-\\\\n-def run_first_stage(image, net, scale, threshold):\\\\n-    """Run P-Net, generate bounding boxes, and do NMS.\\\\n-\\\\n-    Arguments:\\\\n-        image: an instance of PIL.Image.\\\\n-        net: an instance of pytorch\\\\\\\'s nn.Module, P-Net.\\\\n-        scale: a float number,\\\\n-            scale width and height of the image by this number.\\\\n-        threshold: a float number,\\\\n-            threshold on the probability of a face when generating\\\\n-            bounding boxes from predictions of the net.\\\\n-\\\\n-    Returns:\\\\n-        a float numpy array of shape [n_boxes, 9],\\\\n-            bounding boxes with scores and offsets (4 + 1 + 4).\\\\n-    """\\\\n-\\\\n-    # scale the image and convert it to a float array\\\\n-    width, height = image.size\\\\n-    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\\\\n-    img = image.resize((sw, sh), Image.BILINEAR)\\\\n-    img = np.asarray(img, \\\\\\\'float32\\\\\\\')\\\\n-\\\\n-    img = torch.FloatTensor(_preprocess(img)).to(device)\\\\n-    with torch.no_grad():\\\\n-        output = net(img)\\\\n-        probs = output[1].cpu().data.numpy()[0, 1, :, :]\\\\n-        offsets = output[0].cpu().data.numpy()\\\\n-        # probs: probability of a face at each sliding window\\\\n-        # offsets: transformations to true bounding boxes\\\\n-\\\\n-        boxes = _generate_bboxes(probs, offsets, scale, threshold)\\\\n-        if len(boxes) == 0:\\\\n-            return None\\\\n-\\\\n-        keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\\\\n-    return boxes[keep]\\\\n-\\\\n-\\\\n-def _generate_bboxes(probs, offsets, scale, threshold):\\\\n-    """Generate bounding boxes at places\\\\n-    where there is probably a face.\\\\n-\\\\n-    Arguments:\\\\n-        probs: a float numpy array of shape [n, m].\\\\n-        offsets: a float numpy array of shape [1, 4, n, m].\\\\n-        scale: a float number,\\\\n-            width and height of the image were scaled by this number.\\\\n-        threshold: a float number.\\\\n-\\\\n-    Returns:\\\\n-        a float numpy array of shape [n_boxes, 9]\\\\n-    """\\\\n-\\\\n-    # applying P-Net is equivalent, in some sense, to\\\\n-    # moving 12x12 window with stride 2\\\\n-    stride = 2\\\\n-    cell_size = 12\\\\n-\\\\n-    # indices of boxes where there is probably a face\\\\n-    inds = np.where(probs > threshold)\\\\n-\\\\n-    if inds[0].size == 0:\\\\n-        return np.array([])\\\\n-\\\\n-    # transformations of bounding boxes\\\\n-    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\\\\n-    # they are defined as:\\\\n-    # w = x2 - x1 + 1\\\\n-    # h = y2 - y1 + 1\\\\n-    # x1_true = x1 + tx1*w\\\\n-    # x2_true = x2 + tx2*w\\\\n-    # y1_true = y1 + ty1*h\\\\n-    # y2_true = y2 + ty2*h\\\\n-\\\\n-    offsets = np.array([tx1, ty1, tx2, ty2])\\\\n-    score = probs[inds[0], inds[1]]\\\\n-\\\\n-    # P-Net is applied to scaled images\\\\n-    # so we need to rescale bounding boxes back\\\\n-    bounding_boxes = np.vstack([\\\\n-        np.round((stride*inds[1] + 1.0)/scale),\\\\n-        np.round((stride*inds[0] + 1.0)/scale),\\\\n-        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\\\\n-        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\\\\n-        score, offsets\\\\n-    ])\\\\n-    # why one is added?\\\\n-\\\\n-    return bounding_boxes.T\\\\ndiff --git a/mtcnn_pytorch/src/get_nets.py b/mtcnn_pytorch/src/get_nets.py\\\\ndeleted file mode 100644\\\\nindex 6df979a..0000000\\\\n--- a/mtcnn_pytorch/src/get_nets.py\\\\n+++ /dev/null\\\\n@@ -1,169 +0,0 @@\\\\n-import torch\\\\n-import torch.nn as nn\\\\n-import torch.nn.functional as F\\\\n-from collections import OrderedDict\\\\n-import numpy as np\\\\n-\\\\n-\\\\n-class Flatten(nn.Module):\\\\n-\\\\n-    def __init__(self):\\\\n-        super(Flatten, self).__init__()\\\\n-\\\\n-    def forward(self, x):\\\\n-        """\\\\n-        Arguments:\\\\n-            x: a float tensor with shape [batch_size, c, h, w].\\\\n-        Returns:\\\\n-            a float tensor with shape [batch_size, c*h*w].\\\\n-        """\\\\n-\\\\n-        # without this pretrained model isn\\\\\\\'t working\\\\n-        x = x.transpose(3, 2).contiguous()\\\\n-\\\\n-        return x.view(x.size(0), -1)\\\\n-\\\\n-\\\\n-class PNet(nn.Module):\\\\n-\\\\n-    def __init__(self):\\\\n-\\\\n-        super(PNet, self).__init__()\\\\n-\\\\n-        # suppose we have input with size HxW, then\\\\n-        # after first layer: H - 2,\\\\n-        # after pool: ceil((H - 2)/2),\\\\n-        # after second conv: ceil((H - 2)/2) - 2,\\\\n-        # after last conv: ceil((H - 2)/2) - 4,\\\\n-        # and the same for W\\\\n-\\\\n-        self.features = nn.Sequential(OrderedDict([\\\\n-            (\\\\\\\'conv1\\\\\\\', nn.Conv2d(3, 10, 3, 1)),\\\\n-            (\\\\\\\'prelu1\\\\\\\', nn.PReLU(10)),\\\\n-            (\\\\\\\'pool1\\\\\\\', nn.MaxPool2d(2, 2, ceil_mode=True)),\\\\n-\\\\n-            (\\\\\\\'conv2\\\\\\\', nn.Conv2d(10, 16, 3, 1)),\\\\n-            (\\\\\\\'prelu2\\\\\\\', nn.PReLU(16)),\\\\n-\\\\n-            (\\\\\\\'conv3\\\\\\\', nn.Conv2d(16, 32, 3, 1)),\\\\n-            (\\\\\\\'prelu3\\\\\\\', nn.PReLU(32))\\\\n-        ]))\\\\n-\\\\n-        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\\\\n-        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\\\\n-\\\\n-        weights = np.load(\\\\\\\'mtcnn_pytorch/src/weights/pnet.npy\\\\\\\')[()]\\\\n-        for n, p in self.named_parameters():\\\\n-            p.data = torch.FloatTensor(weights[n])\\\\n-\\\\n-    def forward(self, x):\\\\n-        """\\\\n-        Arguments:\\\\n-            x: a float tensor with shape [batch_size, 3, h, w].\\\\n-        Returns:\\\\n-            b: a float tensor with shape [batch_size, 4, h\\\\\\\', w\\\\\\\'].\\\\n-            a: a float tensor with shape [batch_size, 2, h\\\\\\\', w\\\\\\\'].\\\\n-        """\\\\n-        x = self.features(x)\\\\n-        a = self.conv4_1(x)\\\\n-        b = self.conv4_2(x)\\\\n-        a = F.softmax(a, dim= 1)\\\\n-        return b, a\\\\n-\\\\n-\\\\n-class RNet(nn.Module):\\\\n-\\\\n-    def __init__(self):\\\\n-\\\\n-        super(RNet, self).__init__()\\\\n-\\\\n-        self.features = nn.Sequential(OrderedDict([\\\\n-            (\\\\\\\'conv1\\\\\\\', nn.Conv2d(3, 28, 3, 1)),\\\\n-            (\\\\\\\'prelu1\\\\\\\', nn.PReLU(28)),\\\\n-            (\\\\\\\'pool1\\\\\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\\\n-\\\\n-            (\\\\\\\'conv2\\\\\\\', nn.Conv2d(28, 48, 3, 1)),\\\\n-            (\\\\\\\'prelu2\\\\\\\', nn.PReLU(48)),\\\\n-            (\\\\\\\'pool2\\\\\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\\\n-\\\\n-            (\\\\\\\'conv3\\\\\\\', nn.Conv2d(48, 64, 2, 1)),\\\\n-            (\\\\\\\'prelu3\\\\\\\', nn.PReLU(64)),\\\\n-\\\\n-            (\\\\\\\'flatten\\\\\\\', Flatten()),\\\\n-            (\\\\\\\'conv4\\\\\\\', nn.Linear(576, 128)),\\\\n-            (\\\\\\\'prelu4\\\\\\\', nn.PReLU(128))\\\\n-        ]))\\\\n-\\\\n-        self.conv5_1 = nn.Linear(128, 2)\\\\n-        self.conv5_2 = nn.Linear(128, 4)\\\\n-\\\\n-        weights = np.load(\\\\\\\'mtcnn_pytorch/src/weights/rnet.npy\\\\\\\')[()]\\\\n-        for n, p in self.named_parameters():\\\\n-            p.data = torch.FloatTensor(weights[n])\\\\n-\\\\n-    def forward(self, x):\\\\n-        """\\\\n-        Arguments:\\\\n-            x: a float tensor with shape [batch_size, 3, h, w].\\\\n-        Returns:\\\\n-            b: a float tensor with shape [batch_size, 4].\\\\n-            a: a float tensor with shape [batch_size, 2].\\\\n-        """\\\\n-        x = self.features(x)\\\\n-        a = self.conv5_1(x)\\\\n-        b = self.conv5_2(x)\\\\n-        a = F.softmax(a, dim=-1)\\\\n-        return b, a\\\\n-\\\\n-\\\\n-class ONet(nn.Module):\\\\n-\\\\n-    def __init__(self):\\\\n-\\\\n-        super(ONet, self).__init__()\\\\n-\\\\n-        self.features = nn.Sequential(OrderedDict([\\\\n-            (\\\\\\\'conv1\\\\\\\', nn.Conv2d(3, 32, 3, 1)),\\\\n-            (\\\\\\\'prelu1\\\\\\\', nn.PReLU(32)),\\\\n-            (\\\\\\\'pool1\\\\\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\\\n-\\\\n-            (\\\\\\\'conv2\\\\\\\', nn.Conv2d(32, 64, 3, 1)),\\\\n-            (\\\\\\\'prelu2\\\\\\\', nn.PReLU(64)),\\\\n-            (\\\\\\\'pool2\\\\\\\', nn.MaxPool2d(3, 2, ceil_mode=True)),\\\\n-\\\\n-            (\\\\\\\'conv3\\\\\\\', nn.Conv2d(64, 64, 3, 1)),\\\\n-            (\\\\\\\'prelu3\\\\\\\', nn.PReLU(64)),\\\\n-            (\\\\\\\'pool3\\\\\\\', nn.MaxPool2d(2, 2, ceil_mode=True)),\\\\n-\\\\n-            (\\\\\\\'conv4\\\\\\\', nn.Conv2d(64, 128, 2, 1)),\\\\n-            (\\\\\\\'prelu4\\\\\\\', nn.PReLU(128)),\\\\n-\\\\n-            (\\\\\\\'flatten\\\\\\\', Flatten()),\\\\n-            (\\\\\\\'conv5\\\\\\\', nn.Linear(1152, 256)),\\\\n-            (\\\\\\\'drop5\\\\\\\', nn.Dropout(0.25)),\\\\n-            (\\\\\\\'prelu5\\\\\\\', nn.PReLU(256)),\\\\n-        ]))\\\\n-\\\\n-        self.conv6_1 = nn.Linear(256, 2)\\\\n-        self.conv6_2 = nn.Linear(256, 4)\\\\n-        self.conv6_3 = nn.Linear(256, 10)\\\\n-\\\\n-        weights = np.load(\\\\\\\'mtcnn_pytorch/src/weights/onet.npy\\\\\\\')[()]\\\\n-        for n, p in self.named_parameters():\\\\n-            p.data = torch.FloatTensor(weights[n])\\\\n-\\\\n-    def forward(self, x):\\\\n-        """\\\\n-        Arguments:\\\\n-            x: a float tensor with shape [batch_size, 3, h, w].\\\\n-        Returns:\\\\n-            c: a float tensor with shape [batch_size, 10].\\\\n-            b: a float tensor with shape [batch_size, 4].\\\\n-            a: a float tensor with shape [batch_size, 2].\\\\n-        """\\\\n-        x = self.features(x)\\\\n-        a = self.conv6_1(x)\\\\n-        b = self.conv6_2(x)\\\\n-        c = self.conv6_3(x)\\\\n-        a = F.softmax(a, dim = -1)\\\\n-        return c, b, a\\\\ndiff --git a/mtcnn_pytorch/src/matlab_cp2tform.py b/mtcnn_pytorch/src/matlab_cp2tform.py\\\\ndeleted file mode 100644\\\\nindex cdcdf96..0000000\\\\n--- a/mtcnn_pytorch/src/matlab_cp2tform.py\\\\n+++ /dev/null\\\\n@@ -1,350 +0,0 @@\\\\n-# -*- coding: utf-8 -*-\\\\n-"""\\\\n-Created on Tue Jul 11 06:54:28 2017\\\\n-\\\\n-@author: zhaoyafei\\\\n-"""\\\\n-\\\\n-import numpy as np\\\\n-from numpy.linalg import inv, norm, lstsq\\\\n-from numpy.linalg import matrix_rank as rank\\\\n-\\\\n-class MatlabCp2tormException(Exception):\\\\n-    def __str__(self):\\\\n-        return \\\\\\\'In File {}:{}\\\\\\\'.format(\\\\n-                __file__, super.__str__(self))\\\\n-\\\\n-def tformfwd(trans, uv):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        apply affine transform \\\\\\\'trans\\\\\\\' to uv\\\\n-\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @trans: 3x3 np.array\\\\n-            transform matrix\\\\n-        @uv: Kx2 np.array\\\\n-            each row is a pair of coordinates (x, y)\\\\n-\\\\n-    Returns:\\\\n-    ----------\\\\n-        @xy: Kx2 np.array\\\\n-            each row is a pair of transformed coordinates (x, y)\\\\n-    """\\\\n-    uv = np.hstack((\\\\n-        uv, np.ones((uv.shape[0], 1))\\\\n-    ))\\\\n-    xy = np.dot(uv, trans)\\\\n-    xy = xy[:, 0:-1]\\\\n-    return xy\\\\n-\\\\n-\\\\n-def tforminv(trans, uv):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        apply the inverse of affine transform \\\\\\\'trans\\\\\\\' to uv\\\\n-\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @trans: 3x3 np.array\\\\n-            transform matrix\\\\n-        @uv: Kx2 np.array\\\\n-            each row is a pair of coordinates (x, y)\\\\n-\\\\n-    Returns:\\\\n-    ----------\\\\n-        @xy: Kx2 np.array\\\\n-            each row is a pair of inverse-transformed coordinates (x, y)\\\\n-    """\\\\n-    Tinv = inv(trans)\\\\n-    xy = tformfwd(Tinv, uv)\\\\n-    return xy\\\\n-\\\\n-\\\\n-def findNonreflectiveSimilarity(uv, xy, options=None):\\\\n-\\\\n-    options = {\\\\\\\'K\\\\\\\': 2}\\\\n-\\\\n-    K = options[\\\\\\\'K\\\\\\\']\\\\n-    M = xy.shape[0]\\\\n-    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\\\\n-    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\\\\n-    # print(\\\\\\\'--->x, y:\\\\\\\\n\\\\\\\', x, y\\\\n-\\\\n-    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\\\\n-    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\\\\n-    X = np.vstack((tmp1, tmp2))\\\\n-    # print(\\\\\\\'--->X.shape: \\\\\\\', X.shape\\\\n-    # print(\\\\\\\'X:\\\\\\\\n\\\\\\\', X\\\\n-\\\\n-    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\\\\n-    v = uv[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\\\\n-    U = np.vstack((u, v))\\\\n-    # print(\\\\\\\'--->U.shape: \\\\\\\', U.shape\\\\n-    # print(\\\\\\\'U:\\\\\\\\n\\\\\\\', U\\\\n-\\\\n-    # We know that X * r = U\\\\n-    if rank(X) >= 2 * K:\\\\n-        r, _, _, _ = lstsq(X, U)\\\\n-        r = np.squeeze(r)\\\\n-    else:\\\\n-        raise Exception(\\\\\\\'cp2tform:twoUniquePointsReq\\\\\\\')\\\\n-\\\\n-    # print(\\\\\\\'--->r:\\\\\\\\n\\\\\\\', r\\\\n-\\\\n-    sc = r[0]\\\\n-    ss = r[1]\\\\n-    tx = r[2]\\\\n-    ty = r[3]\\\\n-\\\\n-    Tinv = np.array([\\\\n-        [sc, -ss, 0],\\\\n-        [ss,  sc, 0],\\\\n-        [tx,  ty, 1]\\\\n-    ])\\\\n-\\\\n-    # print(\\\\\\\'--->Tinv:\\\\\\\\n\\\\\\\', Tinv\\\\n-\\\\n-    T = inv(Tinv)\\\\n-    # print(\\\\\\\'--->T:\\\\\\\\n\\\\\\\', T\\\\n-\\\\n-    T[:, 2] = np.array([0, 0, 1])\\\\n-\\\\n-    return T, Tinv\\\\n-\\\\n-\\\\n-def findSimilarity(uv, xy, options=None):\\\\n-\\\\n-    options = {\\\\\\\'K\\\\\\\': 2}\\\\n-\\\\n-#    uv = np.array(uv)\\\\n-#    xy = np.array(xy)\\\\n-\\\\n-    # Solve for trans1\\\\n-    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\\\\n-\\\\n-    # Solve for trans2\\\\n-\\\\n-    # manually reflect the xy data across the Y-axis\\\\n-    xyR = xy\\\\n-    xyR[:, 0] = -1 * xyR[:, 0]\\\\n-\\\\n-    trans2r, trans2r_inv = findNonreflectiveSimilarity(uv, xyR, options)\\\\n-\\\\n-    # manually reflect the tform to undo the reflection done on xyR\\\\n-    TreflectY = np.array([\\\\n-        [-1, 0, 0],\\\\n-        [0, 1, 0],\\\\n-        [0, 0, 1]\\\\n-    ])\\\\n-\\\\n-    trans2 = np.dot(trans2r, TreflectY)\\\\n-\\\\n-    # Figure out if trans1 or trans2 is better\\\\n-    xy1 = tformfwd(trans1, uv)\\\\n-    norm1 = norm(xy1 - xy)\\\\n-\\\\n-    xy2 = tformfwd(trans2, uv)\\\\n-    norm2 = norm(xy2 - xy)\\\\n-\\\\n-    if norm1 <= norm2:\\\\n-        return trans1, trans1_inv\\\\n-    else:\\\\n-        trans2_inv = inv(trans2)\\\\n-        return trans2, trans2_inv\\\\n-\\\\n-\\\\n-def get_similarity_transform(src_pts, dst_pts, reflective=True):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        Find Similarity Transform Matrix \\\\\\\'trans\\\\\\\':\\\\n-            u = src_pts[:, 0]\\\\n-            v = src_pts[:, 1]\\\\n-            x = dst_pts[:, 0]\\\\n-            y = dst_pts[:, 1]\\\\n-            [x, y, 1] = [u, v, 1] * trans\\\\n-\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @src_pts: Kx2 np.array\\\\n-            source points, each row is a pair of coordinates (x, y)\\\\n-        @dst_pts: Kx2 np.array\\\\n-            destination points, each row is a pair of transformed\\\\n-            coordinates (x, y)\\\\n-        @reflective: True or False\\\\n-            if True:\\\\n-                use reflective similarity transform\\\\n-            else:\\\\n-                use non-reflective similarity transform\\\\n-\\\\n-    Returns:\\\\n-    ----------\\\\n-       @trans: 3x3 np.array\\\\n-            transform matrix from uv to xy\\\\n-        trans_inv: 3x3 np.array\\\\n-            inverse of trans, transform matrix from xy to uv\\\\n-    """\\\\n-\\\\n-    if reflective:\\\\n-        trans, trans_inv = findSimilarity(src_pts, dst_pts)\\\\n-    else:\\\\n-        trans, trans_inv = findNonreflectiveSimilarity(src_pts, dst_pts)\\\\n-\\\\n-    return trans, trans_inv\\\\n-\\\\n-\\\\n-def cvt_tform_mat_for_cv2(trans):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        Convert Transform Matrix \\\\\\\'trans\\\\\\\' into \\\\\\\'cv2_trans\\\\\\\' which could be\\\\n-        directly used by cv2.warpAffine():\\\\n-            u = src_pts[:, 0]\\\\n-            v = src_pts[:, 1]\\\\n-            x = dst_pts[:, 0]\\\\n-            y = dst_pts[:, 1]\\\\n-            [x, y].T = cv_trans * [u, v, 1].T\\\\n-\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @trans: 3x3 np.array\\\\n-            transform matrix from uv to xy\\\\n-\\\\n-    Returns:\\\\n-    ----------\\\\n-        @cv2_trans: 2x3 np.array\\\\n-            transform matrix from src_pts to dst_pts, could be directly used\\\\n-            for cv2.warpAffine()\\\\n-    """\\\\n-    cv2_trans = trans[:, 0:2].T\\\\n-\\\\n-    return cv2_trans\\\\n-\\\\n-\\\\n-def get_similarity_transform_for_cv2(src_pts, dst_pts, reflective=True):\\\\n-    """\\\\n-    Function:\\\\n-    ----------\\\\n-        Find Similarity Transform Matrix \\\\\\\'cv2_trans\\\\\\\' which could be\\\\n-        directly used by cv2.warpAffine():\\\\n-            u = src_pts[:, 0]\\\\n-            v = src_pts[:, 1]\\\\n-            x = dst_pts[:, 0]\\\\n-            y = dst_pts[:, 1]\\\\n-            [x, y].T = cv_trans * [u, v, 1].T\\\\n-\\\\n-    Parameters:\\\\n-    ----------\\\\n-        @src_pts: Kx2 np.array\\\\n-            source points, each row is a pair of coordinates (x, y)\\\\n-        @dst_pts: Kx2 np.array\\\\n-            destination points, each row is a pair of transformed\\\\n-            coordinates (x, y)\\\\n-        reflective: True or False\\\\n-            if True:\\\\n-                use reflective similarity transform\\\\n-            else:\\\\n-                use non-reflective similarity transform\\\\n-\\\\n-    Returns:\\\\n-    ----------\\\\n-        @cv2_trans: 2x3 np.array\\\\n-            transform matrix from src_pts to dst_pts, could be directly used\\\\n-            for cv2.warpAffine()\\\\n-    """\\\\n-    trans, trans_inv = get_similarity_transform(src_pts, dst_pts, reflective)\\\\n-    cv2_trans = cvt_tform_mat_for_cv2(trans)\\\\n-\\\\n-    return cv2_trans\\\\n-\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    """\\\\n-    u = [0, 6, -2]\\\\n-    v = [0, 3, 5]\\\\n-    x = [-1, 0, 4]\\\\n-    y = [-1, -10, 4]\\\\n-\\\\n-    # In Matlab, run:\\\\n-    #\\\\n-    #   uv = [u\\\\\\\'; v\\\\\\\'];\\\\n-    #   xy = [x\\\\\\\'; y\\\\\\\'];\\\\n-    #   tform_sim=cp2tform(uv,xy,\\\\\\\'similarity\\\\\\\');\\\\n-    #\\\\n-    #   trans = tform_sim.tdata.T\\\\n-    #   ans =\\\\n-    #       -0.0764   -1.6190         0\\\\n-    #        1.6190   -0.0764         0\\\\n-    #       -3.2156    0.0290    1.0000\\\\n-    #   trans_inv = tform_sim.tdata.Tinv\\\\n-    #    ans =\\\\n-    #\\\\n-    #       -0.0291    0.6163         0\\\\n-    #       -0.6163   -0.0291         0\\\\n-    #       -0.0756    1.9826    1.0000\\\\n-    #    xy_m=tformfwd(tform_sim, u,v)\\\\n-    #\\\\n-    #    xy_m =\\\\n-    #\\\\n-    #       -3.2156    0.0290\\\\n-    #        1.1833   -9.9143\\\\n-    #        5.0323    2.8853\\\\n-    #    uv_m=tforminv(tform_sim, x,y)\\\\n-    #\\\\n-    #    uv_m =\\\\n-    #\\\\n-    #        0.5698    1.3953\\\\n-    #        6.0872    2.2733\\\\n-    #       -2.6570    4.3314\\\\n-    """\\\\n-    u = [0, 6, -2]\\\\n-    v = [0, 3, 5]\\\\n-    x = [-1, 0, 4]\\\\n-    y = [-1, -10, 4]\\\\n-\\\\n-    uv = np.array((u, v)).T\\\\n-    xy = np.array((x, y)).T\\\\n-\\\\n-    print(\\\\\\\'\\\\\\\\n--->uv:\\\\\\\')\\\\n-    print(uv)\\\\n-    print(\\\\\\\'\\\\\\\\n--->xy:\\\\\\\')\\\\n-    print(xy)\\\\n-\\\\n-    trans, trans_inv = get_similarity_transform(uv, xy)\\\\n-\\\\n-    print(\\\\\\\'\\\\\\\\n--->trans matrix:\\\\\\\')\\\\n-    print(trans)\\\\n-\\\\n-    print(\\\\\\\'\\\\\\\\n--->trans_inv matrix:\\\\\\\')\\\\n-    print(trans_inv)\\\\n-\\\\n-    print(\\\\\\\'\\\\\\\\n---> apply transform to uv\\\\\\\')\\\\n-    print(\\\\\\\'\\\\\\\\nxy_m = uv_augmented * trans\\\\\\\')\\\\n-    uv_aug = np.hstack((\\\\n-        uv, np.ones((uv.shape[0], 1))\\\\n-    ))\\\\n-    xy_m = np.dot(uv_aug, trans)\\\\n-    print(xy_m)\\\\n-\\\\n-    print(\\\\\\\'\\\\\\\\nxy_m = tformfwd(trans, uv)\\\\\\\')\\\\n-    xy_m = tformfwd(trans, uv)\\\\n-    print(xy_m)\\\\n-\\\\n-    print(\\\\\\\'\\\\\\\\n---> apply inverse transform to xy\\\\\\\')\\\\n-    print(\\\\\\\'\\\\\\\\nuv_m = xy_augmented * trans_inv\\\\\\\')\\\\n-    xy_aug = np.hstack((\\\\n-        xy, np.ones((xy.shape[0], 1))\\\\n-    ))\\\\n-    uv_m = np.dot(xy_aug, trans_inv)\\\\n-    print(uv_m)\\\\n-\\\\n-    print(\\\\\\\'\\\\\\\\nuv_m = tformfwd(trans_inv, xy)\\\\\\\')\\\\n-    uv_m = tformfwd(trans_inv, xy)\\\\n-    print(uv_m)\\\\n-\\\\n-    uv_m = tforminv(trans, xy)\\\\n-    print(\\\\\\\'\\\\\\\\nuv_m = tforminv(trans, xy)\\\\\\\')\\\\n-    print(uv_m)\\\\ndiff --git a/mtcnn_pytorch/src/visualization_utils.py b/mtcnn_pytorch/src/visualization_utils.py\\\\ndeleted file mode 100644\\\\nindex bab02be..0000000\\\\n--- a/mtcnn_pytorch/src/visualization_utils.py\\\\n+++ /dev/null\\\\n@@ -1,31 +0,0 @@\\\\n-from PIL import ImageDraw\\\\n-\\\\n-\\\\n-def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\\\\n-    """Draw bounding boxes and facial landmarks.\\\\n-\\\\n-    Arguments:\\\\n-        img: an instance of PIL.Image.\\\\n-        bounding_boxes: a float numpy array of shape [n, 5].\\\\n-        facial_landmarks: a float numpy array of shape [n, 10].\\\\n-\\\\n-    Returns:\\\\n-        an instance of PIL.Image.\\\\n-    """\\\\n-\\\\n-    img_copy = img.copy()\\\\n-    draw = ImageDraw.Draw(img_copy)\\\\n-\\\\n-    for b in bounding_boxes:\\\\n-        draw.rectangle([\\\\n-            (b[0], b[1]), (b[2], b[3])\\\\n-        ], outline=\\\\\\\'white\\\\\\\')\\\\n-\\\\n-    for p in facial_landmarks:\\\\n-        for i in range(5):\\\\n-            draw.ellipse([\\\\n-                (p[i] - 1.0, p[i + 5] - 1.0),\\\\n-                (p[i] + 1.0, p[i + 5] + 1.0)\\\\n-            ], outline=\\\\\\\'blue\\\\\\\')\\\\n-\\\\n-    return img_copy\\\\ndiff --git a/mtcnn_pytorch/src/weights/onet.npy b/mtcnn_pytorch/src/weights/onet.npy\\\\ndeleted file mode 100644\\\\nindex e8f63e5..0000000\\\\nBinary files a/mtcnn_pytorch/src/weights/onet.npy and /dev/null differ\\\\ndiff --git a/mtcnn_pytorch/src/weights/pnet.npy b/mtcnn_pytorch/src/weights/pnet.npy\\\\ndeleted file mode 100644\\\\nindex 91f8f9c..0000000\\\\nBinary files a/mtcnn_pytorch/src/weights/pnet.npy and /dev/null differ\\\\ndiff --git a/mtcnn_pytorch/src/weights/rnet.npy b/mtcnn_pytorch/src/weights/rnet.npy\\\\ndeleted file mode 100644\\\\nindex 5e9bbab..0000000\\\\nBinary files a/mtcnn_pytorch/src/weights/rnet.npy and /dev/null differ\\\\ndiff --git a/prepare_data.py b/prepare_data.py\\\\ndeleted file mode 100644\\\\nindex 91ab673..0000000\\\\n--- a/prepare_data.py\\\\n+++ /dev/null\\\\n@@ -1,17 +0,0 @@\\\\n-from pathlib import Path\\\\n-from config import get_config\\\\n-from data.data_pipe import load_bin, load_mx_rec\\\\n-import argparse\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    parser = argparse.ArgumentParser(description=\\\\\\\'for face verification\\\\\\\')\\\\n-    parser.add_argument("-r", "--rec_path", help="mxnet record file path",default=\\\\\\\'faces_emore\\\\\\\', type=str)\\\\n-    args = parser.parse_args()\\\\n-    conf = get_config()\\\\n-    rec_path = conf.data_path/args.rec_path\\\\n-    load_mx_rec(rec_path)\\\\n-    \\\\n-    bin_files = [\\\\\\\'agedb_30\\\\\\\', \\\\\\\'cfp_fp\\\\\\\', \\\\\\\'lfw\\\\\\\', \\\\\\\'calfw\\\\\\\', \\\\\\\'cfp_ff\\\\\\\', \\\\\\\'cplfw\\\\\\\', \\\\\\\'vgg2_fp\\\\\\\']\\\\n-    \\\\n-    for i in range(len(bin_files)):\\\\n-        load_bin(rec_path/(bin_files[i]+\\\\\\\'.bin\\\\\\\'), rec_path/bin_files[i], conf.test_transform)\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/requirements.txt b/requirements.txt\\\\ndeleted file mode 100644\\\\nindex cb619de..0000000\\\\nBinary files a/requirements.txt and /dev/null differ\\\\ndiff --git a/templates/index.html b/templates/index.html\\\\ndeleted file mode 100644\\\\nindex b54f90b..0000000\\\\n--- a/templates/index.html\\\\n+++ /dev/null\\\\n@@ -1,23 +0,0 @@\\\\n-<html>\\\\n-    <head>\\\\n-    <style>\\\\n-    body {\\\\n-        display:block;\\\\n-        background: url("E:\\\\\\\\Github\\\\\\\\Flask\\\\\\\\images.png"); \\\\n-    }\\\\n-    h1{\\\\n-        text-align: center;\\\\n-        text-decoration: underline;\\\\n-        text-transform: uppercase;\\\\n-    }\\\\n-    </style>\\\\n-    <title>Face Recognition</title>\\\\n-    </head>\\\\n-    <body>\\\\n-    \\\\n-    <h1 align = "center">Face Recognition</h1>\\\\n-    <p align ="center">\\\\n-    <img src="{{ url_for(\\\\\\\'video_feed\\\\\\\') }}">\\\\n-    </p>\\\\n-    </body>\\\\n-</html>\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/utils.py b/utils.py\\\\ndeleted file mode 100644\\\\nindex b50cbc8..0000000\\\\n--- a/utils.py\\\\n+++ /dev/null\\\\n@@ -1,156 +0,0 @@\\\\n-from datetime import datetime\\\\n-from PIL import Image\\\\n-import numpy as np\\\\n-import matplotlib.pyplot as plt\\\\n-plt.switch_backend(\\\\\\\'agg\\\\\\\')\\\\n-import io\\\\n-from torchvision import transforms as trans\\\\n-from data.data_pipe import de_preprocess\\\\n-import torch\\\\n-from model import l2_norm\\\\n-import pdb\\\\n-import cv2\\\\n-\\\\n-def separate_bn_paras(modules):\\\\n-    if not isinstance(modules, list):\\\\n-        modules = [*modules.modules()]\\\\n-    paras_only_bn = []\\\\n-    paras_wo_bn = []\\\\n-    for layer in modules:\\\\n-        if \\\\\\\'model\\\\\\\' in str(layer.__class__):\\\\n-            continue\\\\n-        if \\\\\\\'container\\\\\\\' in str(layer.__class__):\\\\n-            continue\\\\n-        else:\\\\n-            if \\\\\\\'batchnorm\\\\\\\' in str(layer.__class__):\\\\n-                paras_only_bn.extend([*layer.parameters()])\\\\n-            else:\\\\n-                paras_wo_bn.extend([*layer.parameters()])\\\\n-    return paras_only_bn, paras_wo_bn\\\\n-\\\\n-def prepare_facebank(conf, model, mtcnn, tta = True):\\\\n-    model.eval()\\\\n-    embeddings =  []\\\\n-    names = [\\\\\\\'Unknown\\\\\\\']\\\\n-    for path in conf.facebank_path.iterdir():\\\\n-        if path.is_file():\\\\n-            continue\\\\n-        else:\\\\n-            embs = []\\\\n-            for file in path.iterdir():\\\\n-                if not file.is_file():\\\\n-                    continue\\\\n-                else:\\\\n-                    try:\\\\n-                        img = Image.open(file)\\\\n-                    except:\\\\n-                        continue\\\\n-                    if img.size != (112, 112):\\\\n-                        img = mtcnn.align(img)\\\\n-                    with torch.no_grad():\\\\n-                        if tta:\\\\n-                            mirror = trans.functional.hflip(img)\\\\n-                            emb = model(conf.test_transform(img).to(conf.device).unsqueeze(0))\\\\n-                            emb_mirror = model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\\\\n-                            embs.append(l2_norm(emb + emb_mirror))\\\\n-                        else:                        \\\\n-                            embs.append(model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\\\\n-        if len(embs) == 0:\\\\n-            continue\\\\n-        embedding = torch.cat(embs).mean(0,keepdim=True)\\\\n-        embeddings.append(embedding)\\\\n-        names.append(path.name)\\\\n-    embeddings = torch.cat(embeddings)\\\\n-    names = np.array(names)\\\\n-    torch.save(embeddings, conf.facebank_path/\\\\\\\'facebank.pth\\\\\\\')\\\\n-    np.save(conf.facebank_path/\\\\\\\'names\\\\\\\', names)\\\\n-    return embeddings, names\\\\n-\\\\n-def load_facebank(conf):\\\\n-    embeddings = torch.load(conf.facebank_path/\\\\\\\'facebank.pth\\\\\\\', map_location=torch.device(\\\\\\\'cpu\\\\\\\'))\\\\n-    names = np.load(conf.facebank_path/\\\\\\\'names.npy\\\\\\\')\\\\n-    return embeddings, names\\\\n-\\\\n-def face_reader(conf, conn, flag, boxes_arr, result_arr, learner, mtcnn, targets, tta):\\\\n-    while True:\\\\n-        try:\\\\n-            image = conn.recv()\\\\n-        except:\\\\n-            continue\\\\n-        try:            \\\\n-            bboxes, faces = mtcnn.align_multi(image, limit=conf.face_limit)\\\\n-        except:\\\\n-            bboxes = []\\\\n-            \\\\n-        results = learner.infer(conf, faces, targets, tta)\\\\n-        \\\\n-        if len(bboxes) > 0:\\\\n-            print(\\\\\\\'bboxes in reader : {}\\\\\\\'.format(bboxes))\\\\n-            bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\\\\n-            bboxes = bboxes.astype(int)\\\\n-            bboxes = bboxes + [-1,-1,1,1] # personal choice            \\\\n-            assert bboxes.shape[0] == results.shape[0],\\\\\\\'bbox and faces number not same\\\\\\\'\\\\n-            bboxes = bboxes.reshape([-1])\\\\n-            for i in range(len(boxes_arr)):\\\\n-                if i < len(bboxes):\\\\n-                    boxes_arr[i] = bboxes[i]\\\\n-                else:\\\\n-                    boxes_arr[i] = 0 \\\\n-            for i in range(len(result_arr)):\\\\n-                if i < len(results):\\\\n-                    result_arr[i] = results[i]\\\\n-                else:\\\\n-                    result_arr[i] = -1 \\\\n-        else:\\\\n-            for i in range(len(boxes_arr)):\\\\n-                boxes_arr[i] = 0 # by default,it\\\\\\\'s all 0\\\\n-            for i in range(len(result_arr)):\\\\n-                result_arr[i] = -1 # by default,it\\\\\\\'s all -1\\\\n-        print(\\\\\\\'boxes_arr \\\\xef\\\\xbc\\\\x9a {}\\\\\\\'.format(boxes_arr[:4]))\\\\n-        print(\\\\\\\'result_arr \\\\xef\\\\xbc\\\\x9a {}\\\\\\\'.format(result_arr[:4]))\\\\n-        flag.value = 0\\\\n-\\\\n-hflip = trans.Compose([\\\\n-            de_preprocess,\\\\n-            trans.ToPILImage(),\\\\n-            trans.functional.hflip,\\\\n-            trans.ToTensor(),\\\\n-            trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\\\\n-        ])\\\\n-\\\\n-def hflip_batch(imgs_tensor):\\\\n-    hfliped_imgs = torch.empty_like(imgs_tensor)\\\\n-    for i, img_ten in enumerate(imgs_tensor):\\\\n-        hfliped_imgs[i] = hflip(img_ten)\\\\n-    return hfliped_imgs\\\\n-\\\\n-def get_time():\\\\n-    return (str(datetime.now())[:-10]).replace(\\\\\\\' \\\\\\\',\\\\\\\'-\\\\\\\').replace(\\\\\\\':\\\\\\\',\\\\\\\'-\\\\\\\')\\\\n-\\\\n-def gen_plot(fpr, tpr):\\\\n-    """Create a pyplot plot and save to buffer."""\\\\n-    plt.figure()\\\\n-    plt.xlabel("FPR", fontsize=14)\\\\n-    plt.ylabel("TPR", fontsize=14)\\\\n-    plt.title("ROC Curve", fontsize=14)\\\\n-    plot = plt.plot(fpr, tpr, linewidth=2)\\\\n-    buf = io.BytesIO()\\\\n-    plt.savefig(buf, format=\\\\\\\'jpeg\\\\\\\')\\\\n-    buf.seek(0)\\\\n-    plt.close()\\\\n-    return buf\\\\n-\\\\n-def draw_box_name(bbox,name,frame):\\\\n-    if name == "Unknown":\\\\n-        frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,000),6)\\\\n-    else:\\\\n-        frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,000),6)\\\\n-        frame = cv2.putText(frame,\\\\n-                        name,\\\\n-                        (bbox[0],bbox[1]), \\\\n-                        cv2.FONT_HERSHEY_SIMPLEX, \\\\n-                        2,\\\\n-                        (0,255,0),\\\\n-                        3,\\\\n-                        cv2.LINE_AA)\\\\n-    return frame\\\\ndiff --git a/verifacation.py b/verifacation.py\\\\ndeleted file mode 100644\\\\nindex ff7d5ed..0000000\\\\n--- a/verifacation.py\\\\n+++ /dev/null\\\\n@@ -1,170 +0,0 @@\\\\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset\\\\n-"""\\\\n-\\\\n-# MIT License\\\\n-#\\\\n-# Copyright (c) 2016 David Sandberg\\\\n-#\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-#\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-#\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-import numpy as np\\\\n-from sklearn.model_selection import KFold\\\\n-from sklearn.decomposition import PCA\\\\n-import sklearn\\\\n-from scipy import interpolate\\\\n-import datetime\\\\n-import mxnet as mx\\\\n-\\\\n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, pca=0):\\\\n-    assert (embeddings1.shape[0] == embeddings2.shape[0])\\\\n-    assert (embeddings1.shape[1] == embeddings2.shape[1])\\\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\\\n-    nrof_thresholds = len(thresholds)\\\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\\\n-\\\\n-    tprs = np.zeros((nrof_folds, nrof_thresholds))\\\\n-    fprs = np.zeros((nrof_folds, nrof_thresholds))\\\\n-    accuracy = np.zeros((nrof_folds))\\\\n-    best_thresholds = np.zeros((nrof_folds))\\\\n-    indices = np.arange(nrof_pairs)\\\\n-    # print(\\\\\\\'pca\\\\\\\', pca)\\\\n-\\\\n-    if pca == 0:\\\\n-        diff = np.subtract(embeddings1, embeddings2)\\\\n-        dist = np.sum(np.square(diff), 1)\\\\n-\\\\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\\\n-        # print(\\\\\\\'train_set\\\\\\\', train_set)\\\\n-        # print(\\\\\\\'test_set\\\\\\\', test_set)\\\\n-        if pca > 0:\\\\n-            print(\\\\\\\'doing pca on\\\\\\\', fold_idx)\\\\n-            embed1_train = embeddings1[train_set]\\\\n-            embed2_train = embeddings2[train_set]\\\\n-            _embed_train = np.concatenate((embed1_train, embed2_train), axis=0)\\\\n-            # print(_embed_train.shape)\\\\n-            pca_model = PCA(n_components=pca)\\\\n-            pca_model.fit(_embed_train)\\\\n-            embed1 = pca_model.transform(embeddings1)\\\\n-            embed2 = pca_model.transform(embeddings2)\\\\n-            embed1 = sklearn.preprocessing.normalize(embed1)\\\\n-            embed2 = sklearn.preprocessing.normalize(embed2)\\\\n-            # print(embed1.shape, embed2.shape)\\\\n-            diff = np.subtract(embed1, embed2)\\\\n-            dist = np.sum(np.square(diff), 1)\\\\n-\\\\n-        # Find the best threshold for the fold\\\\n-        acc_train = np.zeros((nrof_thresholds))\\\\n-        for threshold_idx, threshold in enumerate(thresholds):\\\\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\\\\n-        best_threshold_index = np.argmax(acc_train)\\\\n-#         print(\\\\\\\'best_threshold_index\\\\\\\', best_threshold_index, acc_train[best_threshold_index])\\\\n-        best_thresholds[fold_idx] = thresholds[best_threshold_index]\\\\n-        for threshold_idx, threshold in enumerate(thresholds):\\\\n-            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\\\\n-                                                                                                 dist[test_set],\\\\n-                                                                                                 actual_issame[\\\\n-                                                                                                     test_set])\\\\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set],\\\\n-                                                      actual_issame[test_set])\\\\n-\\\\n-    tpr = np.mean(tprs, 0)\\\\n-    fpr = np.mean(fprs, 0)\\\\n-    return tpr, fpr, accuracy, best_thresholds\\\\n-\\\\n-\\\\n-def calculate_accuracy(threshold, dist, actual_issame):\\\\n-    predict_issame = np.less(dist, threshold)\\\\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\\\\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\\\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\\\\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\\\\n-\\\\n-    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\\\\n-    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\\\\n-    acc = float(tp + tn) / dist.size\\\\n-    return tpr, fpr, acc\\\\n-\\\\n-\\\\n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):\\\\n-    \\\\\\\'\\\\\\\'\\\\\\\'\\\\n-    Copy from [insightface](https://github.com/deepinsight/insightface)\\\\n-    :param thresholds:\\\\n-    :param embeddings1:\\\\n-    :param embeddings2:\\\\n-    :param actual_issame:\\\\n-    :param far_target:\\\\n-    :param nrof_folds:\\\\n-    :return:\\\\n-    \\\\\\\'\\\\\\\'\\\\\\\'\\\\n-    assert (embeddings1.shape[0] == embeddings2.shape[0])\\\\n-    assert (embeddings1.shape[1] == embeddings2.shape[1])\\\\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\\\\n-    nrof_thresholds = len(thresholds)\\\\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\\\\n-\\\\n-    val = np.zeros(nrof_folds)\\\\n-    far = np.zeros(nrof_folds)\\\\n-\\\\n-    diff = np.subtract(embeddings1, embeddings2)\\\\n-    dist = np.sum(np.square(diff), 1)\\\\n-    indices = np.arange(nrof_pairs)\\\\n-\\\\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\\\\n-\\\\n-        # Find the threshold that gives FAR = far_target\\\\n-        far_train = np.zeros(nrof_thresholds)\\\\n-        for threshold_idx, threshold in enumerate(thresholds):\\\\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\\\\n-        if np.max(far_train) >= far_target:\\\\n-            f = interpolate.interp1d(far_train, thresholds, kind=\\\\\\\'slinear\\\\\\\')\\\\n-            threshold = f(far_target)\\\\n-        else:\\\\n-            threshold = 0.0\\\\n-\\\\n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\\\\n-\\\\n-    val_mean = np.mean(val)\\\\n-    far_mean = np.mean(far)\\\\n-    val_std = np.std(val)\\\\n-    return val_mean, val_std, far_mean\\\\n-\\\\n-\\\\n-def calculate_val_far(threshold, dist, actual_issame):\\\\n-    predict_issame = np.less(dist, threshold)\\\\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\\\\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\\\\n-    n_same = np.sum(actual_issame)\\\\n-    n_diff = np.sum(np.logical_not(actual_issame))\\\\n-    val = float(true_accept) / float(n_same)\\\\n-    far = float(false_accept) / float(n_diff)\\\\n-    return val, far\\\\n-\\\\n-\\\\n-def evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\\\\n-    # Calculate evaluation metrics\\\\n-    thresholds = np.arange(0, 4, 0.01)\\\\n-    embeddings1 = embeddings[0::2]\\\\n-    embeddings2 = embeddings[1::2]\\\\n-    tpr, fpr, accuracy, best_thresholds = calculate_roc(thresholds, embeddings1, embeddings2,\\\\n-                                       np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)\\\\n-#     thresholds = np.arange(0, 4, 0.001)\\\\n-#     val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\\\\n-#                                       np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\\\\n-#     return tpr, fpr, accuracy, best_thresholds, val, val_std, far\\\\n-    return tpr, fpr, accuracy, best_thresholds\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/work_space/history b/work_space/history\\\\ndeleted file mode 100644\\\\nindex 30c2973..0000000\\\\n--- a/work_space/history\\\\n+++ /dev/null\\\\n@@ -1 +0,0 @@\\\\n-/home/f/learning/face_studio/history\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/work_space/log b/work_space/log\\\\ndeleted file mode 100644\\\\nindex 2b970ae..0000000\\\\n--- a/work_space/log\\\\n+++ /dev/null\\\\n@@ -1 +0,0 @@\\\\n-/home/f/learning/face_studio/log\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/work_space/model/model_ir_se50.pth b/work_space/model/model_ir_se50.pth\\\\ndeleted file mode 100644\\\\nindex d3a030d..0000000\\\\n--- a/work_space/model/model_ir_se50.pth\\\\n+++ /dev/null\\\\n@@ -1,3 +0,0 @@\\\\n-version https://git-lfs.github.com/spec/v1\\\\n-oid sha256:a035c768259b98ab1ce0e646312f48b9e1e218197a0f80ac6765e88f8b6ddf28\\\\n-size 175367323\\\\ndiff --git a/work_space/save/model_cpu_final.pth b/work_space/save/model_cpu_final.pth\\\\ndeleted file mode 100644\\\\nindex d3a030d..0000000\\\\n--- a/work_space/save/model_cpu_final.pth\\\\n+++ /dev/null\\\\n@@ -1,3 +0,0 @@\\\\n-version https://git-lfs.github.com/spec/v1\\\\n-oid sha256:a035c768259b98ab1ce0e646312f48b9e1e218197a0f80ac6765e88f8b6ddf28\\\\n-size 175367323\\\'\\n\\\\ No newline at end of file\\ndiff --git a/face_verify.py b/face_verify.py\\nindex b1aa7bd..158b2f6 100644\\n--- a/face_verify.py\\n+++ b/face_verify.py\\n@@ -11,7 +11,7 @@ from utils import load_facebank, draw_box_name, prepare_facebank\\n from database import mark_attendance_db\\n import time\\n import threading\\n-\\n+import numpy as np\\n # Create a semaphore with the maximum number of threads allowed\\n semaphore = threading.Semaphore()\\n \\n@@ -70,15 +70,15 @@ class faceRec:\\n         while cap.isOpened():\\n             isSuccess,frame = cap.read()\\n             if isSuccess:            \\n-                try:\\n-    #                 image = Image.fromarray(frame[...,::-1]) #bgr to rgb\\n+                try:     \\n                     image = Image.fromarray(frame)\\n                     bboxes, faces = mtcnn.align_multi(image, conf.face_limit, conf.min_face_size)\\n+                    if isinstance(bboxes, list):\\n+                        continue\\n                     bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\\n                     bboxes = bboxes.astype(int)\\n-                    bboxes = bboxes + [-1,-1,1,1] # personal choice    \\n+                    bboxes = bboxes + [-1,-1,1,1] # personal choice\\n                     results, score = learner.infer(conf, faces, targets, args.tta)\\n-                    # print(score[0])\\n                     for idx,bbox in enumerate(bboxes):\\n                         if args.score:\\n                             frame = draw_box_name(bbox, names[results[idx] + 1] + \\\'_{:.2f}\\\'.format(score[idx]), frame)\\n@@ -86,16 +86,15 @@ class faceRec:\\n                             if float(\\\'{:.2f}\\\'.format(score[idx])) > .98:\\n                                 name = names[0]\\n                             else:    \\n-                                name = names[results[idx]+1]\\n+                                name = names[results+1]\\n                                 thread = threading.Thread(target=mark_attendance_db_thread, args=(name,))\\n                                 thread.start()\\n-\\n-                            frame = draw_box_name(bbox, names[results[idx] + 1], frame)\\n-                except:\\n-                    pass    \\n+                            print(f"name: {names[results + 1]}")\\n+                            frame = draw_box_name(bbox, names[results + 1], frame)\\n+                except ValueError:\\n+                    pass\\n                 ret, jpeg = cv2.imencode(\\\'.jpg\\\', frame)\\n                 return jpeg.tostring()\\n-                # cv2.imshow(\\\'Arc Face Recognizer\\\', frame)\\n \\n \\n             if cv2.waitKey(1)&0xFF == ord(\\\'q\\\'):\\ndiff --git a/main.py b/main.py\\nindex 87eefe1..6d89188 100644\\n--- a/main.py\\n+++ b/main.py\\n@@ -20,8 +20,6 @@ app = FastAPI()\\n \\n templates = Jinja2Templates(directory="templates")\\n \\n-app.mount("/static", StaticFiles(directory="static"), name="static")\\n-\\n def json_serial(obj):\\n     """JSON serializer for objects not serializable by default json code"""\\n \\ndiff --git a/students.sqlite b/students.sqlite\\nindex 56a0ee3..bb375b6 100644\\nBinary files a/students.sqlite and b/students.sqlite differ\\ndiff --git a/templates/index.html b/templates/index.html\\nindex b54f90b..51ee07f 100644\\n--- a/templates/index.html\\n+++ b/templates/index.html\\n@@ -15,7 +15,7 @@\\n     </head>\\n     <body>\\n     \\n-    <h1 align = "center">Face Recognition</h1>\\n+    <h1 align = "center">SDC: Face Recognition</h1>\\n     <p align ="center">\\n     <img src="{{ url_for(\\\'video_feed\\\') }}">\\n     </p>\'\n\\ No newline at end of file\ndiff --git a/face_verify.py b/face_verify.py\nindex b1aa7bd..158b2f6 100644\n--- a/face_verify.py\n+++ b/face_verify.py\n@@ -11,7 +11,7 @@ from utils import load_facebank, draw_box_name, prepare_facebank\n from database import mark_attendance_db\n import time\n import threading\n-\n+import numpy as np\n # Create a semaphore with the maximum number of threads allowed\n semaphore = threading.Semaphore()\n \n@@ -70,15 +70,15 @@ class faceRec:\n         while cap.isOpened():\n             isSuccess,frame = cap.read()\n             if isSuccess:            \n-                try:\n-    #                 image = Image.fromarray(frame[...,::-1]) #bgr to rgb\n+                try:     \n                     image = Image.fromarray(frame)\n                     bboxes, faces = mtcnn.align_multi(image, conf.face_limit, conf.min_face_size)\n+                    if isinstance(bboxes, list):\n+                        continue\n                     bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\n                     bboxes = bboxes.astype(int)\n-                    bboxes = bboxes + [-1,-1,1,1] # personal choice    \n+                    bboxes = bboxes + [-1,-1,1,1] # personal choice\n                     results, score = learner.infer(conf, faces, targets, args.tta)\n-                    # print(score[0])\n                     for idx,bbox in enumerate(bboxes):\n                         if args.score:\n                             frame = draw_box_name(bbox, names[results[idx] + 1] + \'_{:.2f}\'.format(score[idx]), frame)\n@@ -86,16 +86,15 @@ class faceRec:\n                             if float(\'{:.2f}\'.format(score[idx])) > .98:\n                                 name = names[0]\n                             else:    \n-                                name = names[results[idx]+1]\n+                                name = names[results+1]\n                                 thread = threading.Thread(target=mark_attendance_db_thread, args=(name,))\n                                 thread.start()\n-\n-                            frame = draw_box_name(bbox, names[results[idx] + 1], frame)\n-                except:\n-                    pass    \n+                            print(f"name: {names[results + 1]}")\n+                            frame = draw_box_name(bbox, names[results + 1], frame)\n+                except ValueError:\n+                    pass\n                 ret, jpeg = cv2.imencode(\'.jpg\', frame)\n                 return jpeg.tostring()\n-                # cv2.imshow(\'Arc Face Recognizer\', frame)\n \n \n             if cv2.waitKey(1)&0xFF == ord(\'q\'):\ndiff --git a/main.py b/main.py\nindex 87eefe1..6d89188 100644\n--- a/main.py\n+++ b/main.py\n@@ -20,8 +20,6 @@ app = FastAPI()\n \n templates = Jinja2Templates(directory="templates")\n \n-app.mount("/static", StaticFiles(directory="static"), name="static")\n-\n def json_serial(obj):\n     """JSON serializer for objects not serializable by default json code"""\n \ndiff --git a/students.sqlite b/students.sqlite\nindex 56a0ee3..bb375b6 100644\nBinary files a/students.sqlite and b/students.sqlite differ\ndiff --git a/templates/index.html b/templates/index.html\nindex b54f90b..51ee07f 100644\n--- a/templates/index.html\n+++ b/templates/index.html\n@@ -15,7 +15,7 @@\n     </head>\n     <body>\n     \n-    <h1 align = "center">Face Recognition</h1>\n+    <h1 align = "center">SDC: Face Recognition</h1>\n     <p align ="center">\n     <img src="{{ url_for(\'video_feed\') }}">\n     </p>'